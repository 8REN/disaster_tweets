{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda env export > environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc847e3",
   "metadata": {},
   "source": [
    "replace contractions/location abbreviations via dictionary\n",
    "denoise text/apply word segmentation\n",
    "create word count dictionary\n",
    "next, spell check:\n",
    "create dictionary from reoccuring words in text? or use freq dict symspell\n",
    "apply spell check to words in word count dictionary occuring only one time\n",
    "apply pos tag to tweets creating new column\n",
    "lemmatize tweets based on pos tag column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlikely-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473e8c8",
   "metadata": {},
   "source": [
    "## import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2cab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "#test_df = pd.read_csv('test.csv')\n",
    "#target = pd.read_csv('train.csv', usecols=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc632945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "#print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b0bcf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     keyword location                                               text  \\\n",
       "0        NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1        NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2        NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3        NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4        NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "...      ...      ...                                                ...   \n",
       "7608     NaN      NaN  Two giant cranes holding a bridge collapse int...   \n",
       "7609     NaN      NaN  @aria_ahrary @TheTawniest The out of control w...   \n",
       "7610     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "7611     NaN      NaN  Police investigating after an e-bike collided ...   \n",
       "7612     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "7608       1  \n",
       "7609       1  \n",
       "7610       1  \n",
       "7611       1  \n",
       "7612       1  \n",
       "\n",
       "[7613 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f433b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df.drop(['id'], axis=1, inplace=True)\n",
    "#df.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb5bba24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7552\n"
     ]
    }
   ],
   "source": [
    "#dups = df[df.duplicated(['text'])]\n",
    "df.drop_duplicates(subset=['text', 'location', 'keyword'], keep='first', inplace=True)\n",
    "#test_df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "\n",
    "print(len(df))\n",
    "#print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce86907",
   "metadata": {},
   "source": [
    "# process text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d573344",
   "metadata": {},
   "source": [
    "## dictionary based word replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83272deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and read previously created dictionary as csv for abbreviations and expansions of words\n",
    "def csv2dict(csv_name):\n",
    "    with open(csv_name, mode='r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        return {rows[1]:rows[2] for rows in reader}\n",
    "    \n",
    "# import and assign precomposed dictionaries for word replacement for contractions and location initialisms and abbreviations\n",
    "location_expansion_dict = csv2dict('utils\\location_expansion.csv')\n",
    "contractions_dict = csv2dict('utils\\contractions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5d477",
   "metadata": {},
   "source": [
    "## denoise text functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c90dec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from spellchecker import SpellChecker\n",
    "import pkg_resources\n",
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5aa60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions related to expanded hashtags in text\n",
    "def pascal_case_split(text):\n",
    "    # expand hashtags formatted in pascal case, ex: #ThisIsPascal\n",
    "    return re.sub(r'([A-Z])([?=a-z0-9+])', r' \\1\\2', text)\n",
    "\n",
    "def camel_case_split(text):\n",
    "    # expand hashtags formatted in pascal case, ex: #thisIsCamel\n",
    "    return re.sub(r'([a-z0-9+])([?<=A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "def replace_spell(text):\n",
    "    r_re = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, filtered_repl.keys()))+r'\\b')\n",
    "    def replace(match):\n",
    "        replacement =  f\"{filtered_repl[match.group(0)]}\"\n",
    "        return replacement\n",
    "    text = r_re.sub(replace, text)\n",
    "    return text\n",
    "\n",
    "def spell_check_word(word):\n",
    "    # lookup suggestions for individual words\n",
    "    spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "                      \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    corrections = spell.lookup(word, Verbosity.TOP, include_unknown=True)\n",
    "    closest = corrections[0]\n",
    "    # display suggestion term, edit distance, and term frequency\n",
    "    return closest.term\n",
    "        \n",
    "def spell_check_words(word):\n",
    "    # lookup suggestions for multi word string input\n",
    "    spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=8)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "    corrections = spell.lookup_compound(word, max_edit_distance=2)\n",
    "    closest = corrections[0]\n",
    "# display suggestion term, edit distance, and term frequency\n",
    "    return closest.term\n",
    "\n",
    "def desegment_strings(text):\n",
    "    spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=8)\n",
    "    dictionary_path = pkg_resources.resource_filename(\"symspellpy\",\"frequency_dictionary_en_82_765.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    if text.isupper() | text.islower() and text.startswith('#'):\n",
    "        result = spell.word_segmentation(text)\n",
    "        text = result.corrected_string\n",
    "    return text \n",
    "\n",
    "def remove_hash(text):\n",
    "    # remove hash symbol in front of hashtag text and remove non unicode chars\n",
    "    return re.sub('#', '', text)\n",
    "\n",
    "def expand_strings(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    h_text = remove_hash(p_text)\n",
    "    return h_text\n",
    "\n",
    "def expand_strings_sc(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash, spell check using compound lookup\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    lu_text = spell_check_words(p_text)\n",
    "    h_text = remove_hash(lu_text)\n",
    "    return h_text\n",
    "\n",
    "def expand_strings_ds(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash. spell check using wordsegmentation\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    lu_text = desegment_strings(p_text)\n",
    "    h_text = remove_hash(lu_text)\n",
    "    return h_text\n",
    "\n",
    "def expand_strings_indv_word(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash. spell check using wordsegmentation\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    lu_text = spell_check_word(p_text)\n",
    "    h_text = remove_hash(lu_text)\n",
    "    return h_text\n",
    "\n",
    "\n",
    "def word_expansion_from_dict(text, expansion_dict):\n",
    "    c_re = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, expansion_dict.keys()))+r'\\b')\n",
    "    def replace(match):\n",
    "        expansion =  f\"{expansion_dict[match.group(0)]}\"\n",
    "        return expansion\n",
    "    text = c_re.sub(replace, text)\n",
    "    return text\n",
    "\n",
    "def reduce_repeat_vowels(text, keep_reps=1):\n",
    "    # function to reduce repeated vowel occurences in string, \n",
    "    # keep_reps argument as int refers to number of occurences to keep in string\n",
    "    # keep_reps argument default : 1, can be set to any integer\n",
    "    vowels = ['a','e','i','o','u']\n",
    "    for ch in iter(vowels):\n",
    "        text = re.sub(r'(?i)'+rf'([{ch}])'+r'{3,}', r'\\1'*int(keep_reps), text)\n",
    "    return text\n",
    "\n",
    "def reduce_repeat_consonants(text, keep_reps=2):\n",
    "    # function to reduce repeated consonant occurences in string, \n",
    "    # keep_reps argument as int refers to number of occurences to keep in string\n",
    "    # keep_reps argument default : 1, can be set to any integer\n",
    "    consonants = ['b','c','d','f','g','h','j','k','l','m','n','p','q','r','s','t','v','w','x','y','z']\n",
    "    for ch in iter(consonants):\n",
    "        text = re.sub(r'(?i)'+ rf'([{ch}])'+r'{3,}', r'\\1'*int(keep_reps), text)\n",
    "    return text\n",
    "    \n",
    "def remove_urls(text):\n",
    "    # function to remove urls from string\n",
    "    return re.sub(r\"http\\S+\",  r\"\", text)\n",
    "\n",
    "def remove_handles(text):\n",
    "    # function to remove twitter handles from string\n",
    "    return re.sub(\"@\\S*\",  r\" \", text)\n",
    "\n",
    "def remove_non_uni(text):\n",
    "    # function to remove non unicode characters from string\n",
    "    return re.sub('[^\\u0000-\\u007f]', ' ', text)\n",
    "\n",
    "def remove_nan_str(text):\n",
    "    # function to remove 'nan' from string\n",
    "    return re.sub('nan', '', text)\n",
    "\n",
    "def remove_ws(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    # function to remove non alpha chars from string\n",
    "    return re.sub(\"[^a-zA-Z]\",  r\" \", text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    #reduce repeating characters\n",
    "    text = reduce_repeat_vowels(text)\n",
    "    text = reduce_repeat_consonants(text)\n",
    "    #remove xml tag strings\n",
    "    text = html.unescape(text)\n",
    "    #remove numeric and punctuation chars\n",
    "    text = remove_non_alpha(text)\n",
    "    #remove extra spaces\n",
    "    text = remove_ws(text)\n",
    "    return text   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf0fa6",
   "metadata": {},
   "source": [
    "## tweet text denoise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "concrete-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### text preprocessing specifically formatted for tweets but will work on any text\n",
    "def process_tweet(df): \n",
    "    \"\"\"combine regex and nltk processing for tweet text and keyword text processing\"\"\"\n",
    "    def expand_strings_ds(text):\n",
    "        # combine hashtag split functions for specific cases and remove hash\n",
    "        # remove urls and usernames first due to alternating casing inside before case format expansion\n",
    "        u_text = remove_urls(text)\n",
    "        hd_text = remove_handles(u_text)\n",
    "        c_text = camel_case_split(hd_text)\n",
    "        p_text = pascal_case_split(c_text)\n",
    "        lu_text = desegment_strings(p_text)\n",
    "        h_text = remove_hash(lu_text)\n",
    "        return h_text\n",
    "    \n",
    "    def expand_text(text):\n",
    "        e_text = expand_strings_ds(text)\n",
    "        le_text = word_expansion_from_dict(e_text, expansion_dict=location_expansion_dict)\n",
    "        ce_text = word_expansion_from_dict(le_text.lower(), expansion_dict=contractions_dict)\n",
    "        return ce_text\n",
    "\n",
    "    def process_text(text):\n",
    "        exp_text = expand_text(text)\n",
    "        clean_text = denoise_text(exp_text)\n",
    "        return clean_text\n",
    "\n",
    "    processed_df = [process_text(x) for x in df]\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16f797",
   "metadata": {},
   "source": [
    "## keyword text denoise process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ee31a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keyword(df):\n",
    "    text = df.str.replace('%20', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed85b7",
   "metadata": {},
   "source": [
    "## location text denoise process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "skilled-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_location(df):\n",
    "    \"\"\"combine regex and nltk processing for location text processing\"\"\"\n",
    "    \n",
    "    def expand_loc_text(text):\n",
    "        #exp_text = expand_strings_ds(text) \n",
    "        loc_dict_lower = {k.lower(): v for k, v in location_expansion_dict.items()}\n",
    "        exp_text = word_expansion_from_dict(text.lower(), loc_dict_lower)\n",
    "        return exp_text \n",
    "        \n",
    "    def process_text(text):\n",
    "        exp_text = expand_loc_text(text)\n",
    "        clean_text = denoise_text(exp_text)\n",
    "        return clean_text\n",
    "    \n",
    "    loc_df = [process_text(x) for x in df]\n",
    "    return loc_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08771c0",
   "metadata": {},
   "source": [
    "# create new dataframe of denoised text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "084979e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_data_to_df(df):\n",
    "    df['tweets'] = process_tweet(df.text)\n",
    "    df['clean_keyword'] = process_keyword(df.keyword.astype(str))\n",
    "    df['clean_location'] = process_location(df.location.astype(str))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb314aa",
   "metadata": {},
   "source": [
    "## use frequency dictionary for text to spell correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8696778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.probability import FreqDist\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4eb3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_rare(df):\n",
    "    corpus = df.tweets.str.split()\n",
    "    word_dict = Counter(corpus.sum())\n",
    "    single_occ_words = [k for k,v in word_dict.items() if int(v) <= 1]\n",
    "    corrections = []\n",
    "    for x in single_occ_words:\n",
    "        repl = spell_check_words(x)\n",
    "        corrections.append(repl)\n",
    "    replacement_dict = dict(zip(single_occ_words, corrections))\n",
    "    filtered_repl = dict(filter(lambda elem: elem[0] != elem[1], replacement_dict.items()))\n",
    "    def replace_spell(text):\n",
    "        r_re = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, filtered_repl.keys()))+r'\\b')\n",
    "        def replace(match):\n",
    "            replacement =  f\"{filtered_repl[match.group(0)]}\"\n",
    "            return replacement\n",
    "        text = r_re.sub(replace, text)\n",
    "        return text\n",
    "    df['new'] = df['tweets'].apply(replace_spell)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae1b6950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_compound(df):\n",
    "    corpus = df.tweets.str.split()\n",
    "    word_dict = Counter(corpus.sum())\n",
    "    single_occ_words = [k for k,v in word_dict.items() if int(v) <= 1]\n",
    "    corrections = []\n",
    "    for x in single_occ_words:\n",
    "        repl = spell_check_words(x)\n",
    "        corrections.append(repl)\n",
    "    replacement_dict = dict(zip(single_occ_words, corrections))\n",
    "    filtered_repl = dict(filter(lambda elem: elem[0] != elem[1], replacement_dict.items()))\n",
    "    def replace_spell(text):\n",
    "        r_re = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, filtered_repl.keys()))+r'\\b')\n",
    "        def replace(match):\n",
    "            replacement =  f\"{filtered_repl[match.group(0)]}\"\n",
    "            return replacement\n",
    "        text = r_re.sub(replace, text)\n",
    "        return text\n",
    "    df['new'] = df['tweets'].apply(replace_spell)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3451af05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        our deeds are the reason of this earthquake m...\n",
       "1                   forest fire near la ronge sask canada\n",
       "2        all residents asked to shelter in place are b...\n",
       "3        people receive wildfires evacuation orders in...\n",
       "4        just got sent this photo from ruby alaska as ...\n",
       "                              ...                        \n",
       "7547     two giant cranes holding a bridge collapse in...\n",
       "7548     the out of control wild fires in california e...\n",
       "7549                        m utc km s of volcano hawaii \n",
       "7550     police investigating after an e bike collided...\n",
       "7551     the latest more homes razed by northern calif...\n",
       "Name: tweets, Length: 7552, dtype: object"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4177b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    tweets = df['tweets']\n",
    "    corpus = ' '.join(word for word in tweets)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "433bd791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(str):\n",
    "    counts = dict()\n",
    "    words = str.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "corpus = get_corpus(train_df2)\n",
    "\n",
    "word_count_dict = word_count(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4cf712da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13685"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(word_count_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6bdc0940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>target</th>\n",
       "      <th>tweets</th>\n",
       "      <th>clean_keyword</th>\n",
       "      <th>clean_location</th>\n",
       "      <th>new</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>all_text</th>\n",
       "      <th>combined_tokens</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>filtered</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake m...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>our deeds are the reason of this earthquake m...</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "      <td>deed reason earthquake may allah forgive</td>\n",
       "      <td>['deed', 'reason', 'earthquake', 'may', 'allah...</td>\n",
       "      <td>['deed', 'reason', 'earthquake', 'may', 'allah...</td>\n",
       "      <td>['deed', 'reason', 'earthquake', 'may', 'allah...</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>forest fire near la range sask canada</td>\n",
       "      <td>forest fire near range sask canada</td>\n",
       "      <td>forest fire near range sask canada</td>\n",
       "      <td>['forest', 'fire', 'near', 'range', 'sask', 'c...</td>\n",
       "      <td>['forest', 'fire', 'near', 'range', 'sask', 'c...</td>\n",
       "      <td>['forest', 'fire', 'near', 'range', 'canada']</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents asked to shelter in place are b...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>all residents asked to shelter in place are b...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>resident ask shelter place notify officer evac...</td>\n",
       "      <td>['resident', 'ask', 'shelter', 'place', 'notif...</td>\n",
       "      <td>['resident', 'ask', 'shelter', 'place', 'notif...</td>\n",
       "      <td>['resident', 'ask', 'shelter', 'place', 'offic...</td>\n",
       "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>people receive wildfire evacuation order calif...</td>\n",
       "      <td>['people', 'receive', 'wildfire', 'evacuation'...</td>\n",
       "      <td>['people', 'receive', 'wildfire', 'evacuation'...</td>\n",
       "      <td>['people', 'receive', 'wildfire', 'evacuation'...</td>\n",
       "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>just got sent this photo from ruby alaska as ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>just got sent this photo from ruby alaska as ...</td>\n",
       "      <td>get send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>get send photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>['get', 'send', 'photo', 'ruby', 'alaska', 'sm...</td>\n",
       "      <td>['get', 'send', 'photo', 'ruby', 'alaska', 'sm...</td>\n",
       "      <td>['get', 'send', 'photo', 'ruby', 'alaska', 'sm...</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>7608</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant cranes holding a bridge collapse in...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>two giant cranes holding a bridge collapse in...</td>\n",
       "      <td>two giant crane hold bridge collapse nearby home</td>\n",
       "      <td>two giant crane hold bridge collapse nearby home</td>\n",
       "      <td>['two', 'giant', 'crane', 'hold', 'bridge', 'c...</td>\n",
       "      <td>['two', 'giant', 'crane', 'hold', 'bridge', 'c...</td>\n",
       "      <td>['two', 'giant', 'crane', 'hold', 'bridge', 'c...</td>\n",
       "      <td>[two, giant, cranes, holding, a, bridge, colla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>7609</td>\n",
       "      <td>1</td>\n",
       "      <td>the out of control wild fires in california e...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>the out of control wild fires in california e...</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "      <td>control wild fire california even northern par...</td>\n",
       "      <td>['control', 'wild', 'fire', 'california', 'eve...</td>\n",
       "      <td>['control', 'wild', 'fire', 'california', 'eve...</td>\n",
       "      <td>['control', 'wild', 'fire', 'california', 'eve...</td>\n",
       "      <td>[the, out, of, control, wild, fires, in, calif...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7549</th>\n",
       "      <td>7610</td>\n",
       "      <td>1</td>\n",
       "      <td>m utc km s of volcano hawaii</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>m utc km s of volcano hawaii</td>\n",
       "      <td>utc volcano hawaii</td>\n",
       "      <td>utc volcano hawaii</td>\n",
       "      <td>['utc', 'volcano', 'hawaii']</td>\n",
       "      <td>['utc', 'volcano', 'hawaii']</td>\n",
       "      <td>['utc', 'volcano', 'hawaii']</td>\n",
       "      <td>[m, utc, km, s, of, volcano, hawaii]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7550</th>\n",
       "      <td>7611</td>\n",
       "      <td>1</td>\n",
       "      <td>police investigating after an e bike collided...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>police investigating after an e bike collided...</td>\n",
       "      <td>police investigate bike collide car little por...</td>\n",
       "      <td>police investigate bike collide car little por...</td>\n",
       "      <td>['police', 'investigate', 'bike', 'collide', '...</td>\n",
       "      <td>['police', 'investigate', 'bike', 'collide', '...</td>\n",
       "      <td>['police', 'investigate', 'bike', 'collide', '...</td>\n",
       "      <td>[police, investigating, after, an, e, bike, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7551</th>\n",
       "      <td>7612</td>\n",
       "      <td>1</td>\n",
       "      <td>the latest more homes razed by northern calif...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>the latest more homes razed by northern calif...</td>\n",
       "      <td>late home raze northern california wildfire ab...</td>\n",
       "      <td>late home raze northern california wildfire ab...</td>\n",
       "      <td>['late', 'home', 'raze', 'northern', 'californ...</td>\n",
       "      <td>['late', 'home', 'raze', 'northern', 'californ...</td>\n",
       "      <td>['late', 'home', 'raze', 'northern', 'californ...</td>\n",
       "      <td>[the, latest, more, homes, razed, by, northern...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7552 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  target                                             tweets  \\\n",
       "0              0       1   our deeds are the reason of this earthquake m...   \n",
       "1              1       1              forest fire near la ronge sask canada   \n",
       "2              2       1   all residents asked to shelter in place are b...   \n",
       "3              3       1   people receive wildfires evacuation orders in...   \n",
       "4              4       1   just got sent this photo from ruby alaska as ...   \n",
       "...          ...     ...                                                ...   \n",
       "7547        7608       1   two giant cranes holding a bridge collapse in...   \n",
       "7548        7609       1   the out of control wild fires in california e...   \n",
       "7549        7610       1                      m utc km s of volcano hawaii    \n",
       "7550        7611       1   police investigating after an e bike collided...   \n",
       "7551        7612       1   the latest more homes razed by northern calif...   \n",
       "\n",
       "     clean_keyword clean_location  \\\n",
       "0                                   \n",
       "1                                   \n",
       "2                                   \n",
       "3                                   \n",
       "4                                   \n",
       "...            ...            ...   \n",
       "7547                                \n",
       "7548                                \n",
       "7549                                \n",
       "7550                                \n",
       "7551                                \n",
       "\n",
       "                                                    new  \\\n",
       "0      our deeds are the reason of this earthquake m...   \n",
       "1                 forest fire near la range sask canada   \n",
       "2      all residents asked to shelter in place are b...   \n",
       "3      people receive wildfires evacuation orders in...   \n",
       "4      just got sent this photo from ruby alaska as ...   \n",
       "...                                                 ...   \n",
       "7547   two giant cranes holding a bridge collapse in...   \n",
       "7548   the out of control wild fires in california e...   \n",
       "7549                      m utc km s of volcano hawaii    \n",
       "7550   police investigating after an e bike collided...   \n",
       "7551   the latest more homes razed by northern calif...   \n",
       "\n",
       "                                            clean_tweet  \\\n",
       "0              deed reason earthquake may allah forgive   \n",
       "1                    forest fire near range sask canada   \n",
       "2     resident ask shelter place notify officer evac...   \n",
       "3     people receive wildfire evacuation order calif...   \n",
       "4     get send photo ruby alaska smoke wildfire pour...   \n",
       "...                                                 ...   \n",
       "7547   two giant crane hold bridge collapse nearby home   \n",
       "7548  control wild fire california even northern par...   \n",
       "7549                                 utc volcano hawaii   \n",
       "7550  police investigate bike collide car little por...   \n",
       "7551  late home raze northern california wildfire ab...   \n",
       "\n",
       "                                               all_text  \\\n",
       "0              deed reason earthquake may allah forgive   \n",
       "1                    forest fire near range sask canada   \n",
       "2     resident ask shelter place notify officer evac...   \n",
       "3     people receive wildfire evacuation order calif...   \n",
       "4     get send photo ruby alaska smoke wildfire pour...   \n",
       "...                                                 ...   \n",
       "7547   two giant crane hold bridge collapse nearby home   \n",
       "7548  control wild fire california even northern par...   \n",
       "7549                                 utc volcano hawaii   \n",
       "7550  police investigate bike collide car little por...   \n",
       "7551  late home raze northern california wildfire ab...   \n",
       "\n",
       "                                        combined_tokens  \\\n",
       "0     ['deed', 'reason', 'earthquake', 'may', 'allah...   \n",
       "1     ['forest', 'fire', 'near', 'range', 'sask', 'c...   \n",
       "2     ['resident', 'ask', 'shelter', 'place', 'notif...   \n",
       "3     ['people', 'receive', 'wildfire', 'evacuation'...   \n",
       "4     ['get', 'send', 'photo', 'ruby', 'alaska', 'sm...   \n",
       "...                                                 ...   \n",
       "7547  ['two', 'giant', 'crane', 'hold', 'bridge', 'c...   \n",
       "7548  ['control', 'wild', 'fire', 'california', 'eve...   \n",
       "7549                       ['utc', 'volcano', 'hawaii']   \n",
       "7550  ['police', 'investigate', 'bike', 'collide', '...   \n",
       "7551  ['late', 'home', 'raze', 'northern', 'californ...   \n",
       "\n",
       "                                           tweet_tokens  \\\n",
       "0     ['deed', 'reason', 'earthquake', 'may', 'allah...   \n",
       "1     ['forest', 'fire', 'near', 'range', 'sask', 'c...   \n",
       "2     ['resident', 'ask', 'shelter', 'place', 'notif...   \n",
       "3     ['people', 'receive', 'wildfire', 'evacuation'...   \n",
       "4     ['get', 'send', 'photo', 'ruby', 'alaska', 'sm...   \n",
       "...                                                 ...   \n",
       "7547  ['two', 'giant', 'crane', 'hold', 'bridge', 'c...   \n",
       "7548  ['control', 'wild', 'fire', 'california', 'eve...   \n",
       "7549                       ['utc', 'volcano', 'hawaii']   \n",
       "7550  ['police', 'investigate', 'bike', 'collide', '...   \n",
       "7551  ['late', 'home', 'raze', 'northern', 'californ...   \n",
       "\n",
       "                                               filtered  \\\n",
       "0     ['deed', 'reason', 'earthquake', 'may', 'allah...   \n",
       "1         ['forest', 'fire', 'near', 'range', 'canada']   \n",
       "2     ['resident', 'ask', 'shelter', 'place', 'offic...   \n",
       "3     ['people', 'receive', 'wildfire', 'evacuation'...   \n",
       "4     ['get', 'send', 'photo', 'ruby', 'alaska', 'sm...   \n",
       "...                                                 ...   \n",
       "7547  ['two', 'giant', 'crane', 'hold', 'bridge', 'c...   \n",
       "7548  ['control', 'wild', 'fire', 'california', 'eve...   \n",
       "7549                       ['utc', 'volcano', 'hawaii']   \n",
       "7550  ['police', 'investigate', 'bike', 'collide', '...   \n",
       "7551  ['late', 'home', 'raze', 'northern', 'californ...   \n",
       "\n",
       "                                                 tokens  \n",
       "0     [our, deeds, are, the, reason, of, this, earth...  \n",
       "1         [forest, fire, near, la, ronge, sask, canada]  \n",
       "2     [all, residents, asked, to, shelter, in, place...  \n",
       "3     [people, receive, wildfires, evacuation, order...  \n",
       "4     [just, got, sent, this, photo, from, ruby, ala...  \n",
       "...                                                 ...  \n",
       "7547  [two, giant, cranes, holding, a, bridge, colla...  \n",
       "7548  [the, out, of, control, wild, fires, in, calif...  \n",
       "7549               [m, utc, km, s, of, volcano, hawaii]  \n",
       "7550  [police, investigating, after, an, e, bike, co...  \n",
       "7551  [the, latest, more, homes, razed, by, northern...  \n",
       "\n",
       "[7552 rows x 12 columns]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "446ec5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 106022 words in the entire training dataset\n",
      "There are 13683 unique words in the training dataset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# count number of words in corpus\n",
    "num_words = sum(word_dict[w] for w in word_dict)\n",
    "print(f'There are {num_words} words in the entire training dataset')\n",
    "\n",
    "# count number of unique words in corpus\n",
    "word_count_sorted = [(value, key) for key, value in word_dict.items()]\n",
    "word_count_sorted.sort()\n",
    "print(f'There are {len(word_count_sorted)} unique words in the training dataset')\n",
    "\n",
    "single_words_list= [k for k,v in word_dict.items() if int(v) <=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "187e2d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deeds',\n",
       " 'forgive',\n",
       " 'ronge',\n",
       " 'sask',\n",
       " 'notified',\n",
       " 'receive',\n",
       " 'ruby',\n",
       " 'pours',\n",
       " 'directions',\n",
       " 'afire',\n",
       " 'manitou',\n",
       " 'woods',\n",
       " 'fvck',\n",
       " 'count',\n",
       " 'bago',\n",
       " 'fruits',\n",
       " 'skiing',\n",
       " 'pasta',\n",
       " 'africanbaze',\n",
       " 'mufc',\n",
       " 'acquisitions',\n",
       " 'epl',\n",
       " 'inec',\n",
       " 'abia',\n",
       " 'barbados',\n",
       " 'bridgetown',\n",
       " 'elizabeth',\n",
       " 'superintende',\n",
       " 'nsfw',\n",
       " 'visiting',\n",
       " 'cfc',\n",
       " 'ancop',\n",
       " 'tita',\n",
       " 'vida',\n",
       " 'pumped',\n",
       " 'preaching',\n",
       " 'tracklist',\n",
       " 'retainers',\n",
       " 'brighton',\n",
       " 'superintendent',\n",
       " 'lanford',\n",
       " 'deliberately',\n",
       " 'noches',\n",
       " 'bestia',\n",
       " 'teammates',\n",
       " 'goodnight',\n",
       " 'gunners',\n",
       " 'kurds',\n",
       " 'trampling',\n",
       " 'turkmen',\n",
       " 'vandalized',\n",
       " 'offices',\n",
       " 'diyala',\n",
       " 'voortrekker',\n",
       " 'tambo',\n",
       " 'intl',\n",
       " 'skyline',\n",
       " 'kiss',\n",
       " 'lips',\n",
       " 'revel',\n",
       " 'wmv',\n",
       " 'farewell',\n",
       " 'gtx',\n",
       " 'wm',\n",
       " 'progressive',\n",
       " 'greetings',\n",
       " 'pens',\n",
       " 'publications',\n",
       " 'rene',\n",
       " 'jacinta',\n",
       " 'edit',\n",
       " 'mar',\n",
       " 'tinderbox',\n",
       " 'hood',\n",
       " 'buff',\n",
       " 'magnitude',\n",
       " 'testicles',\n",
       " 'clara',\n",
       " 'pkwy',\n",
       " 'personalinjury',\n",
       " 'advice',\n",
       " 'solicitor',\n",
       " 'otley',\n",
       " 'stlouis',\n",
       " 'caraccidentlawyer',\n",
       " 'speeding',\n",
       " 'tee',\n",
       " 'curry',\n",
       " 'herman',\n",
       " 'stephenson',\n",
       " 'overturned',\n",
       " 'rig',\n",
       " 'awareness',\n",
       " 'mooresville',\n",
       " 'iredell',\n",
       " 'ramp',\n",
       " 'pills',\n",
       " 'cabrillo',\n",
       " 'magellan',\n",
       " 'mir',\n",
       " 'congestion',\n",
       " 'pastor',\n",
       " 'rover',\n",
       " 'wished',\n",
       " 'spilt',\n",
       " 'mayonnaise',\n",
       " 'pissed',\n",
       " 'donnie',\n",
       " 'overturns',\n",
       " 'interstate',\n",
       " 'ashville',\n",
       " 'median',\n",
       " 'traveling',\n",
       " 'piner',\n",
       " 'horndale',\n",
       " 'yf',\n",
       " 'chandanee',\n",
       " 'magu',\n",
       " 'taxi',\n",
       " 'rammed',\n",
       " 'halfway',\n",
       " 'conf',\n",
       " 'eddy',\n",
       " 'wpd',\n",
       " 'willis',\n",
       " 'foreman',\n",
       " 'aashiqui',\n",
       " 'actress',\n",
       " 'anu',\n",
       " 'aggarwal',\n",
       " 'suffield',\n",
       " 'backup',\n",
       " 'langtree',\n",
       " 'alternate',\n",
       " 'determine',\n",
       " 'financially',\n",
       " 'hagerstown',\n",
       " 'whag',\n",
       " 'marinading',\n",
       " 'bahrain',\n",
       " 'disciplinary',\n",
       " 'measures',\n",
       " 'nganga',\n",
       " 'scuf',\n",
       " 'cya',\n",
       " 'painful',\n",
       " 'roger',\n",
       " 'bannister',\n",
       " 'dwight',\n",
       " 'eisenhower',\n",
       " 'remembers',\n",
       " 'schulz',\n",
       " 'xb',\n",
       " 'harder',\n",
       " 'triumph',\n",
       " 'paine',\n",
       " 'spoiled',\n",
       " 'clay',\n",
       " 'pigeon',\n",
       " 'coaster',\n",
       " 'disclaimer',\n",
       " 'wisdom',\n",
       " 'bonus',\n",
       " 'habits',\n",
       " 'improve',\n",
       " 'lifehacks',\n",
       " 'wiedemer',\n",
       " 'http',\n",
       " 'silverwood',\n",
       " 'difficulties',\n",
       " 'osteen',\n",
       " 'belief',\n",
       " 'praise',\n",
       " 'wdyouth',\n",
       " 'biblestudy',\n",
       " 'interrupt',\n",
       " 'bernard',\n",
       " 'shaw',\n",
       " 'oyster',\n",
       " 'andrew',\n",
       " 'carnegie',\n",
       " 'hmu',\n",
       " 'strict',\n",
       " 'liability',\n",
       " 'component',\n",
       " 'aviation',\n",
       " 'wedn',\n",
       " 'aww',\n",
       " 'cuties',\n",
       " 'osama',\n",
       " 'ironic',\n",
       " 'mhmm',\n",
       " 'cessna',\n",
       " 'ocampo',\n",
       " 'coahuila',\n",
       " 'mbataweel',\n",
       " 'binladen',\n",
       " 'coworker',\n",
       " 'nudes',\n",
       " 'unbelievably',\n",
       " 'aeroplane',\n",
       " 'freaky',\n",
       " 'usama',\n",
       " 'ladins',\n",
       " 'naturally',\n",
       " 'nicole',\n",
       " 'fletcher',\n",
       " 'turbo',\n",
       " 'boing',\n",
       " 'statistically',\n",
       " 'colombia',\n",
       " 'vicinity',\n",
       " 'airports',\n",
       " 'begging',\n",
       " 'rod',\n",
       " 'kiai',\n",
       " 'ambulances',\n",
       " 'lorry',\n",
       " 'ne',\n",
       " 'welcomes',\n",
       " 'travelling',\n",
       " 'aberystwyth',\n",
       " 'shrewsbury',\n",
       " 'halt',\n",
       " 'shrews',\n",
       " 'nanotech',\n",
       " 'clots',\n",
       " 'hella',\n",
       " 'mosh',\n",
       " 'pits',\n",
       " 'justsaying',\n",
       " 'randomthought',\n",
       " 'dna',\n",
       " 'hahahah',\n",
       " 'nissan',\n",
       " 'minimum',\n",
       " 'wage',\n",
       " 'paramedics',\n",
       " 'johns',\n",
       " 'hatzolah',\n",
       " 'responding',\n",
       " 'lesotho',\n",
       " 'standardised',\n",
       " 'clinical',\n",
       " 'trunks',\n",
       " 'freiza',\n",
       " 'cleanest',\n",
       " 'petebests',\n",
       " 'dessicated',\n",
       " 'kneel',\n",
       " 'sundowns',\n",
       " 'celtic',\n",
       " 'improvement',\n",
       " 'mizzou',\n",
       " 'muschamp',\n",
       " 'compete',\n",
       " 'bama',\n",
       " 'abs',\n",
       " 'mba',\n",
       " 'careen',\n",
       " 'ovm',\n",
       " 'luka',\n",
       " 'alois',\n",
       " 'trancy',\n",
       " 'fella',\n",
       " 'entirely',\n",
       " 'promised',\n",
       " 'nukes',\n",
       " 'armenians',\n",
       " 'instantly',\n",
       " 'annihilate',\n",
       " 'tryouts',\n",
       " 'minus',\n",
       " 'toenail',\n",
       " 'oryx',\n",
       " 'symbol',\n",
       " 'peninsula',\n",
       " 'bucs',\n",
       " 'nights',\n",
       " 'philip',\n",
       " 'domain',\n",
       " 'sophistication',\n",
       " 'closely',\n",
       " 'zr',\n",
       " 'nf',\n",
       " 'judas',\n",
       " 'priest',\n",
       " 'scorpions',\n",
       " 'astonishing',\n",
       " 'gig',\n",
       " 'skipping',\n",
       " 'fant',\n",
       " 'stic',\n",
       " 'bummer',\n",
       " 'explaining',\n",
       " 'evolved',\n",
       " 'godlike',\n",
       " 'cech',\n",
       " 'keegan',\n",
       " 'imperfect',\n",
       " 'lesnar',\n",
       " 'cena',\n",
       " 'summerslam',\n",
       " 'brock',\n",
       " 'damascus',\n",
       " 'grinds',\n",
       " 'alloosh',\n",
       " 'manure',\n",
       " 'forthright',\n",
       " 'coma',\n",
       " 'kebab',\n",
       " 'tahini',\n",
       " 'pickles',\n",
       " 'fries',\n",
       " 'simmons',\n",
       " 'camden',\n",
       " 'handsome',\n",
       " 'juanny',\n",
       " 'beisbol',\n",
       " 'lgm',\n",
       " 'sold',\n",
       " 'evildead',\n",
       " 'civilization',\n",
       " 'annihilating',\n",
       " 'quarterstaff',\n",
       " 'aliens',\n",
       " 'exterminate',\n",
       " 'stardate',\n",
       " 'planetary',\n",
       " 'signatures',\n",
       " 'punished',\n",
       " 'contributor',\n",
       " 'reject',\n",
       " 'misguided',\n",
       " 'prophets',\n",
       " 'imprison',\n",
       " 'fueling',\n",
       " 'kasich',\n",
       " 'tanto',\n",
       " 'bookslast',\n",
       " 'dante',\n",
       " 'johnny',\n",
       " 'fukurodani',\n",
       " 'bokuto',\n",
       " 'ppor',\n",
       " 'jocelyn',\n",
       " 'janenelson',\n",
       " 'scifi',\n",
       " 'adaptation',\n",
       " 'optioned',\n",
       " 'sciencefiction',\n",
       " 'internetradio',\n",
       " 'collegeradi',\n",
       " 'stages',\n",
       " 'interpretation',\n",
       " 'romeo',\n",
       " 'juliet',\n",
       " 'warmbodies',\n",
       " 'fittscott',\n",
       " 'popularmmos',\n",
       " 'enormous',\n",
       " 'candylit',\n",
       " 'sarumi',\n",
       " 'impending',\n",
       " 'biblical',\n",
       " 'grizzly',\n",
       " 'peak',\n",
       " 'dystopian',\n",
       " 'cairo',\n",
       " 'xv',\n",
       " 'pierc',\n",
       " 'geek',\n",
       " 'hesse',\n",
       " 'zombies',\n",
       " 'boxing',\n",
       " 'posters',\n",
       " 'reminded',\n",
       " 'freshly',\n",
       " 'coiffed',\n",
       " 'yss',\n",
       " 'russaky',\n",
       " 'prod',\n",
       " 'ouvindo',\n",
       " 'faction',\n",
       " 'lith',\n",
       " 'voodoo',\n",
       " 'seduction',\n",
       " 'astrology',\n",
       " 'rtrrt',\n",
       " 'lotz',\n",
       " 'pla',\n",
       " 'hsu',\n",
       " 'hao',\n",
       " 'flawless',\n",
       " 'affleck',\n",
       " 'eonlinechat',\n",
       " 'coat',\n",
       " 'worn',\n",
       " 'certainty',\n",
       " 'spying',\n",
       " 'hidden',\n",
       " 'nsa',\n",
       " 'software',\n",
       " 'hyider',\n",
       " 'ghost',\n",
       " 'fighterdena',\n",
       " 'sketch',\n",
       " 'startrek',\n",
       " 'tos',\n",
       " 'coefficient',\n",
       " 'plz',\n",
       " 'paid',\n",
       " 'income',\n",
       " 'bruce',\n",
       " 'auction',\n",
       " 'goof',\n",
       " 'guild',\n",
       " 'saunders',\n",
       " 'gaining',\n",
       " 'preppers',\n",
       " 'doomsday',\n",
       " 'shtf',\n",
       " 'preppertalk',\n",
       " 'prepper',\n",
       " 'eep',\n",
       " 'ahamedis',\n",
       " 'messiah',\n",
       " 'dajaal',\n",
       " 'gog',\n",
       " 'magog',\n",
       " 'ethics',\n",
       " 'patron',\n",
       " 'blueprint',\n",
       " 'impressive',\n",
       " 'european',\n",
       " 'fitba',\n",
       " 'cufi',\n",
       " 'jews',\n",
       " 'convert',\n",
       " 'preseasonworkouts',\n",
       " 'lee',\n",
       " 'tories',\n",
       " 'prove',\n",
       " 'begun',\n",
       " 'tren',\n",
       " 'craig',\n",
       " 'brics',\n",
       " 'goofballs',\n",
       " 'comin',\n",
       " 'aberdeen',\n",
       " 'sosfam',\n",
       " 'whitewalkers',\n",
       " 'intrigued',\n",
       " 'lzktjnox',\n",
       " 'hosts',\n",
       " 'reconnect',\n",
       " 'fathers',\n",
       " 'infantry',\n",
       " 'mens',\n",
       " 'lume',\n",
       " 'dial',\n",
       " 'analog',\n",
       " 'quartz',\n",
       " 'wrist',\n",
       " 'nylon',\n",
       " 'fabric',\n",
       " 'victorinox',\n",
       " 'mop',\n",
       " 'ayesha',\n",
       " 'ko',\n",
       " 'mun',\n",
       " 'jawab',\n",
       " 'ki',\n",
       " 'verano',\n",
       " 'britney',\n",
       " 'lana',\n",
       " 'rey',\n",
       " 'stony',\n",
       " 'felons',\n",
       " 'rejects',\n",
       " 'appoints',\n",
       " 'brig',\n",
       " 'gen',\n",
       " 'kaiser',\n",
       " 'commission',\n",
       " 'mrc',\n",
       " 'killings',\n",
       " 'spokane',\n",
       " 'struggling',\n",
       " 'attend',\n",
       " 'organized',\n",
       " 'protesting',\n",
       " 'familia',\n",
       " 'lesbians',\n",
       " 'burglary',\n",
       " 'cracks',\n",
       " 'mariah',\n",
       " 'shira',\n",
       " 'banki',\n",
       " 'miprv',\n",
       " 'latimes',\n",
       " 'prayed',\n",
       " 'treated',\n",
       " 'faked',\n",
       " 'citation',\n",
       " 'possesion',\n",
       " 'decriminalized',\n",
       " 'substance',\n",
       " 'facing',\n",
       " 'forgets',\n",
       " 'bloor',\n",
       " 'ossington',\n",
       " 'mattress',\n",
       " 'northumberland',\n",
       " 'cbcto',\n",
       " 'vegan',\n",
       " 'torching',\n",
       " 'strictly',\n",
       " 'unite',\n",
       " 'headlines',\n",
       " 'nightbeat',\n",
       " 'removing',\n",
       " 'sought',\n",
       " 'trick',\n",
       " 'spotlight',\n",
       " 'paradise',\n",
       " 'wnia',\n",
       " 'gospel',\n",
       " 'melted',\n",
       " 'cube',\n",
       " 'arsonists',\n",
       " 'plastics',\n",
       " 'adelaide',\n",
       " 'adl',\n",
       " 'hotboy',\n",
       " 'zodiac',\n",
       " 'trey',\n",
       " 'dupree',\n",
       " 'sparkz',\n",
       " 'beatz',\n",
       " 'chuck',\n",
       " 'francisco',\n",
       " 'jokin',\n",
       " 'cracking',\n",
       " 'mink',\n",
       " 'earl',\n",
       " 'elliott',\n",
       " 'pleaded',\n",
       " 'trusting',\n",
       " 'inviting',\n",
       " 'brigade',\n",
       " 'vigilent',\n",
       " 'liberties',\n",
       " 'nativehuman',\n",
       " 'myreligion',\n",
       " 'inspiring',\n",
       " 'rediscover',\n",
       " 'fantabulous',\n",
       " 'stans',\n",
       " 'copies',\n",
       " 'scares',\n",
       " 'versions',\n",
       " 'governments',\n",
       " 'isil',\n",
       " 'funniest',\n",
       " 'yeda',\n",
       " 'yakub',\n",
       " 'waziristan',\n",
       " 'mint',\n",
       " 'delhi',\n",
       " 'hospitals',\n",
       " 'smb',\n",
       " 'relay',\n",
       " 'steals',\n",
       " 'credentials',\n",
       " 'goat',\n",
       " 'graveyard',\n",
       " 'handside',\n",
       " 'volleyball',\n",
       " 'simulation',\n",
       " 'notley',\n",
       " 'tactful',\n",
       " 'direct',\n",
       " 'premier',\n",
       " 'ableg',\n",
       " 'cdnpoli',\n",
       " 'assailant',\n",
       " 'shud',\n",
       " 'oth',\n",
       " 'contries',\n",
       " 'alien',\n",
       " 'dhs',\n",
       " 'offenses',\n",
       " 'strongly',\n",
       " 'condemn',\n",
       " 'ary',\n",
       " 'cowardly',\n",
       " 'simply',\n",
       " 'mentality',\n",
       " 'ignoranceshe',\n",
       " 'latinoand',\n",
       " 'benothing',\n",
       " 'morebut',\n",
       " 'prevention',\n",
       " 'secondhand',\n",
       " 'dayton',\n",
       " 'cyber',\n",
       " 'vita',\n",
       " 'infowars',\n",
       " 'nwo',\n",
       " 'pugprobs',\n",
       " 'gunfight',\n",
       " 'frail',\n",
       " 'cis',\n",
       " 'offended',\n",
       " 'transgendered',\n",
       " 'helicopters',\n",
       " 'completed',\n",
       " 'exercises',\n",
       " 'scholars',\n",
       " 'imprisoning',\n",
       " 'telnet',\n",
       " 'streamyx',\n",
       " 'kane',\n",
       " 'cab',\n",
       " 'troll',\n",
       " 'pol',\n",
       " 'rivals',\n",
       " 'abused',\n",
       " 'loosers',\n",
       " 'questioning',\n",
       " 'empathy',\n",
       " 'kelly',\n",
       " 'osbourne',\n",
       " 'remark',\n",
       " 'latinos',\n",
       " 'ai',\n",
       " 'calmly',\n",
       " 'lvl',\n",
       " 'earned',\n",
       " 'satoshis',\n",
       " 'robotcoingame',\n",
       " 'bitcoin',\n",
       " 'uranium',\n",
       " 'digging',\n",
       " 'blues',\n",
       " 'roadhouse',\n",
       " 'reebok',\n",
       " 'xl',\n",
       " 'hockey',\n",
       " 'tix',\n",
       " 'fury',\n",
       " 'xvii',\n",
       " 'kings',\n",
       " 'mgm',\n",
       " 'wrote',\n",
       " 'greatly',\n",
       " 'roy',\n",
       " 'spx',\n",
       " 'finite',\n",
       " 'recreates',\n",
       " 'clever',\n",
       " 'akito',\n",
       " 'roses',\n",
       " 'frothy',\n",
       " 'gyp',\n",
       " 'weddinghour',\n",
       " 'preseason',\n",
       " 'scotiabank',\n",
       " 'saddledome',\n",
       " 'catechize',\n",
       " 'confidential',\n",
       " 'respects',\n",
       " 'creating',\n",
       " 'poplar',\n",
       " 'ordained',\n",
       " 'tumbling',\n",
       " 'spurgeon',\n",
       " 'punk',\n",
       " 'performance',\n",
       " 'chip',\n",
       " 'saver',\n",
       " 'blazer',\n",
       " 'sniping',\n",
       " 'chevrolet',\n",
       " 'ltz',\n",
       " 'chiasson',\n",
       " 'sens',\n",
       " 'rudd',\n",
       " 'emile',\n",
       " 'hirsch',\n",
       " 'gordon',\n",
       " 'filmmakers',\n",
       " 'entretenimento',\n",
       " 'neal',\n",
       " 'rigga',\n",
       " 'drafted',\n",
       " 'overall',\n",
       " 'rappers',\n",
       " 'flex',\n",
       " 'awake',\n",
       " 'jerseys',\n",
       " 'nowplay',\n",
       " 'lt',\n",
       " 'pickup',\n",
       " 'snowflake',\n",
       " 'jedi',\n",
       " 'hasbro',\n",
       " 'bull',\n",
       " 'colonel',\n",
       " 'letter',\n",
       " 'rotten',\n",
       " 'tomatoes',\n",
       " 'tanks',\n",
       " 'assistant',\n",
       " 'arti',\n",
       " 'kaboom',\n",
       " 'kus',\n",
       " 'petersen',\n",
       " 'bowhunting',\n",
       " 'bows',\n",
       " 'throwback',\n",
       " 'kelby',\n",
       " 'tomlinson',\n",
       " 'mild',\n",
       " 'mannered',\n",
       " 'baseman',\n",
       " 'metropolitan',\n",
       " 'rbi',\n",
       " 'giants',\n",
       " 'saturn',\n",
       " 'animations',\n",
       " 'motivation',\n",
       " 'item',\n",
       " 'phantasmal',\n",
       " 'cummerbund',\n",
       " 'stormtrooper',\n",
       " 'engaged',\n",
       " 'somme',\n",
       " 'avengers',\n",
       " 'sigh',\n",
       " 'realization',\n",
       " 'span',\n",
       " 'goats',\n",
       " 'pine',\n",
       " 'descendants',\n",
       " 'gallipoli',\n",
       " 'gary',\n",
       " 'busey',\n",
       " 'dixie',\n",
       " 'fiddle',\n",
       " 'celebration',\n",
       " 'sequence',\n",
       " 'busines',\n",
       " 'amp',\n",
       " 'commerce',\n",
       " 'subcommittee',\n",
       " 'oversight',\n",
       " 'htt',\n",
       " 'mobile',\n",
       " 'alisonannyoung',\n",
       " 'exclusive',\n",
       " 'researchers',\n",
       " 'select',\n",
       " 'corp',\n",
       " 'nyse',\n",
       " 'fdx',\n",
       " 'packages',\n",
       " 'chronicle',\n",
       " 'microbes',\n",
       " 'orchardalley',\n",
       " 'les',\n",
       " 'manufactured',\n",
       " 'repression',\n",
       " 'gardens',\n",
       " 'biz',\n",
       " 'journal',\n",
       " 'bioter',\n",
       " 'trucking',\n",
       " 'wxia',\n",
       " 'biolab',\n",
       " 'facilities',\n",
       " 'dumbfounded',\n",
       " 'fema',\n",
       " 'targeted',\n",
       " 'lithium',\n",
       " 'extensive',\n",
       " 'pathogen',\n",
       " 'handling',\n",
       " 'firepower',\n",
       " 'resource',\n",
       " 'automation',\n",
       " 'infectious',\n",
       " 'diseases',\n",
       " 'pox',\n",
       " 'infectiousdiseases',\n",
       " 'superbug',\n",
       " 'biolabs',\n",
       " 'epidemics',\n",
       " 'biosurveillance',\n",
       " 'outbreaks',\n",
       " 'homeland',\n",
       " 'creation',\n",
       " 'workforce',\n",
       " 'inequality',\n",
       " 'horowitz',\n",
       " 'vaccines',\n",
       " 'transgender',\n",
       " 'fold',\n",
       " 'difficult',\n",
       " 'dispersed',\n",
       " 'volunteers',\n",
       " 'participate',\n",
       " 'simulating',\n",
       " 'newspaper',\n",
       " 'organisms',\n",
       " 'hhs',\n",
       " 'selects',\n",
       " 'regional',\n",
       " 'centers',\n",
       " 'sys',\n",
       " 'clergyforced',\n",
       " 'younger',\n",
       " 'grossly',\n",
       " 'disfigured',\n",
       " 'bioterroris',\n",
       " 'possibility',\n",
       " 'mysteries',\n",
       " 'therapies',\n",
       " 'technologies',\n",
       " 'sexuality',\n",
       " 'diagnosis',\n",
       " 'digitalhealth',\n",
       " 'hcsm',\n",
       " 'ugliness',\n",
       " 'frat',\n",
       " 'bioterrorismi',\n",
       " 'tolewant',\n",
       " 'evade',\n",
       " 'prosecute',\n",
       " 'kidnap',\n",
       " 'agreements',\n",
       " 'allay',\n",
       " 'glanders',\n",
       " 'equestrian',\n",
       " 'craving',\n",
       " 'raisinfingers',\n",
       " 'hammond',\n",
       " 'listing',\n",
       " 'amazin',\n",
       " 'dey',\n",
       " 'dedication',\n",
       " 'virtual',\n",
       " 'listings',\n",
       " 'fir',\n",
       " 'cannon',\n",
       " 'dorrie',\n",
       " 'caruana',\n",
       " 'pendleton',\n",
       " 'horno',\n",
       " 'apt',\n",
       " 'balcony',\n",
       " 'alrighty',\n",
       " 'slave',\n",
       " 'trade',\n",
       " 'fade',\n",
       " 'rages',\n",
       " 'cary',\n",
       " 'condo',\n",
       " 'babes',\n",
       " 'lan',\n",
       " 'skywars',\n",
       " 'que',\n",
       " 'hack',\n",
       " 'flechadas',\n",
       " 'sinistras',\n",
       " 'dems',\n",
       " 'interrogation',\n",
       " 'facility',\n",
       " 'rapper',\n",
       " 'thisispublichealth',\n",
       " 'jackass',\n",
       " 'stuffin',\n",
       " 'drew',\n",
       " 'socialmedia',\n",
       " 'personalize',\n",
       " 'customer',\n",
       " 'shout',\n",
       " 'sothwest',\n",
       " 'beginnings',\n",
       " 'vela',\n",
       " 'mixtape',\n",
       " 'goin',\n",
       " 'bedrooms',\n",
       " 'baths',\n",
       " 'daem',\n",
       " 'smooth',\n",
       " 'asf',\n",
       " 'artiste',\n",
       " 'coastdjs',\n",
       " 'jiwonle',\n",
       " 'banger',\n",
       " 'bright',\n",
       " 'weddings',\n",
       " 'bez',\n",
       " 'vibez',\n",
       " 'listenlive',\n",
       " 'montgomery',\n",
       " 'rejected',\n",
       " 'slogan',\n",
       " 'transcend',\n",
       " 'trail',\n",
       " 'diversified',\n",
       " 'dmpl',\n",
       " 'cameo',\n",
       " 'stealth',\n",
       " 'silenced',\n",
       " 'pacquiao',\n",
       " 'marquez',\n",
       " 'unfilled',\n",
       " 'du',\n",
       " 'rv',\n",
       " 'od',\n",
       " 'radios',\n",
       " 'stoponesounds',\n",
       " 'airwaves',\n",
       " 'dynasty',\n",
       " 'hinatobot',\n",
       " 'shouout',\n",
       " 'vocals',\n",
       " 'xleak',\n",
       " 'festac',\n",
       " 'delta',\n",
       " 'sed',\n",
       " 'bbm',\n",
       " 'leaked',\n",
       " 'pictures',\n",
       " 'temperature',\n",
       " 'harmony',\n",
       " 'latin',\n",
       " 'reddish',\n",
       " 'colored',\n",
       " 'dummies',\n",
       " 'pronouncing',\n",
       " 'beating',\n",
       " 'escorts',\n",
       " 'gfe',\n",
       " 'dubai',\n",
       " 'missy',\n",
       " 'suited',\n",
       " 'summertime',\n",
       " 'yee',\n",
       " 'haw',\n",
       " 'magical',\n",
       " 'elwoods',\n",
       " 'doug',\n",
       " 'ch',\n",
       " 'pl',\n",
       " 'satin',\n",
       " 'etisalat',\n",
       " 'manitoba',\n",
       " 'tecno',\n",
       " 'ime',\n",
       " 'slapping',\n",
       " 'showdown',\n",
       " 'unpacked',\n",
       " 'archetype',\n",
       " 'grounded',\n",
       " 'readiness',\n",
       " 'fp',\n",
       " 'oj',\n",
       " 'nugget',\n",
       " 'slit',\n",
       " 'throat',\n",
       " 'apologize',\n",
       " 'landolina',\n",
       " 'realestate',\n",
       " 'typewriter',\n",
       " 'gunk',\n",
       " 'hugs',\n",
       " 'takis',\n",
       " 'rubbing',\n",
       " 'madara',\n",
       " 'tak',\n",
       " 'sedar',\n",
       " 'waited',\n",
       " 'cams',\n",
       " 'concrete',\n",
       " 'liberal',\n",
       " 'deadpool',\n",
       " 'wears',\n",
       " 'suit',\n",
       " 'defs',\n",
       " 'wannabe',\n",
       " 'artist',\n",
       " 'instincts',\n",
       " 'scratches',\n",
       " 'stefano',\n",
       " 'pileup',\n",
       " 'rave',\n",
       " ...]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9c777fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7385"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(single_words_list )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476305c",
   "metadata": {},
   "source": [
    "## create column containing combined column text & tokens columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "0a220324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(df):\n",
    "    df['clean_keyword'] = df.clean_keyword.astype(str)\n",
    "    df['clean_keyword'] = df.clean_keyword.str.lower()\n",
    "    df['clean_keyword'] = df.clean_keyword.str.replace(r'\\bnan\\b', r'')\n",
    "    df['clean_keyword'] = df.clean_keyword.str.replace(r' ', r'_')\n",
    "    df['clean_keyword'] = df.clean_keyword.str.strip()\n",
    "    df['clean_location'] = df.clean_location.astype(str)\n",
    "    df['clean_location'] = df.clean_location.str.lower()\n",
    "    df['clean_location'] = df.clean_location.str.strip()\n",
    "    df['clean_location'] = df.clean_location.str.replace(r'\\bnan\\b', r'')\n",
    "    df['clean_location'] = df.clean_location.str.replace(r' ', r'_')\n",
    "    #df['tweets'] = df.tweets.astype(str)\n",
    "    df['tokens'] = df.tweets.apply(nltk.word_tokenize)\n",
    "    #df['combined_tokens'] = df.combined_tokens.apply(lambda row: row.split())\n",
    "    # \n",
    "    new_df = df\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cca63",
   "metadata": {},
   "source": [
    "## check single occurence words again/remove words used only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "319d2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_use_words(tokens):\n",
    "    # function to remove stopwords from text\n",
    "    stop_list = filtered_remove \n",
    "    new_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_list:\n",
    "            new_tokens.append(word)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4a3f8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 106022 words in the entire training dataset\n",
      "There are 13683 unique words in the training dataset\n"
     ]
    }
   ],
   "source": [
    "tweets = train_df.tokens\n",
    "word_dict = Counter(tweets.sum())\n",
    "# count number of words in corpus\n",
    "num_words = sum(word_dict[w] for w in word_dict)\n",
    "print(f'There are {num_words} words in the entire training dataset')\n",
    "\n",
    "# count number of unique words in corpus\n",
    "word_count_sorted = [(value, key) for key, value in word_dict.items()]\n",
    "word_count_sorted.sort()\n",
    "print(f'There are {len(word_count_sorted)} unique words in the training dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dab3d",
   "metadata": {},
   "source": [
    "## process  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1ad511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrub_data_to_df(df)\n",
    "df.to_csv('train_dn_pt.csv')\n",
    "spell_check_rare(df)\n",
    "df.to_csv('train_scr_pt.csv')\n",
    "#test_df['clean_tweet'] = test_df['new'].apply(tokenize_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ea116c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "scc = spell_check_compound(df)\n",
    "scc.to_csv('train_sc_compound.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e37379b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "scc = pd.read_csv('train_sc_compound.csv')\n",
    "df = pd.read_csv('train_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "9e902b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   1,   11,   33, ..., 7530, 7534, 7541], dtype=int64),)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "idx = np.where((train_df['new']!=train_df['tweets']) )\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "b4ff4afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1916"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = train_df.loc[idx]\n",
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "89f0392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-159-4ab27c84be16>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_keyword'] = df.clean_keyword.str.replace(r'\\bnan\\b', r'')\n",
      "<ipython-input-159-4ab27c84be16>:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_location'] = df.clean_location.str.replace(r'\\bnan\\b', r'')\n"
     ]
    }
   ],
   "source": [
    "train_df2 = combine_columns(scc)\n",
    "train_tweets2 = scc.tokens\n",
    "train_word_dict2 = Counter(train_tweets2.sum())\n",
    "filtered_mono2 = dict(filter(lambda elem: elem[1] == 1, train_word_dict2.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "132e3255",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-159-4ab27c84be16>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_keyword'] = df.clean_keyword.str.replace(r'\\bnan\\b', r'')\n",
      "<ipython-input-159-4ab27c84be16>:10: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_location'] = df.clean_location.str.replace(r'\\bnan\\b', r'')\n"
     ]
    }
   ],
   "source": [
    "train_df = combine_columns(df)\n",
    "train_tweets = train_df.tokens\n",
    "train_word_dict = Counter(train_tweets.sum())\n",
    "filtered_mono = dict(filter(lambda elem: elem[1] == 1, train_word_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "59bfebb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_mono == filtered_mono2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "57076d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrub_data_to_df(test_df)\n",
    "test_df.to_csv('test_dn_pt.csv')\n",
    "spell_check_rare(test_df)\n",
    "test_df.to_csv('test_scr_pt.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "0c9b331a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-169-1a87af0fd212>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_keyword'] = df.clean_keyword.str.replace(r'\\bnan\\b', r'')\n",
      "<ipython-input-169-1a87af0fd212>:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['clean_location'] = df.clean_location.str.replace(r'\\bnan\\b', r'')\n"
     ]
    }
   ],
   "source": [
    "test_df = combine_columns(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1eb772d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = test_df.tweet_tokens\n",
    "test_word_dict = Counter(test_tweets.sum())\n",
    "filtered_remove = dict(filter(lambda elem: elem[1] == 1, test_word_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "ae803b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['filtered'] = test_df.tweet_tokens.apply(remove_single_use_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "03f6bfc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>tweets</th>\n",
       "      <th>clean_keyword</th>\n",
       "      <th>clean_location</th>\n",
       "      <th>new</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>all_text</th>\n",
       "      <th>combined_tokens</th>\n",
       "      <th>tweet_tokens</th>\n",
       "      <th>filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>just happened a terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>happen terrible car crash</td>\n",
       "      <td>[happen, terrible, car, crash]</td>\n",
       "      <td>[happen, terrible, car, crash]</td>\n",
       "      <td>[happen, terrible, car, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['#earthquake']</td>\n",
       "      <td>heard about earthquake is different cities st...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>heard about earthquake is different cities st...</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "      <td>heard earthquake different city stay safe ever...</td>\n",
       "      <td>[heard, earthquake, different, city, stay, saf...</td>\n",
       "      <td>[heard, earthquake, different, city, stay, saf...</td>\n",
       "      <td>[heard, earthquake, different, city, stay, saf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "      <td>forest fire spot pond goose flee across street...</td>\n",
       "      <td>forest fire spot pond goose flee across street...</td>\n",
       "      <td>[forest, fire, spot, pond, goose, flee, across...</td>\n",
       "      <td>[forest, fire, spot, pond, goose, flee, across...</td>\n",
       "      <td>[forest, fire, spot, pond, across, street, save]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>['#Spokane', '#wildfires']</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>apocalypse light spokane wildfire</td>\n",
       "      <td>apocalypse light spokane wildfire</td>\n",
       "      <td>[apocalypse, light, spokane, wildfire]</td>\n",
       "      <td>[apocalypse, light, spokane, wildfire]</td>\n",
       "      <td>[apocalypse, light, wildfire]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>[]</td>\n",
       "      <td>typhoon soudelor kills in china and taiwan</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>typhoon soudelor kills in china and taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>typhoon soudelor kill china taiwan</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "      <td>[typhoon, soudelor, kill, china, taiwan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>[]</td>\n",
       "      <td>earthquake safety los angeles safety fasteners...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>earthquake safety los angeles safety fasteners...</td>\n",
       "      <td>earthquake safety los angeles safety fastener</td>\n",
       "      <td>earthquake safety los angeles safety fastener</td>\n",
       "      <td>[earthquake, safety, los, angeles, safety, fas...</td>\n",
       "      <td>[earthquake, safety, los, angeles, safety, fas...</td>\n",
       "      <td>[earthquake, safety, los, angeles, safety]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>[]</td>\n",
       "      <td>storm in rhode island worse than last hurrica...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>storm in rhode island worse than last hurrica...</td>\n",
       "      <td>storm rhode island bad last hurricane city oth...</td>\n",
       "      <td>storm rhode island bad last hurricane city oth...</td>\n",
       "      <td>[storm, rhode, island, bad, last, hurricane, c...</td>\n",
       "      <td>[storm, rhode, island, bad, last, hurricane, c...</td>\n",
       "      <td>[storm, rhode, island, bad, last, hurricane, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>[]</td>\n",
       "      <td>green line derailment in chicago</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>green line derailment in chicago</td>\n",
       "      <td>green line derailment chicago</td>\n",
       "      <td>green line derailment chicago</td>\n",
       "      <td>[green, line, derailment, chicago]</td>\n",
       "      <td>[green, line, derailment, chicago]</td>\n",
       "      <td>[green, line, derailment, chicago]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>[]</td>\n",
       "      <td>meg issues hazardous weather outlook hwo</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>meg issues hazardous weather outlook hwo</td>\n",
       "      <td>meg issue hazardous weather outlook hwo</td>\n",
       "      <td>meg issue hazardous weather outlook hwo</td>\n",
       "      <td>[meg, issue, hazardous, weather, outlook, hwo]</td>\n",
       "      <td>[meg, issue, hazardous, weather, outlook, hwo]</td>\n",
       "      <td>[meg, issue, hazardous, weather, outlook, hwo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>['#CityofCalgary', '#yycstorm']</td>\n",
       "      <td>cityof calgary has activated it is municipal ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>city of calgary has activated it is municipal...</td>\n",
       "      <td>city calgary activate municipal emergency plan...</td>\n",
       "      <td>city calgary activate municipal emergency plan...</td>\n",
       "      <td>[city, calgary, activate, municipal, emergency...</td>\n",
       "      <td>[city, calgary, activate, municipal, emergency...</td>\n",
       "      <td>[city, calgary, activate, municipal, emergency...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                         hashtags  \\\n",
       "0         0                               []   \n",
       "1         2                  ['#earthquake']   \n",
       "2         3                               []   \n",
       "3         9       ['#Spokane', '#wildfires']   \n",
       "4        11                               []   \n",
       "...     ...                              ...   \n",
       "3258  10861                               []   \n",
       "3259  10865                               []   \n",
       "3260  10868                               []   \n",
       "3261  10874                               []   \n",
       "3262  10875  ['#CityofCalgary', '#yycstorm']   \n",
       "\n",
       "                                                 tweets clean_keyword  \\\n",
       "0                    just happened a terrible car crash                 \n",
       "1      heard about earthquake is different cities st...                 \n",
       "2     there is a forest fire at spot pond geese are ...                 \n",
       "3                 apocalypse lighting spokane wildfires                 \n",
       "4            typhoon soudelor kills in china and taiwan                 \n",
       "...                                                 ...           ...   \n",
       "3258  earthquake safety los angeles safety fasteners...                 \n",
       "3259   storm in rhode island worse than last hurrica...                 \n",
       "3260                  green line derailment in chicago                  \n",
       "3261          meg issues hazardous weather outlook hwo                  \n",
       "3262   cityof calgary has activated it is municipal ...                 \n",
       "\n",
       "     clean_location                                                new  \\\n",
       "0                                   just happened a terrible car crash   \n",
       "1                     heard about earthquake is different cities st...   \n",
       "2                    there is a forest fire at spot pond geese are ...   \n",
       "3                                apocalypse lighting spokane wildfires   \n",
       "4                           typhoon soudelor kills in china and taiwan   \n",
       "...             ...                                                ...   \n",
       "3258                 earthquake safety los angeles safety fasteners...   \n",
       "3259                  storm in rhode island worse than last hurrica...   \n",
       "3260                                 green line derailment in chicago    \n",
       "3261                         meg issues hazardous weather outlook hwo    \n",
       "3262                  city of calgary has activated it is municipal...   \n",
       "\n",
       "                                            clean_tweet  \\\n",
       "0                             happen terrible car crash   \n",
       "1     heard earthquake different city stay safe ever...   \n",
       "2     forest fire spot pond goose flee across street...   \n",
       "3                     apocalypse light spokane wildfire   \n",
       "4                    typhoon soudelor kill china taiwan   \n",
       "...                                                 ...   \n",
       "3258      earthquake safety los angeles safety fastener   \n",
       "3259  storm rhode island bad last hurricane city oth...   \n",
       "3260                      green line derailment chicago   \n",
       "3261            meg issue hazardous weather outlook hwo   \n",
       "3262  city calgary activate municipal emergency plan...   \n",
       "\n",
       "                                               all_text  \\\n",
       "0                             happen terrible car crash   \n",
       "1     heard earthquake different city stay safe ever...   \n",
       "2     forest fire spot pond goose flee across street...   \n",
       "3                     apocalypse light spokane wildfire   \n",
       "4                    typhoon soudelor kill china taiwan   \n",
       "...                                                 ...   \n",
       "3258      earthquake safety los angeles safety fastener   \n",
       "3259  storm rhode island bad last hurricane city oth...   \n",
       "3260                      green line derailment chicago   \n",
       "3261            meg issue hazardous weather outlook hwo   \n",
       "3262  city calgary activate municipal emergency plan...   \n",
       "\n",
       "                                        combined_tokens  \\\n",
       "0                        [happen, terrible, car, crash]   \n",
       "1     [heard, earthquake, different, city, stay, saf...   \n",
       "2     [forest, fire, spot, pond, goose, flee, across...   \n",
       "3                [apocalypse, light, spokane, wildfire]   \n",
       "4              [typhoon, soudelor, kill, china, taiwan]   \n",
       "...                                                 ...   \n",
       "3258  [earthquake, safety, los, angeles, safety, fas...   \n",
       "3259  [storm, rhode, island, bad, last, hurricane, c...   \n",
       "3260                 [green, line, derailment, chicago]   \n",
       "3261     [meg, issue, hazardous, weather, outlook, hwo]   \n",
       "3262  [city, calgary, activate, municipal, emergency...   \n",
       "\n",
       "                                           tweet_tokens  \\\n",
       "0                        [happen, terrible, car, crash]   \n",
       "1     [heard, earthquake, different, city, stay, saf...   \n",
       "2     [forest, fire, spot, pond, goose, flee, across...   \n",
       "3                [apocalypse, light, spokane, wildfire]   \n",
       "4              [typhoon, soudelor, kill, china, taiwan]   \n",
       "...                                                 ...   \n",
       "3258  [earthquake, safety, los, angeles, safety, fas...   \n",
       "3259  [storm, rhode, island, bad, last, hurricane, c...   \n",
       "3260                 [green, line, derailment, chicago]   \n",
       "3261     [meg, issue, hazardous, weather, outlook, hwo]   \n",
       "3262  [city, calgary, activate, municipal, emergency...   \n",
       "\n",
       "                                               filtered  \n",
       "0                        [happen, terrible, car, crash]  \n",
       "1     [heard, earthquake, different, city, stay, saf...  \n",
       "2      [forest, fire, spot, pond, across, street, save]  \n",
       "3                         [apocalypse, light, wildfire]  \n",
       "4              [typhoon, soudelor, kill, china, taiwan]  \n",
       "...                                                 ...  \n",
       "3258         [earthquake, safety, los, angeles, safety]  \n",
       "3259  [storm, rhode, island, bad, last, hurricane, c...  \n",
       "3260                 [green, line, derailment, chicago]  \n",
       "3261     [meg, issue, hazardous, weather, outlook, hwo]  \n",
       "3262  [city, calgary, activate, municipal, emergency...  \n",
       "\n",
       "[3263 rows x 11 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f8c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
