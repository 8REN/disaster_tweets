{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda env export > environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc847e3",
   "metadata": {},
   "source": [
    "replace contractions/location abbreviations via dictionary\n",
    "denoise text/apply word segmentation\n",
    "create word count dictionary\n",
    "next, spell check:\n",
    "add to dictionary from reoccuring words in text? or use freq dict symspell\n",
    "apply spell check to words in word count dictionary occuring only one time\n",
    "apply pos tag to tweets creating new column\n",
    "lemmatize tweets based on pos tag column\n",
    "bigrams with Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlikely-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473e8c8",
   "metadata": {},
   "source": [
    "## import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "0c2cab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "#target = pd.read_csv('train.csv', usecols=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc632945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "472d972e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3263\n"
     ]
    }
   ],
   "source": [
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f433b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df.drop(['id'], axis=1, inplace=True)\n",
    "df.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333b0dcc",
   "metadata": {},
   "source": [
    "#dups = df[df.duplicated(['text'])]\n",
    "df.drop_duplicates(subset=['text', 'location', 'keyword'], keep='first', inplace=True)\n",
    "#test_df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "\n",
    "print(len(df))\n",
    "#print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba7f01",
   "metadata": {},
   "source": [
    "## visualize target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c03cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='solarizedl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution of target in dataset\n",
    "plt.figure(figsize=(11,11))\n",
    "colors = ['lightblue', 'red']\n",
    "expl = (0, 0.1)\n",
    "df.target.value_counts().plot(kind='pie', legend=True, startangle=45, shadow=True, \n",
    "                             colors=colors, autopct='%1.1f%%')\n",
    "plt.title('Binary Distribution of Disaster Tweet Dataset', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755eb75a",
   "metadata": {},
   "source": [
    "# clean and process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8862ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from spellchecker import SpellChecker\n",
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0461e",
   "metadata": {},
   "source": [
    "## hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4408dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# functions related to expanded hashtags in text\n",
    "def pascal_case_split(text):\n",
    "    # expand hashtags formatted in pascal case, ex: #ThisIsPascal\n",
    "    return re.sub(r'([A-Z])([?=a-z0-9+])', r' \\1\\2', text)\n",
    "\n",
    "def camel_case_split(text):\n",
    "    # expand hashtags formatted in pascal case, ex: #thisIsCamel\n",
    "    return re.sub(r'([a-z0-9+])([?<=A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "def spell_check_word(word):\n",
    "    # lookup suggestions for individual words\n",
    "    spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=5)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "                      \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    corrections = spell.lookup(word, Verbosity.TOP, include_unknown=True)\n",
    "    closest = corrections[0]\n",
    "    # display suggestion term, edit distance, and term frequency\n",
    "    return closest.term\n",
    "    \n",
    "def spell_check_words(word):\n",
    "    # lookup suggestions for multi word string input\n",
    "    spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=5)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "    corrections = spell.lookup_compound(word, max_edit_distance=2)\n",
    "    closest = corrections[0]\n",
    "# display suggestion term, edit distance, and term frequency\n",
    "    return closest.term\n",
    "\n",
    "def desegment_strings(text):\n",
    "    spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=8)\n",
    "    dictionary_path = pkg_resources.resource_filename(\"symspellpy\",\"frequency_dictionary_en_82_765.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    if text.isupper() | text.islower() and text.startswith('#'):\n",
    "        result = spell.word_segmentation(text)\n",
    "        text = result.corrected_string\n",
    "    return text \n",
    "\n",
    "def remove_hash(text):\n",
    "    # remove hash symbol in front of hashtag text and remove non unicode chars\n",
    "    return re.sub('#', '', text)\n",
    "\n",
    "def expand_strings(text):\n",
    "    # desegment hashtags & other strings with similar formatting\n",
    "    text = re.sub('CAfire', 'california fire', text)\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    h_text = remove_hash(p_text)\n",
    "    return h_text\n",
    "\n",
    "def expand_strings_sc(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash, spell check using compound lookup\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    lu_text = spell_check_words(p_text)\n",
    "    h_text = remove_hash(lu_text)\n",
    "    return h_text\n",
    "\n",
    "def expand_strings_ds(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash. spell check using wordsegmentation\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    lu_text = desegment_strings(p_text)\n",
    "    h_text = remove_hash(lu_text)\n",
    "    return h_text\n",
    "\n",
    "def expand_strings_indv_word(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash. spell check using wordsegmentation\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    lu_text = spell_check_word(p_text)\n",
    "    h_text = remove_hash(lu_text)\n",
    "    return h_text\n",
    "\n",
    "# def expand_strings_ds(text):\n",
    "#     # combine hashtag split functions for specific cases and remove hash\n",
    "#     c_text = camel_case_split(text)\n",
    "#     p_text = pascal_case_split(c_text)\n",
    "#     lu_text = desegment_hashtag(p_text)\n",
    "#     h_text = remove_hash(lu_text)\n",
    "#     return h_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b79731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for extracted hashtags\n",
    "df['hashtags'] = df.text.str.findall(r'(?:(?<=\\s)|(?<=^))#.*?(?=\\s|$)')\n",
    "#test_df['hashtags'] = test_df.text.str.findall(r'(?:(?<=\\s)|(?<=^))#.*?(?=\\s|$)')\n",
    "df.hashtags = df.hashtags.astype(str)\n",
    "#test_df.hashtags = test_df.hashtags.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e207d9a",
   "metadata": {},
   "source": [
    "%time\n",
    "df['exp_hash'] = df['hashtags'].map(expand_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d17b2b7",
   "metadata": {},
   "source": [
    "%time\n",
    "# with desegment_hashtag\n",
    "df['exp_hash'] = df['hashtags'].map(expand_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee3ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fce86907",
   "metadata": {},
   "source": [
    "# process text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d573344",
   "metadata": {},
   "source": [
    "## dictionary based word replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83272deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mineral-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and read previously created dictionary as csv for abbreviations and expansions of words\n",
    "def csv2dict(csv_name):\n",
    "    with open(csv_name, mode='r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        return {rows[1]:rows[2] for rows in reader}\n",
    "location_expansion_dict = csv2dict('utils\\location_expansion.csv')\n",
    "contractions_dict = csv2dict('utils\\contractions.csv')\n",
    "internet_initialisms_dict = csv2dict('utils\\internet_initialisms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5d477",
   "metadata": {},
   "source": [
    "## denoise text functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90dec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5aa60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_expansion_from_dict(text, expansion_dict):\n",
    "    c_re = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, expansion_dict.keys()))+r'\\b')\n",
    "    def replace(match):\n",
    "        expansion =  f\"{expansion_dict[match.group(0)]}\"\n",
    "        return expansion\n",
    "    text = c_re.sub(replace, text)\n",
    "    return text\n",
    "\n",
    "def reduce_repeat_vowels(text, keep_reps=1):\n",
    "    # function to reduce repeated vowel occurences in string, \n",
    "    # keep_reps argument as int refers to number of occurences to keep in string\n",
    "    # keep_reps argument default : 1, can be set to any integer\n",
    "    vowels = ['a','e','i','o','u']\n",
    "    for ch in iter(vowels):\n",
    "        text = re.sub(r'(?i)'+rf'([{ch}])'+r'{3,}', r'\\1'*int(keep_reps), text)\n",
    "    return text\n",
    "\n",
    "def reduce_repeat_consonants(text, keep_reps=2):\n",
    "    # function to reduce repeated consonant occurences in string, \n",
    "    # keep_reps argument as int refers to number of occurences to keep in string\n",
    "    # keep_reps argument default : 1, can be set to any integer\n",
    "    consonants = ['b','c','d','f','g','h','j','k','l','m','n','p','q','r','s','t','v','w','x','y','z']\n",
    "    for ch in iter(consonants):\n",
    "        text = re.sub(r'(?i)'+ rf'([{ch}])'+r'{3,}', r'\\1'*int(keep_reps), text)\n",
    "    return text\n",
    "    \n",
    "def remove_urls(text):\n",
    "    # function to remove urls from string\n",
    "    return re.sub(r\"http\\S+\",  r\"\", text)\n",
    "\n",
    "def remove_handles(text):\n",
    "    # function to remove twitter handles from string\n",
    "    return re.sub(\"@\\S*\",  r\" \", text)\n",
    "\n",
    "def remove_non_uni(text):\n",
    "    # function to remove non unicode characters from string\n",
    "    return re.sub('[^\\u0000-\\u007f]', ' ', text)\n",
    "\n",
    "def remove_nan_str(text):\n",
    "    # function to remove 'nan' from string\n",
    "    return re.sub('nan', '', text)\n",
    "\n",
    "def remove_ws(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    # function to remove non alpha chars from string\n",
    "    return re.sub(\"[^a-zA-Z]\",  r\" \", text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    #reduce repeating characters\n",
    "    text = reduce_repeat_vowels(text)\n",
    "    text = reduce_repeat_consonants(text)\n",
    "    #remove xml tag strings\n",
    "    text = html.unescape(text)\n",
    "    #remove numeric and punctuation chars\n",
    "    text = remove_non_alpha(text)\n",
    "    #remove extra spaces\n",
    "    text = remove_ws(text)\n",
    "    return text   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf0fa6",
   "metadata": {},
   "source": [
    "## tweet text denoise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "concrete-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### text preprocessing specifically formatted for tweets but will work on any text\n",
    "def process_tweet(df): \n",
    "    \"\"\"combine regex and nltk processing for tweet text and keyword text processing\"\"\"\n",
    "    def expand_strings_ds(text):\n",
    "        # combine hashtag split functions for specific cases and remove hash\n",
    "        # remove urls and usernames first due to alternating casing inside before case format expansion\n",
    "        u_text = remove_urls(text)\n",
    "        hd_text = remove_handles(u_text)\n",
    "        c_text = camel_case_split(hd_text)\n",
    "        p_text = pascal_case_split(c_text)\n",
    "        lu_text = desegment_strings(p_text)\n",
    "        h_text = remove_hash(lu_text)\n",
    "        return h_text\n",
    "    \n",
    "    def expand_text(text):\n",
    "        e_text = expand_strings_ds(text)\n",
    "        le_text = word_expansion_from_dict(e_text, expansion_dict=location_expansion_dict)\n",
    "        ce_text = word_expansion_from_dict(le_text.lower(), expansion_dict=contractions_dict)\n",
    "        ii_text = word_expansion_from_dict(ce_text.lower(), expansion_dict=internet_initialisms_dict)\n",
    "        return ii_text\n",
    "\n",
    "    def process_text(text):\n",
    "        exp_text = expand_text(text)\n",
    "        clean_text = denoise_text(exp_text)\n",
    "        #lem_text = lemmatize_text(clean_text)\n",
    "        return clean_text\n",
    "\n",
    "    processed_df = [process_text(x) for x in df]\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16f797",
   "metadata": {},
   "source": [
    "## keyword text denoise process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee31a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keyword(df):\n",
    "    text = df.str.replace('%20', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed85b7",
   "metadata": {},
   "source": [
    "## location text denoise process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "eedf9dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AK': 'Alaska',\n",
       " 'AL': 'Alabama',\n",
       " 'AR': 'Arkansas',\n",
       " 'AZ': 'Arizona',\n",
       " 'CA': 'California',\n",
       " 'CO': 'Colorado',\n",
       " 'CT': 'Connecticut',\n",
       " 'DC': 'District of Columbia',\n",
       " 'DE': 'Delaware',\n",
       " 'FL': 'Florida',\n",
       " 'GA': 'Georgia',\n",
       " 'GU': 'Guam',\n",
       " 'HI': 'Hawaii',\n",
       " 'IA': 'Iowa',\n",
       " 'ID': 'Idaho',\n",
       " 'IL': 'Illinois',\n",
       " 'KS': 'Kansas',\n",
       " 'KY': 'Kentucky',\n",
       " 'LA': 'Louisiana',\n",
       " 'MA': 'Massachusetts',\n",
       " 'MD': 'Maryland',\n",
       " 'ME': 'Maine',\n",
       " 'MI': 'Michigan',\n",
       " 'MN': 'Minnesota',\n",
       " 'MO': 'Missouri',\n",
       " 'MP': 'Northern Mariana Islands',\n",
       " 'MS': 'Mississippi',\n",
       " 'MT': 'Montana',\n",
       " 'NC': 'North Carolina',\n",
       " 'ND': 'North Dakota',\n",
       " 'NE': 'Nebraska',\n",
       " 'NH': 'New Hampshire',\n",
       " 'NJ': 'New Jersey',\n",
       " 'NM': 'New Mexico',\n",
       " 'NV': 'Nevada',\n",
       " 'NY': 'New York',\n",
       " 'OH': 'Ohio',\n",
       " 'OK': 'Oklahoma',\n",
       " 'OR': 'Oregon',\n",
       " 'PA': 'Pennsylvania',\n",
       " 'PR': 'Puerto Rico',\n",
       " 'PW': 'Palau',\n",
       " 'RI': 'Rhode Island',\n",
       " 'SC': 'South Carolina',\n",
       " 'SD': 'South Dakota',\n",
       " 'TN': 'Tennessee',\n",
       " 'TX': 'Texas',\n",
       " 'UT': 'Utah',\n",
       " 'VA': 'Virginia',\n",
       " 'VI': 'Virgin Islands',\n",
       " 'VT': 'Vermont',\n",
       " 'WA': 'Washington',\n",
       " 'WI': 'Wisconsin',\n",
       " 'WV': 'West Virginia',\n",
       " 'WY': 'Wyoming',\n",
       " 'AB': 'Alberta',\n",
       " 'BC': 'British Columbia',\n",
       " 'MB': 'Manitoba',\n",
       " 'NB': 'New Brunswick',\n",
       " 'NL': 'Newfoundland and Labrador',\n",
       " 'NS': 'Nova Scotia',\n",
       " 'NT': 'Northwest Territories',\n",
       " 'NU': 'Nunavut',\n",
       " 'ON': 'Ontario',\n",
       " 'PE': 'Prince Edward Island',\n",
       " 'QC': 'Quebec',\n",
       " 'SK': 'Saskatchewan',\n",
       " 'YT': 'Yukon',\n",
       " 'UK': 'United Kingdom',\n",
       " 'USA': 'United States',\n",
       " 'U.S.A': 'United States'}"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_expansion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "skilled-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_location(df):\n",
    "    \"\"\"combine regex and nltk processing for location text processing\"\"\"\n",
    "    \n",
    "    def expand_loc_text(text):\n",
    "        ex_text = word_expansion_from_dict(text, location_expansion_dict)\n",
    "        return ex_text\n",
    "       \n",
    "        \n",
    "    def process_text(text):\n",
    "        text = expand_loc_text(text)\n",
    "        clean_text = denoise_text(text)\n",
    "        #lem_text = lemmatize_text(clean_text)\n",
    "        return clean_text\n",
    "    \n",
    "    loc_df = [process_text(x) for x in df]\n",
    "    return loc_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08771c0",
   "metadata": {},
   "source": [
    "# create new dataframe of denoised text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "084979e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_data_to_df(df):\n",
    "    df['tweets'] = process_tweet(df.text)\n",
    "    df['clean_keyword'] = process_keyword(df.keyword.astype(str))\n",
    "    df['clean_location'] = process_location(df.location.astype(str))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0608f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = scrub_data_to_df(df)\n",
    "test_df = scrub_data_to_df(test)\n",
    "all_tweets = pd.DataFrame()\n",
    "all_tweets['tweets'] = pd.concat([train_df.tweets, test_df.tweets], ignore_index=True)\n",
    "all_tweets['clean_keyword'] = pd.concat([train_df.clean_keyword, test_df.clean_keyword], ignore_index=True)\n",
    "all_tweets['clean_location'] = pd.concat([train_df.clean_location, test_df.clean_location], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b4ef9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.to_csv('complete_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb314aa",
   "metadata": {},
   "source": [
    "## create frequency dictionary for tweet text for further spell corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8696778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d115c57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    tweets = all_tweets.split()\n",
    "    corpus = ' '.join(word for word in tweets)\n",
    "    return corpus\n",
    "\n",
    "def word_count(text):\n",
    "    counts = dict()\n",
    "    for row in text:\n",
    "        words = row.split()\n",
    "        for w in words:\n",
    "            if w in counts:\n",
    "                counts[w] += 1\n",
    "            else:\n",
    "                counts[w] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e6c8a15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 153365 words in the entire training dataset\n",
      "There are 16645 unique words in the training dataset\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#tokens = all_tweets.tweets.astype(str).split()\n",
    "# count frequency of words in corpus\n",
    "word_count_dict = word_count(all_tweets.tweets)\n",
    "\n",
    "# count number of words in corpus\n",
    "num_words = sum(word_count_dict[w] for w in word_count_dict)\n",
    "print(f'There are {num_words} words in the entire training dataset')\n",
    "\n",
    "# count number of unique words in corpus\n",
    "word_count_sorted = [(value, key) for key, value in word_count_dict.items()]\n",
    "word_count_sorted.sort()\n",
    "print(f'There are {len(word_count_sorted)} unique words in the training dataset')\n",
    "\n",
    "single_occ_words =  [k for k,v in word_count_dict.items() if int(v) == 1]\n",
    "filtered_dict = {k:v for (k,v) in word_count_dict.items() if int(v)>=4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c5ce9621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_word(word):\n",
    "    # lookup suggestions for individual words\n",
    "    spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=5)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "                      \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    corrections = spell.lookup(word, Verbosity.TOP, include_unknown=True)\n",
    "    closest = corrections[0]\n",
    "    # display suggestion term, edit distance, and term frequency\n",
    "    return closest.term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "c4eb3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_rare(df):\n",
    "    corrections = []\n",
    "    for x in single_occ_words:\n",
    "        repl = spell_check_word(x)\n",
    "        corrections.append(repl)\n",
    "    replacement_dict = dict(zip(single_occ_words, corrections))\n",
    "    filtered_repl = dict(filter(lambda elem: elem[0] != elem[1], replacement_dict.items()))\n",
    "    def replace_spell(text):\n",
    "        r_re = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, filtered_repl.keys()))+r'\\b')\n",
    "        def replace(match):\n",
    "            replacement =  f\"{filtered_repl[match.group(0)]}\"\n",
    "            return replacement\n",
    "        text = r_re.sub(replace, text)\n",
    "        return text\n",
    "    all_tweets['sc'] = all_tweets['tweets'].apply(replace_spell)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "80e1e98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>new</th>\n",
       "      <th>sc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deeds are the reason of this earthquake m...</td>\n",
       "      <td>our deeds are the reason of this earthquake m...</td>\n",
       "      <td>our deeds are the reason of this earthquake m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la range ask canada</td>\n",
       "      <td>forest fire near la range sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all residents asked to shelter in place are b...</td>\n",
       "      <td>all residents asked to shelter in place are b...</td>\n",
       "      <td>all residents asked to shelter in place are b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent this photo from ruby alaska as ...</td>\n",
       "      <td>just got sent this photo from ruby alaska as ...</td>\n",
       "      <td>just got sent this photo from ruby alaska as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>earthquake safety los angeles safety fasteners...</td>\n",
       "      <td>earthquake safety los angeles safety fasteners...</td>\n",
       "      <td>earthquake safety los angeles safety fasteners...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>storm in rhode island worse than last hurrica...</td>\n",
       "      <td>storm in rhode island worse than last hurrica...</td>\n",
       "      <td>storm in rhode island worse than last hurrica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>green line derailment in chicago</td>\n",
       "      <td>green line derailment in chicago</td>\n",
       "      <td>green line derailment in chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>meg issues hazardous weather outlook hwo</td>\n",
       "      <td>meg issues hazardous weather outlook hwo</td>\n",
       "      <td>meg issues hazardous weather outlook hwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>cityof calgary has activated it is municipal ...</td>\n",
       "      <td>cityof calgary has activated it is municipal ...</td>\n",
       "      <td>cityof calgary has activated it is municipal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets  \\\n",
       "0       our deeds are the reason of this earthquake m...   \n",
       "1                  forest fire near la ronge sask canada   \n",
       "2       all residents asked to shelter in place are b...   \n",
       "3       people receive wildfires evacuation orders in...   \n",
       "4       just got sent this photo from ruby alaska as ...   \n",
       "...                                                  ...   \n",
       "10871  earthquake safety los angeles safety fasteners...   \n",
       "10872   storm in rhode island worse than last hurrica...   \n",
       "10873                  green line derailment in chicago    \n",
       "10874          meg issues hazardous weather outlook hwo    \n",
       "10875   cityof calgary has activated it is municipal ...   \n",
       "\n",
       "                                                     new  \\\n",
       "0       our deeds are the reason of this earthquake m...   \n",
       "1                   forest fire near la range ask canada   \n",
       "2       all residents asked to shelter in place are b...   \n",
       "3       people receive wildfires evacuation orders in...   \n",
       "4       just got sent this photo from ruby alaska as ...   \n",
       "...                                                  ...   \n",
       "10871  earthquake safety los angeles safety fasteners...   \n",
       "10872   storm in rhode island worse than last hurrica...   \n",
       "10873                  green line derailment in chicago    \n",
       "10874          meg issues hazardous weather outlook hwo    \n",
       "10875   cityof calgary has activated it is municipal ...   \n",
       "\n",
       "                                                      sc  \n",
       "0       our deeds are the reason of this earthquake m...  \n",
       "1                  forest fire near la range sask canada  \n",
       "2       all residents asked to shelter in place are b...  \n",
       "3       people receive wildfires evacuation orders in...  \n",
       "4       just got sent this photo from ruby alaska as ...  \n",
       "...                                                  ...  \n",
       "10871  earthquake safety los angeles safety fasteners...  \n",
       "10872   storm in rhode island worse than last hurrica...  \n",
       "10873                  green line derailment in chicago   \n",
       "10874          meg issues hazardous weather outlook hwo   \n",
       "10875   cityof calgary has activated it is municipal ...  \n",
       "\n",
       "[10876 rows x 3 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check_rare(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f827683",
   "metadata": {},
   "source": [
    "## tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "945c043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "b6121baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    stopword_list = stopwords.words('english')\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if word not in stopword_list:\n",
    "                tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "# function to convert nltk tag to wordnet tag\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_text(text):\n",
    "    # define lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(tokenize_text(text))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    # initialize empty list \n",
    "    lemmatized_text = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_text.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word, tag))\n",
    "    return ' '.join(x for x in lemmatized_text if len(x) > 1)\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # function to remove stopwords from text\n",
    "    stop_list = stopwords.words('english')  \n",
    "    new_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_list:\n",
    "            new_tokens.append(word)\n",
    "    return new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "2870d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### text preprocessing specifically formatted for tweets but will work on any text\n",
    "def tokenize_tweet(text): \n",
    "    \"\"\"combine regex and nltk processing for tweet text and keyword text processing\"\"\"\n",
    "    lem_text = lemmatize_text(text)\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4f0abd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets['tokens'] = all_tweets.tweets.apply(tokenize_text)\n",
    "all_tweets['stems'] = all_tweets.tokens.apply(lambda row: [stemmer.stem(item) for item in row]) \n",
    "all_tweets['lem'] = all_tweets.tweets.apply(lemmatize_text)\n",
    "all_tweets.to_csv('tweets_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca3c101",
   "metadata": {},
   "source": [
    "## separate test/train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8847db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all_tweets back into 2 separate datasets & save\n",
    "train_clean = all_tweets[:7613]\n",
    "test_clean = all_tweets[7613:]\n",
    "\n",
    "train_clean.to_csv('train_clean.csv')\n",
    "test_clean.to_csv('test_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "d5eddb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import og train dataset to add target to train data before dropping duplicates\n",
    "tr = pd.read_csv('train.csv')\n",
    "te = pd.read_csv('test.csv')\n",
    "\n",
    "# import and read from csvs just created so df is no longer a slice of all_tweets\n",
    "train_clean = pd.read_csv('train_clean.csv', usecols=[\"tweets\", \"tokens\", \"stems\", \"lem\", \"clean_location\", \"clean_keyword\"])\n",
    "test_clean = pd.read_csv('test_clean.csv', usecols=[\"tweets\", \"tokens\", \"stems\", \"lem\", \"clean_location\", \"clean_keyword\"])\n",
    "# add id & target columns from og data to dataframes, no target available for test data\n",
    "train_clean['target'] = tr.target\n",
    "train_clean['id'] = tr.id\n",
    "test_clean['id'] = te.id\n",
    "# drop duplicates from each dataset \n",
    "test_clean.drop_duplicates(subset=['tweets'], keep='first', inplace=True)\n",
    "train_clean.drop_duplicates(subset=['tweets'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "16dbd8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6830\n",
      "3064\n"
     ]
    }
   ],
   "source": [
    "print(len(train_clean))\n",
    "print(len(test_clean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "8b58e2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp_df = pd.concat([train_clean, test_clean], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb708f67",
   "metadata": {},
   "source": [
    "## find and notate recurring bigrams from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "55dd7eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(tokens):\n",
    "    joined_tweet = re.sub(r'[^a-zA-Z]', r' ', str(tokens))\n",
    "    return joined_tweet \n",
    "comp_df['joined_tokens'] = join_tokens(comp_df.tokens)\n",
    "comp_df['joined_stems'] = join_tokens(comp_df.stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "f184f416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser, ENGLISH_CONNECTOR_WORDS\n",
    "\n",
    "def sentence_to_bi_grams(sentence):\n",
    "    return ' '.join(phrases[sentence])\n",
    "texts = comp_df.lem.astype(str)\n",
    "def get_documents(text):\n",
    "    docs = [row.split() for row in text]\n",
    "    return docs\n",
    "doc = get_documents(texts)\n",
    "\n",
    "phrases = Phrases(doc, min_count=5, threshold=.75)\n",
    "frozen_phrases = phrases.freeze()\n",
    "\n",
    "bigrams = []\n",
    "for row in doc: \n",
    "    parsed_sentence = sentence_to_bi_grams(row)\n",
    "    bigrams.append(parsed_sentence)\n",
    "    \n",
    "comp_df['bigrams'] = bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "f27507f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all_tweets back into 2 separate datasets & save for classification process\n",
    "train_final = comp_df[:6830]\n",
    "test_final = comp_df[6830:]\n",
    "train_final.to_csv('train_final.csv')\n",
    "test_final.to_csv('test_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fa01ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
