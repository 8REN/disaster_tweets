{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unlikely-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re, string, unicodedata\n",
    "import pandas as pd \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='solarizedl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "elder-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import inflect\n",
    "from spellchecker import SpellChecker\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "mineral-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv2dict(csv_name):\n",
    "    with open(csv_name, mode='r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        return {rows[1]:rows[2] for rows in reader}\n",
    "    \n",
    "word_expansion_dict = csv2dict('word_expansion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "shaped-mainstream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343\n"
     ]
    }
   ],
   "source": [
    "print(len(word_expansion_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "powered-separate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AK': 'Alaska',\n",
       " 'AL': 'Alabama',\n",
       " 'AR': 'Arkansas',\n",
       " 'AZ': 'Arizona',\n",
       " 'CA': 'California',\n",
       " 'CO': 'Colorado',\n",
       " 'CT': 'Connecticut',\n",
       " 'DC': 'District of Columbia',\n",
       " 'DE': 'Delaware',\n",
       " 'FL': 'Florida',\n",
       " 'GA': 'Georgia',\n",
       " 'GU': 'Guam',\n",
       " 'HI': 'Hawaii',\n",
       " 'IA': 'Iowa',\n",
       " 'ID': 'Idaho',\n",
       " 'IL': 'Illinois',\n",
       " 'IN': 'Indiana',\n",
       " 'KS': 'Kansas',\n",
       " 'KY': 'Kentucky',\n",
       " 'LA': 'Louisiana',\n",
       " 'MA': 'Massachusetts',\n",
       " 'MD': 'Maryland',\n",
       " 'ME': 'Maine',\n",
       " 'MI': 'Michigan',\n",
       " 'MN': 'Minnesota',\n",
       " 'MO': 'Missouri',\n",
       " 'MP': 'Northern Mariana Islands',\n",
       " 'MS': 'Mississippi',\n",
       " 'MT': 'Montana',\n",
       " 'NC': 'North Carolina',\n",
       " 'ND': 'North Dakota',\n",
       " 'NE': 'Nebraska',\n",
       " 'NH': 'New Hampshire',\n",
       " 'NJ': 'New Jersey',\n",
       " 'NM': 'New Mexico',\n",
       " 'NV': 'Nevada',\n",
       " 'NY': 'New York',\n",
       " 'OH': 'Ohio',\n",
       " 'OK': 'Oklahoma',\n",
       " 'OR': 'Oregon',\n",
       " 'PA': 'Pennsylvania',\n",
       " 'PR': 'Puerto Rico',\n",
       " 'PW': 'Palau',\n",
       " 'RI': 'Rhode Island',\n",
       " 'SC': 'South Carolina',\n",
       " 'SD': 'South Dakota',\n",
       " 'TN': 'Tennessee',\n",
       " 'TX': 'Texas',\n",
       " 'UT': 'Utah',\n",
       " 'VA': 'Virginia',\n",
       " 'VI': 'Virgin Islands',\n",
       " 'VT': 'Vermont',\n",
       " 'WA': 'Washington',\n",
       " 'WI': 'Wisconsin',\n",
       " 'WV': 'West Virginia',\n",
       " 'WY': 'Wyoming',\n",
       " 'AB': 'Alberta',\n",
       " 'BC': 'British Columbia',\n",
       " 'MB': 'Manitoba',\n",
       " 'NB': 'New Brunswick',\n",
       " 'NL': 'Newfoundland and Labrador',\n",
       " 'NS': 'Nova Scotia',\n",
       " 'NT': 'Northwest Territories',\n",
       " 'NU': 'Nunavut',\n",
       " 'ON': 'Ontario',\n",
       " 'PE': 'Prince Edward Island',\n",
       " 'QC': 'Quebec',\n",
       " 'SK': 'Saskatchewan',\n",
       " 'YT': 'Yukon'}"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location_expansion_dict = csv2dict('location_expansion.csv')\n",
    "word_expansion_dict = csv2dict('word_expansion.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "expressed-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cultural-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location = df.location.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "falling-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_hashtags(text):\n",
    "    hashtag_list = []\n",
    "    hashtag_exp_list = []\n",
    "\n",
    "\n",
    "    def proper_nouns_lower(text): \n",
    "        text = re.sub('#', ' ', text)\n",
    "        proper_noun = re.match(r\"(?<=[A-Z])[a-z]\", text)\n",
    "        if text is proper_noun:\n",
    "            new_string = re.sub(proper_noun, proper_noun.group(0).lower(), text)\n",
    "        else:\n",
    "            new_string = text\n",
    "        return new_string\n",
    "\n",
    "    def word_expansion(text):\n",
    "        c_re = re.compile('|'.join('(\\b%s\\b)' % re.escape(s) for s in word_expansion_dict.keys()), re.IGNORECASE)\n",
    "        def replace(match):\n",
    "            expansion =  f\" {word_expansion_dict[match.group(0)]}\"\n",
    "            return expansion\n",
    "        text = c_re.sub(replace, text.lower())\n",
    "        return text\n",
    "\n",
    "    def camel_case_split(text):\n",
    "        text = re.sub('#', ' ', text)\n",
    "        exp_hashtags = re.sub(r'((?<!\\A)(?<=[a-z])[A-Z]|(?<!\\A)(?=[A-Z])[a-z+])', r' \\1', text)\n",
    "        # h2.append(re.sub(r'((?<!\\A)(?<=[A-Z])[a-z])', r' \\1', text))\n",
    "        return exp_hashtags\n",
    "        \n",
    "        \n",
    "    def denoise_text(text):\n",
    "        new_text = re.sub(r\"\\S*https?:\\S*\",  r\"\", text)\n",
    "        new_text_punct = re.sub(r\"[^\\w\\s#]\",  r\"\", new_text)\n",
    "        new_text_chars = re.sub('[^\\u0000-\\u007f]', '',  new_text_punct)\n",
    "        new_text_ = re.sub('_', '',  new_text_chars)\n",
    "        return new_text_\n",
    "    \n",
    "    def replace_numbers(tokens):\n",
    "# replace integers with string formatted words for numbers\n",
    "        dig2word = inflect.engine()\n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            if word.isdigit():\n",
    "                new_word = dig2word.number_to_words(word)\n",
    "                new_tokens.append(new_word)\n",
    "            else:\n",
    "                new_tokens.append(word)\n",
    "        return new_tokens\n",
    "    \n",
    "    for tweet in text:\n",
    "        for x in tweet.split():\n",
    "            if x.startswith('#') == True:\n",
    "                clean_text = denoise_text(x)\n",
    "                cleaner_text = replace_numbers(clean_text)\n",
    "                hashtag_list.append(''.join(cleaner_text))\n",
    "                \n",
    "    for hashtag in hashtag_list: \n",
    "        exp_hashtag = camel_case_split(hashtag)\n",
    "        strip_hash = exp_hashtag.strip()\n",
    "        hashtag_exp_list.append(strip_hash)\n",
    "        \n",
    "    return dict(zip(hashtag_list, hashtag_exp_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "voluntary-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags_dict = expand_hashtags(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "concrete-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### text preprocessing specifically formatted for tweets but will work on any text\n",
    "def tweet_preprocess(df): \n",
    "    \"\"\"combine regex and nltk processing for tweet text processing\"\"\"\n",
    "\n",
    "\n",
    "    def word_expansion(text):\n",
    "        c_re = re.compile('|'.join('(\\b%s\\b)' % re.escape(s) for s in word_expansion_dict.keys()), re.IGNORECASE)\n",
    "        def replace(match):\n",
    "            expansion =  f\" {word_expansion_dict[match.group(0)]}\"\n",
    "            return expansion\n",
    "        text = c_re.sub(replace, text.lower())\n",
    "        return text\n",
    "    \n",
    "    def camel_sub(text):\n",
    "        cam_re =  re.compile('|'.join('(\\b%s\\b)' % re.escape(s) for s in hashtags_dict.keys()))\n",
    "        def replace(match):\n",
    "            expansion =  f\" {hashtags_dict[match.group(0)]}\"\n",
    "            return expansion\n",
    "        text = cam_re.sub(replace, text)\n",
    "        return text\n",
    "    \n",
    "\n",
    "    # function to expand contractions, remove urls and characters before tokenization processing\n",
    "    def denoise_text(text):\n",
    "        camel_text = camel_sub(text)\n",
    "        new_text = re.sub(r\"\\S*https?:\\S*\",  r\"\", camel_text.lower())\n",
    "        new_text_exp = word_expansion(new_text)\n",
    "        x = re.sub('#cafire', 'california fire', new_text_exp)\n",
    "        x = re.sub('#calfires', 'california fires', x)\n",
    "        x = re.sub('#calwildfires', 'california wildfires', x)\n",
    "        x = re.sub('#cadrought', 'california drought', x)\n",
    "        new_text_punct = re.sub(r\"[^\\w\\s@]\",  r\"\", x)\n",
    "        unicode_chars = re.sub('[^\\u0000-\\u007f]', '',  new_text_punct)\n",
    "        strip_text = unicode_chars.strip()\n",
    "        #remove_hashtags_text = re.sub('#\\w+', '',  strip_text)\n",
    "        return strip_text \n",
    "    \n",
    "# tokenization & lemmatization function returns tokens    \n",
    "    def lemmatize_text(text):\n",
    "        tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        return [lemmatizer.lemmatize(w, pos='v') for w in tokenizer.tokenize(text)]\n",
    "\n",
    "# tokenization & stemmer function returns tokens\n",
    "    def stem_text(text):\n",
    "        tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(w) for w in tokenizer.tokenize(text)]\n",
    "\n",
    "    def replace_numbers(tokens):\n",
    "# replace integers with string formatted words for numbers\n",
    "        dig2word = inflect.engine()\n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            if word.isdigit():\n",
    "                new_word = dig2word.number_to_words(word)\n",
    "                new_tokens.append(new_word)\n",
    "            else:\n",
    "                new_tokens.append(word)\n",
    "        return new_tokens\n",
    "    \n",
    "    def remove_non_ascii(tokens):\n",
    "# remove non ascii characters from text\n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            new_token = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_tokens.append(new_token)\n",
    "        return new_tokens\n",
    "    \n",
    "# remove stopwords   \n",
    "    def remove_stopwords(tokens):\n",
    "        stop_list = stopwords.words('english')  \n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            if word not in stop_list:\n",
    "                new_tokens.append(word)\n",
    "        return new_tokens\n",
    "  \n",
    " \n",
    "    def norm_text(tokens):\n",
    "        words = replace_numbers(tokens)\n",
    "        #tokens = remove_stopwords(words)\n",
    "        words = remove_non_ascii(words)\n",
    "        return words\n",
    "    \n",
    "\n",
    "    def process_text(text):\n",
    "        clean_text = denoise_text(text)\n",
    "        lem_text = lemmatize_text(clean_text)\n",
    "        text = ' '.join([x for x in norm_text(lem_text)])\n",
    "        text = re.sub(r\"-\",  r\" \", text)\n",
    "        return text\n",
    "    \n",
    "    new_df = [process_text(x) for x in df]\n",
    "\n",
    "    return new_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_re = '([a-z]+)(?=[A-Z])|([A-Z][a-z]+)'\n",
    "        exp_hashtags = filter(None, re.split(h_re, text))\n",
    "        new_hashtag =  ' '.join([x for x in exp_hashtags])\n",
    "        return new_hashtag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "skilled-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_location(df):\n",
    "    def word_expansion(text):\n",
    "        # compile using word boundary so only complete work\n",
    "        l_re = re.compile('|'.join(location_expansion_dict.keys(), re.IGNORECASE))\n",
    "        def replace(match):\n",
    "            expansion =  f\" {location_expansion_dict[match.group(0)]}\"\n",
    "            return expansion\n",
    "        text = l_re.sub(replace, text.lower())\n",
    "        return text\n",
    "    # tokenization & lemmatization function returns tokens    \n",
    "    def lemmatize_text(text):\n",
    "        tokenizer = TweetTokenizer(strip_handles=True)\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        return [lemmatizer.lemmatize(w, pos='n') for w in tokenizer.tokenize(text)]\n",
    "\n",
    "    \n",
    "    def remove_non_ascii(tokens):\n",
    "# remove non ascii characters from text\n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            new_token = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_tokens.append(new_token)\n",
    "        return new_tokens\n",
    "    \n",
    "    def norm_text(tokens):\n",
    "        words = remove_non_ascii(tokens)\n",
    "        return words\n",
    "    \n",
    "    def denoise_location_text(text):\n",
    "        text = str(text)\n",
    "        new_text = re.sub(r\"\\S*https?:\\S*\",  r\"\", text)\n",
    "        new_text_contractions = word_expansion(new_text)\n",
    "        new_string = re.sub(r\"[^\\w\\s]\",  r\" \", new_text_contractions)\n",
    "        unicode_string = re.sub('[^\\u0000-\\u007f]', '',  new_string)\n",
    "        new_text_contractions = word_expansion(unicode_string)\n",
    "        clean_text = re.sub(r\"est september   \",  r\"\", unicode_string)\n",
    "        lem_text = lemmatize_text(clean_text)\n",
    "        text = ' '.join([x for x in norm_text(lem_text)])\n",
    "        text = re.sub(r\"-\",  r\" \", text)\n",
    "        return text\n",
    "\n",
    "    \n",
    "    new_df = [denoise_location_text(x) for x in df]\n",
    "    return new_df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "intimate-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hashtags = extract_hashtags(df.text)\n",
    "df['tweets'] = tweet_preprocess((df.text.astype(str).replace({r\"([#[A-Z][a-z])\": r\" \\1\"}, regex=True)))\n",
    "df['keyword'] = tweet_preprocess(df.keyword.astype(str).replace({r\"%20\" : r\" \"}, regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "speaking-trademark",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "join() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-507-b81513630414>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_location'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_location\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'clean_location'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_loc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-506-0df462e9d2a0>\u001b[0m in \u001b[0;36mpreprocess_location\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mnew_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdenoise_location_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-506-0df462e9d2a0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mnew_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdenoise_location_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-506-0df462e9d2a0>\u001b[0m in \u001b[0;36mdenoise_location_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mnew_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\S*https?:\\S*\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;34mr\"\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mnew_text_contractions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_expansion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mnew_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"[^\\w\\s]\"\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;34mr\" \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_text_contractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0municode_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[^\\u0000-\\u007f]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mnew_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-506-0df462e9d2a0>\u001b[0m in \u001b[0;36mword_expansion\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mword_expansion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[1;31m# compile using word boundary so only complete work\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0ml_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation_expansion_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[0mexpansion\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34mf\" {location_expansion_dict[match.group(0)]}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: join() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "df['clean_location'] = preprocess_location(df.location)\n",
    "df['clean_location'] = clean_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unlikely-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df.filter(['tweets','clean_location','keyword'], axis=1)\n",
    "train_y = df.filter(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "passive-jersey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds be the reason of this earthquake may...\n",
       "1           forest fire near la ronge sask canada nan nan\n",
       "2       all residents ask to shelter in place be be no...\n",
       "3       thirteen thousand people receive wildfires eva...\n",
       "4       just get send this photo from ruby alaska as s...\n",
       "                              ...                        \n",
       "7604    world news fall powerlines on glink tram you p...\n",
       "7605    on the flip side i be at walmart and there be ...\n",
       "7606    suicide bomber kill fifteen in saudi security ...\n",
       "7608    two giant crane hold a bridge collapse into ne...\n",
       "7612    the latest more home raze by northern californ...\n",
       "Length: 7503, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = df.tweets + \" \" + df.clean_location + \" \" + df.keyword\n",
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "median-civilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweets</th>\n",
       "      <th>clean_location</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Our  Deeds are the  Reason of this #earthqua...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds be the reason of this earthquake may...</td>\n",
       "      <td>nan</td>\n",
       "      <td>our deeds be the reason of this earthquake may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Forest fire near  La  Ronge  Sask.  Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>nan</td>\n",
       "      <td>forest fire near la ronge sask canada nan nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>All residents asked to 'shelter in place' ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents ask to shelter in place be be no...</td>\n",
       "      <td>nan</td>\n",
       "      <td>all residents ask to shelter in place be be no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>13,000 people receive #wildfires evacuation o...</td>\n",
       "      <td>1</td>\n",
       "      <td>thirteen thousand people receive wildfires eva...</td>\n",
       "      <td>nan</td>\n",
       "      <td>thirteen thousand people receive wildfires eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Just got sent this photo from  Ruby # Alaska...</td>\n",
       "      <td>1</td>\n",
       "      <td>just get send this photo from ruby alaska as s...</td>\n",
       "      <td>nan</td>\n",
       "      <td>just get send this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>10863</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td># World News  Fallen powerlines on  G:link tr...</td>\n",
       "      <td>1</td>\n",
       "      <td>world news fall powerlines on glink tram you p...</td>\n",
       "      <td>nan</td>\n",
       "      <td>world news fall powerlines on glink tram you p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>10864</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>on the flip side  I'm at  Walmart and there i...</td>\n",
       "      <td>1</td>\n",
       "      <td>on the flip side i be at walmart and there be ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>on the flip side i be at walmart and there be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>10866</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Suicide bomber kills 15 in  Saudi security s...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicide bomber kill fifteen in saudi security ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>suicide bomber kill fifteen in saudi security ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Two giant cranes holding a bridge collapse i...</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane hold a bridge collapse into ne...</td>\n",
       "      <td>nan</td>\n",
       "      <td>two giant crane hold a bridge collapse into ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>The  Latest:  More  Homes  Razed by  Norther...</td>\n",
       "      <td>1</td>\n",
       "      <td>the latest more home raze by northern californ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>the latest more home raze by northern californ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     nan      nan   \n",
       "1         4     nan      nan   \n",
       "2         5     nan      nan   \n",
       "3         6     nan      nan   \n",
       "4         7     nan      nan   \n",
       "...     ...     ...      ...   \n",
       "7604  10863     nan      nan   \n",
       "7605  10864     nan      nan   \n",
       "7606  10866     nan      nan   \n",
       "7608  10869     nan      nan   \n",
       "7612  10873     nan      nan   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0       Our  Deeds are the  Reason of this #earthqua...       1   \n",
       "1            Forest fire near  La  Ronge  Sask.  Canada       1   \n",
       "2       All residents asked to 'shelter in place' ar...       1   \n",
       "3      13,000 people receive #wildfires evacuation o...       1   \n",
       "4       Just got sent this photo from  Ruby # Alaska...       1   \n",
       "...                                                 ...     ...   \n",
       "7604   # World News  Fallen powerlines on  G:link tr...       1   \n",
       "7605   on the flip side  I'm at  Walmart and there i...       1   \n",
       "7606    Suicide bomber kills 15 in  Saudi security s...       1   \n",
       "7608    Two giant cranes holding a bridge collapse i...       1   \n",
       "7612    The  Latest:  More  Homes  Razed by  Norther...       1   \n",
       "\n",
       "                                                 tweets clean_location  \\\n",
       "0     our deeds be the reason of this earthquake may...            nan   \n",
       "1                 forest fire near la ronge sask canada            nan   \n",
       "2     all residents ask to shelter in place be be no...            nan   \n",
       "3     thirteen thousand people receive wildfires eva...            nan   \n",
       "4     just get send this photo from ruby alaska as s...            nan   \n",
       "...                                                 ...            ...   \n",
       "7604  world news fall powerlines on glink tram you p...            nan   \n",
       "7605  on the flip side i be at walmart and there be ...            nan   \n",
       "7606  suicide bomber kill fifteen in saudi security ...            nan   \n",
       "7608  two giant crane hold a bridge collapse into ne...            nan   \n",
       "7612  the latest more home raze by northern californ...            nan   \n",
       "\n",
       "                                               all_text  \n",
       "0     our deeds be the reason of this earthquake may...  \n",
       "1         forest fire near la ronge sask canada nan nan  \n",
       "2     all residents ask to shelter in place be be no...  \n",
       "3     thirteen thousand people receive wildfires eva...  \n",
       "4     just get send this photo from ruby alaska as s...  \n",
       "...                                                 ...  \n",
       "7604  world news fall powerlines on glink tram you p...  \n",
       "7605  on the flip side i be at walmart and there be ...  \n",
       "7606  suicide bomber kill fifteen in saudi security ...  \n",
       "7608  two giant crane hold a bridge collapse into ne...  \n",
       "7612  the latest more home raze by northern californ...  \n",
       "\n",
       "[7503 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['all_text'] = all_text\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text_tags = df['tweets'].apply(lambda row: [nltk.pos_tag(row) for item in row])\n",
    "pos_keyword_tags = df['keyword'].apply(lambda row: [nltk.pos_tag(row) for item in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location[df.location != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keyword.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,11))\n",
    "colors = ['lightblue', 'red']\n",
    "expl = (0, 0.1)\n",
    "df.target.value_counts().plot(kind='pie', legend=True, startangle=45, shadow=True, \n",
    "                             colors=colors, autopct='%1.1f%%')\n",
    "plt.title('target count', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled = spell.unknown(df.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.tweets\n",
    "nlp = Word2Vec(corpus, size=200,   \n",
    "            window=6, min_count=1, sg=1, iter=40)\n",
    "len(nlp.wv.vocab) # number of words in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from symspellpy.symspellpy import SymSpell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags[99:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hash=[]\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=1)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "for term in hashtags:\n",
    "    if len(term)>1:\n",
    "        result = sym_spell.word_segmentation(term)\n",
    "        r = result.corrected_string\n",
    "    else:\n",
    "        r = ''\n",
    "    new_hash.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-breach",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[10:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dicti\n",
    "word_dict = {} \n",
    "\n",
    "for row in df.tweets: \n",
    "    words = tokenizer.tokenize(row) \n",
    "    for word in words: \n",
    "        if word not in word_dict.keys(): \n",
    "            word_dict[word] = 1\n",
    "        else: \n",
    "            word_dict[word] += 1\n",
    "print(len(word_dict))\n",
    "max(word_dict, key=word_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlist=[]\n",
    "for x in hashtags:\n",
    "    x = x.str.replace('[^a-zA-Z]', '')\n",
    "    hlist.append(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
