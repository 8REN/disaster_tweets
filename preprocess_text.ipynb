{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "unlikely-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re, string, unicodedata\n",
    "import pandas as pd \n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='solarizedl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "elder-particle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer, WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import inflect\n",
    "from spellchecker import SpellChecker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appreciated-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('coors.csv', mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    with open('coors_new.csv', mode='w') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    for rows in reader:\n",
    "        k = rows[0]\n",
    "        v = rows[1]\n",
    "        mydict = {k:v for k, v in rows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "apart-reduction",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-fcd58ae6a3ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mword_expansion\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mword_expansion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from .word_expansion import word_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "seeing-wholesale",
   "metadata": {},
   "outputs": [],
   "source": [
    "from setuptools import setup, find_packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "educational-republican",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\distutils\\fancy_getopt.py\", line 233, in getopt\n",
      "    opts, args = getopt.getopt(args, short_opts, self.long_opts)\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\getopt.py\", line 95, in getopt\n",
      "    opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\getopt.py\", line 195, in do_shorts\n",
      "    if short_has_arg(opt, shortopts):\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\getopt.py\", line 211, in short_has_arg\n",
      "    raise GetoptError(_('option -%s not recognized') % opt, opt)\n",
      "getopt.GetoptError: option -f not recognized\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\distutils\\core.py\", line 134, in setup\n",
      "    ok = dist.parse_command_line()\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\distutils\\dist.py\", line 475, in parse_command_line\n",
      "    args = parser.getopt(args=self.script_args, object=self)\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\distutils\\fancy_getopt.py\", line 235, in getopt\n",
      "    raise DistutilsArgError(msg)\n",
      "distutils.errors.DistutilsArgError: option -f not recognized\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3427, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-8-e50bd7e7ac8e>\", line 1, in <module>\n",
      "    setup(name = 'word_expansion', packages = find_packages())\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\site-packages\\setuptools\\__init__.py\", line 153, in setup\n",
      "    return distutils.core.setup(**attrs)\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\distutils\\core.py\", line 136, in setup\n",
      "    raise SystemExit(gen_usage(dist.script_name) + \"\\nerror: %s\" % msg)\n",
      "SystemExit: usage: ipykernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n",
      "   or: ipykernel_launcher.py --help [cmd1 cmd2 ...]\n",
      "   or: ipykernel_launcher.py --help-commands\n",
      "   or: ipykernel_launcher.py cmd --help\n",
      "\n",
      "error: option -f not recognized\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\root\\anaconda3\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "AttributeError: 'tuple' object has no attribute 'tb_frame'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mGetoptError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\distutils\\fancy_getopt.py\u001b[0m in \u001b[0;36mgetopt\u001b[1;34m(self, args, object)\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m             \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshort_opts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong_opts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mgetopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\getopt.py\u001b[0m in \u001b[0;36mgetopt\u001b[1;34m(args, shortopts, longopts)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_shorts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\getopt.py\u001b[0m in \u001b[0;36mdo_shorts\u001b[1;34m(opts, optstring, shortopts, args)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptstring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptstring\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptstring\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 195\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mshort_has_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    196\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moptstring\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\getopt.py\u001b[0m in \u001b[0;36mshort_has_arg\u001b[1;34m(opt, shortopts)\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mshortopts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mGetoptError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'option -%s not recognized'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mGetoptError\u001b[0m: option -f not recognized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mDistutilsArgError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\distutils\\core.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(**attrs)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_command_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mDistutilsArgError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\distutils\\dist.py\u001b[0m in \u001b[0;36mparse_command_line\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_aliases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'licence'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'license'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetopt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscript_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[0moption_order\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_option_order\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\distutils\\fancy_getopt.py\u001b[0m in \u001b[0;36mgetopt\u001b[1;34m(self, args, object)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mgetopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mDistutilsArgError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDistutilsArgError\u001b[0m: option -f not recognized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-e50bd7e7ac8e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'word_expansion'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_packages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\setuptools\\__init__.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(**attrs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0m_install_setup_requires\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mdistutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\distutils\\core.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(**attrs)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mDistutilsArgError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mSystemExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgen_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscript_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"\\nerror: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: usage: ipykernel_launcher.py [global_opts] cmd1 [cmd1_opts] [cmd2 [cmd2_opts] ...]\n   or: ipykernel_launcher.py --help [cmd1 cmd2 ...]\n   or: ipykernel_launcher.py --help-commands\n   or: ipykernel_launcher.py cmd --help\n\nerror: option -f not recognized",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                     stb = ['An exception has occurred, use %tb to see '\n\u001b[0;32m   2046\u001b[0m                            'the full traceback.\\n']\n\u001b[1;32m-> 2047\u001b[1;33m                     stb.extend(self.InteractiveTB.get_exception_only(etype,\n\u001b[0m\u001b[0;32m   2048\u001b[0m                                                                      value))\n\u001b[0;32m   2049\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mget_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mexception\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \"\"\"\n\u001b[1;32m--> 754\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mListTB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstructured_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshow_exception_only\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    627\u001b[0m             \u001b[0mchained_exceptions_tb_offset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m             out_list = (\n\u001b[1;32m--> 629\u001b[1;33m                 self.structured_traceback(\n\u001b[0m\u001b[0;32m    630\u001b[0m                     \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0metb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchained_exc_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m                     chained_exceptions_tb_offset, context)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1369\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1265\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1267\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1122\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1080\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1082\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    380\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 382\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "setup(name = 'word_expansion', packages = find_packages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "connected-cookbook",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\root\\\\Projects\\\\nlp\\\\disaster_tweets\\\\utils')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "chicken-america",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\root\\\\Projects\\\\nlp\\\\disaster_tweets\\\\utils'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "expressed-column",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "infrared-attitude",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-179-8d67e167d37b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34mr\"(^#[A-Z])\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mr\" \\1\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "x = x.replace({r\"(^#[A-Z])\": r\" \\1\"}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "excited-titanium",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-56febc7a46f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mexpanded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcamel_case_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mexpanded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-162-56febc7a46f2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mexpanded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcamel_case_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mexpanded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-162-56febc7a46f2>\u001b[0m in \u001b[0;36mcamel_case_split\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mhashtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"#(\\w+)\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'([A-Z][a-z]+)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhashtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "list1 = []\n",
    "def camel_case_split(text):\n",
    "    t = re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', text for text if text.startswith('#')))\n",
    "    return t\n",
    "expanded = [camel_case_split(x) for x in df.text]\n",
    "        \n",
    "expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "historic-keyboard",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-253-524f42fca3fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"(^#[A-Z])\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mr'( \\1)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[1;32m--> 241\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 304\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    305\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 764\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    765\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[1;33m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m         \u001b[1;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "na_action='ignore'\n",
    "df.test = df['text'].apply(re.findall(r\"([A-Z])\", r'( \\1)', x) for x in str(i for i in row.split())], axis=1)\n",
    ">>> set([re.sub(r\"(\\W+)$\", \"\", j) for j in set([i for i in text.split() if i.startswith(\"#\")])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "interested-twenty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>All residents asked to 'shelter in place' are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>13,000 people receive #wildfires evacuation o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>10863</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>#WorldNews Fallen powerlines on G:link tram: ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>10864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>on the flip side I'm at Walmart and there is ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>10866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>Suicide bomber kills 15 in Saudi security sit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>Two giant cranes holding a bridge collapse in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Cali...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     NaN      nan   \n",
       "1         4     NaN      nan   \n",
       "2         5     NaN      nan   \n",
       "3         6     NaN      nan   \n",
       "4         7     NaN      nan   \n",
       "...     ...     ...      ...   \n",
       "7604  10863     NaN      nan   \n",
       "7605  10864     NaN      nan   \n",
       "7606  10866     NaN      nan   \n",
       "7608  10869     NaN      nan   \n",
       "7612  10873     NaN      nan   \n",
       "\n",
       "                                                   text  target  \n",
       "0      Our Deeds are the Reason of this #earthquake ...       1  \n",
       "1                Forest fire near La Ronge Sask. Canada       1  \n",
       "2      All residents asked to 'shelter in place' are...       1  \n",
       "3      13,000 people receive #wildfires evacuation o...       1  \n",
       "4      Just got sent this photo from Ruby #Alaska as...       1  \n",
       "...                                                 ...     ...  \n",
       "7604   #WorldNews Fallen powerlines on G:link tram: ...       1  \n",
       "7605   on the flip side I'm at Walmart and there is ...       1  \n",
       "7606   Suicide bomber kills 15 in Saudi security sit...       1  \n",
       "7608   Two giant cranes holding a bridge collapse in...       1  \n",
       "7612   The Latest: More Homes Razed by Northern Cali...       1  \n",
       "\n",
       "[7503 rows x 5 columns]"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "floppy-insertion",
   "metadata": {},
   "source": [
    "list1 = []\n",
    "for row in df.text:\n",
    "    for x in row.split():\n",
    "        x = [re.findall(r'[^#A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', x)]\n",
    "        expanded_hash = \" \".join([x for x in row])\n",
    "    else:\n",
    "        expanded_hash = row\n",
    "    list1.append(expanded_hash)\n",
    "list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "dramatic-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = \" \"+df.text\n",
    "df.drop_duplicates(subset=['text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "relevant-combination",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-a369367c7119>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexpanded_ht\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'([A-Z][a-z]+)'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'#'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mexpanded_ht\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "expanded_ht = \" \".join([x for x in re.split('([A-Z][a-z]+)', df.text) if x.startswith('#') == True])    \n",
    "expanded_ht"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "favorite-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'] = \" \"+df.location+\" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "cultural-charger",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location = df.location.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "voluntary-marker",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "replace() takes no keyword arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-180-6001e8e2b200>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34mr\"(^#[A-Z])\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mr\" \\1\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: replace() takes no keyword arguments"
     ]
    }
   ],
   "source": [
    "    for row in df.text:\n",
    "        for x in row:\n",
    "            x = x.replace({r\"(^#[A-Z])\": r\" \\1\"}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "concrete-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### text preprocessing specifically formatted for tweets but will work on any text\n",
    "def tweet_preprocess(df): \n",
    "    \"\"\"combine regex and nltk processing for tweet text processing\"\"\"\n",
    "\n",
    "\n",
    "    def word_expansion(text):\n",
    "\n",
    "        word_expansion_dict = {\n",
    "         'AK': 'Alaska',\n",
    "         'AL': 'Alabama',\n",
    "         'AR': 'Arkansas',\n",
    "         'AZ': 'Arizona',\n",
    "         'CA': 'California',\n",
    "         'CO': 'Colorado',\n",
    "         'CT': 'Connecticut',\n",
    "         'DC': 'District of Columbia',\n",
    "         'DE': 'Delaware',\n",
    "         'FL': 'Florida',\n",
    "         'GA': 'Georgia',\n",
    "         'GU': 'Guam',\n",
    "         'HI': 'Hawaii',\n",
    "         'IA': 'Iowa',\n",
    "         'ID': 'Idaho',\n",
    "         'IL': 'Illinois',\n",
    "         'IN': 'Indiana',\n",
    "         'KS': 'Kansas',\n",
    "         'KY': 'Kentucky',\n",
    "         'LA': 'Louisiana',\n",
    "         'MA': 'Massachusetts',\n",
    "         'MD': 'Maryland',\n",
    "         'ME': 'Maine',\n",
    "         'MI': 'Michigan',\n",
    "         'MN': 'Minnesota',\n",
    "         'MO': 'Missouri',\n",
    "         'MP': 'Northern Mariana Islands',\n",
    "         'MS': 'Mississippi',\n",
    "         'MT': 'Montana',\n",
    "         'NC': 'North Carolina',\n",
    "         'ND': 'North Dakota',\n",
    "         'NE': 'Nebraska',\n",
    "         'NH': 'New Hampshire',\n",
    "         'NJ': 'New Jersey',\n",
    "         'NM': 'New Mexico',\n",
    "         'NV': 'Nevada',\n",
    "         'NY': 'New York',\n",
    "         'OH': 'Ohio',\n",
    "         'OK': 'Oklahoma',\n",
    "         'OR': 'Oregon',\n",
    "         'PA': 'Pennsylvania',\n",
    "         'PR': 'Puerto Rico',\n",
    "         'PW': 'Palau',\n",
    "         'RI': 'Rhode Island',\n",
    "         'SC': 'South Carolina',\n",
    "         'SD': 'South Dakota',\n",
    "         'TN': 'Tennessee',\n",
    "         'TX': 'Texas',\n",
    "         'UT': 'Utah',\n",
    "         'VA': 'Virginia',\n",
    "         'VI': 'Virgin Islands',\n",
    "         'VT': 'Vermont',\n",
    "         'WA': 'Washington',\n",
    "         'WI': 'Wisconsin',\n",
    "         'WV': 'West Virginia',\n",
    "         'WY': 'Wyoming',\n",
    "         'AB': 'Alberta',\n",
    "          'BC': 'British Columbia',\n",
    "          'MB': 'Manitoba',\n",
    "          'NB': 'New Brunswick',\n",
    "          'NL': 'Newfoundland and Labrador',\n",
    "          'NS': 'Nova Scotia',\n",
    "          'NT': 'Northwest Territories',\n",
    "          'NU': 'Nunavut',\n",
    "          'ON': 'Ontario',\n",
    "          'PE': 'Prince Edward Island',\n",
    "          'QC': 'Quebec',\n",
    "          'SK': 'Saskatchewan',\n",
    "          'YT': 'Yukon',\n",
    "         \"\\'cause\": 'because',\n",
    "         '1st': 'first',\n",
    "         '2nd': 'second',\n",
    "         '3rd': 'third',\n",
    "         '4th': 'fourth',\n",
    "         \"ain't\": 'are not',\n",
    "         'aint': 'are not',\n",
    "         \"aren't\": 'are not',\n",
    "         'arent': 'are not',\n",
    "         'b/c': 'because',\n",
    "         'bc': 'because',\n",
    "         \"can't\": 'can not',\n",
    "         \"can't've\": 'can not have',\n",
    "         'cant': 'can not',\n",
    "         'cause': 'because',\n",
    "         \"could've\": 'could have',\n",
    "         \"couldn't\": 'could not',\n",
    "         \"couldn't've\": 'could not have',\n",
    "         'couldnt': 'could not',\n",
    "         'couldve': 'could have',\n",
    "         \"didn't\": 'did not',\n",
    "         'didnt': 'did not',\n",
    "         \"doesn't\": 'does not',\n",
    "         'doesnt': 'does not',\n",
    "         \"don't\": 'do not',\n",
    "         'dont': 'do not',\n",
    "         \"hadn't\": 'had not',\n",
    "         \"hadn't've\": 'had not have',\n",
    "         'hadnt': 'had not',\n",
    "         \"hasn't\": 'has not',\n",
    "         'hasnt': 'has not',\n",
    "         \"haven't\": 'have not',\n",
    "         'havent': 'have not',\n",
    "         \"he'd\": 'he would',\n",
    "         \"he'd've\": 'he would have',\n",
    "         \"he'll\": 'he will',\n",
    "         \"he'll've\": 'he will have',\n",
    "         \"he's\": 'he is',\n",
    "         'hes': 'he is',\n",
    "         \"how'd\": 'how did',\n",
    "         \"how'd'y\": 'how do you',\n",
    "         \"how'll\": 'how will',\n",
    "         \"how's\": 'how is',\n",
    "         'howd': 'how did',\n",
    "         'howdy': 'how do you',\n",
    "         'howll': 'how will',\n",
    "         'hows': 'how is',\n",
    "         \"i'd\": 'i would',\n",
    "         \"i'd've\": 'i would have',\n",
    "         \"i'll\": 'i will',\n",
    "         \"i'll've\": 'i will have',\n",
    "         \"i'm\": 'i am',\n",
    "         \"i've\": 'i have',\n",
    "         'id': 'i would',\n",
    "         'ida': 'i would have',\n",
    "         'im': 'i am',\n",
    "         \"isn't\": 'is not',\n",
    "         'isnt': 'is not',\n",
    "         \"it'd\": 'it had',\n",
    "         \"it'd've\": 'it would have',\n",
    "         \"it'll\": 'it will',\n",
    "         \"it'll've\": 'it will have',\n",
    "         \"it's\": 'it is',\n",
    "         'itd': 'it had',\n",
    "         'itll': 'it will',\n",
    "         'its': 'it is',\n",
    "         'ive': 'i have',\n",
    "         \"let's\": 'let us',\n",
    "         'lets': 'let us',\n",
    "         \"ma'am\": 'madam',\n",
    "         'maam': 'madam',\n",
    "         \"mayn't\": 'may not',\n",
    "         \"might've\": 'might have',\n",
    "         'mighta': 'might have',\n",
    "         \"mightn't\": 'might not',\n",
    "         \"mightn't've\": 'might not have',\n",
    "         'mightnt': 'might not',\n",
    "         'mightve': 'might have',\n",
    "         \"must've\": 'must have',\n",
    "         'musta': 'must have',\n",
    "         \"mustn't\": 'must not',\n",
    "         \"mustn't've\": 'must not have',\n",
    "         'mustnt': 'must not',\n",
    "         'mustve': 'must have',\n",
    "         \"needn't\": 'need not',\n",
    "         \"needn't've\": 'need not have',\n",
    "         'neednt': 'need not',\n",
    "         \"o'clock\": 'of the clock',\n",
    "         'oclock': 'of the clock',\n",
    "         \"oughtn't\": 'ought not',\n",
    "         \"oughtn't've\": 'ought not have',\n",
    "         \"sha'n't\": 'shall not',\n",
    "         \"shan't\": 'shall not',\n",
    "         \"shan't've\": 'shall not have',\n",
    "         \"she'd\": 'she would',\n",
    "         \"she'd've\": 'she would have',\n",
    "         \"she'll\": 'she will',\n",
    "         \"she'll've\": 'she will have',\n",
    "         \"she's\": 'she is',\n",
    "         'shes': 'she is',\n",
    "         \"should've\": 'should have',\n",
    "         'shoulda': 'should have',\n",
    "         \"shouldn't\": 'should not',\n",
    "         \"shouldn't've\": 'should not have',\n",
    "         'shouldnt': 'should not',\n",
    "         'shouldve': 'should have',\n",
    "         \"so'd\": 'so did',\n",
    "         \"so's\": 'so is',\n",
    "         \"so've\": 'so have',\n",
    "         \"that'd\": 'that would',\n",
    "         \"that'd've\": 'that would have',\n",
    "         \"that's\": 'that is',\n",
    "         'thatd': 'that would',\n",
    "         'thats': 'that is',\n",
    "         \"there'd\": 'there had',\n",
    "         \"there'd've\": 'there would have',\n",
    "         \"there's\": 'there is',\n",
    "         'thered': 'there had',\n",
    "         'theres': 'there is',\n",
    "         \"they'd\": 'they would',\n",
    "         \"they'd've\": 'they would have',\n",
    "         \"they'll\": 'they will',\n",
    "         \"they'll've\": 'they will have',\n",
    "         \"they're\": 'they are',\n",
    "         \"they've\": 'they have',\n",
    "         'theyd': 'they would',\n",
    "         'theyda': 'they would have',\n",
    "         'theyll': 'they will',\n",
    "         'theyre': 'they are',\n",
    "         'theyve': 'they have',\n",
    "         \"to've\": 'to have',\n",
    "         \"wasn't\": 'was not',\n",
    "         'wasnt': 'was not',\n",
    "         \"we'd\": 'we had',\n",
    "         \"we'd've\": 'we would have',\n",
    "         \"we'll\": 'we will',\n",
    "         \"we'll've\": 'we will have',\n",
    "         \"we're\": 'we are',\n",
    "         \"we've\": 'we have',\n",
    "         \"weren't\": 'were not',\n",
    "         'werent': 'were not',\n",
    "         'weve': 'we have',\n",
    "         \"what'll\": 'what will',\n",
    "         \"what'll've\": 'what will have',\n",
    "         \"what're\": 'what are',\n",
    "         \"what's\": 'what is',\n",
    "         \"what've\": 'what have',\n",
    "         'whatll': 'what will',\n",
    "         'whatllve': 'what will have',\n",
    "         'whatre': 'what are',\n",
    "         'whats': 'what is',\n",
    "         'whatve': 'what have',\n",
    "         \"when's\": 'when is',\n",
    "         \"when've\": 'when have',\n",
    "         'whens': 'when is',\n",
    "         'whenve': 'when have',\n",
    "         \"where'd\": 'where did',\n",
    "         \"where's\": 'where is',\n",
    "         \"where've\": 'where have',\n",
    "         'whered': 'where did',\n",
    "         'whereve': 'where have',\n",
    "         'whers': 'where is',\n",
    "         \"who'll\": 'who will',\n",
    "         \"who'll've\": 'who will have',\n",
    "         \"who's\": 'who is',\n",
    "         \"who've\": 'who have',\n",
    "         'wholl': 'who will',\n",
    "         'whollve': 'who will have',\n",
    "         'whos': 'who is',\n",
    "         'whove': 'who have',\n",
    "         \"why's\": 'why is',\n",
    "         \"why've\": 'why have',\n",
    "         'whys': 'why is',\n",
    "         'whyve': 'why have',\n",
    "         \"will've\": 'will have',\n",
    "         'willve': 'will have',\n",
    "         \"won't\": 'will not',\n",
    "         \"won't've\": 'will not have',\n",
    "         'wont': 'will not',\n",
    "         'wontve': 'will not have',\n",
    "         \"would've\": 'would have',\n",
    "         \"wouldn't\": 'would not',\n",
    "         \"wouldn't've\": 'would not have',\n",
    "         'wouldnt': 'would not',\n",
    "         'wouldntve': 'would not have',\n",
    "         'wouldve': 'would have',\n",
    "         \"y'all\": 'you all',\n",
    "         \"y'all'd\": 'you all would',\n",
    "         \"y'all'd've\": 'you all would have',\n",
    "         \"y'all're\": 'you all are',\n",
    "         \"y'all've\": 'you all have',\n",
    "         \"y'alls\": 'you alls',\n",
    "         'yall': 'you all',\n",
    "         'yalld': 'you all would',\n",
    "         'yalldve': 'you all would have',\n",
    "         'yallre': 'you all are',\n",
    "         'yalls': 'you alls',\n",
    "         'yallve': 'you all have',\n",
    "         \"you'd\": 'you had',\n",
    "         \"you'd've\": 'you would have',\n",
    "         \"you'da\": 'you would have',\n",
    "         \"you'll\": 'you you will',\n",
    "         \"you'll've\": 'you you will have',\n",
    "         \"you're\": 'you are',\n",
    "         \"you've\": 'you have',\n",
    "         'youd': 'you had',\n",
    "         'youda': 'you would have',\n",
    "         'youdve': 'you would have',\n",
    "         'youll': 'you you will',\n",
    "         'youllve': 'you you will have',\n",
    "         'youre': 'you are',\n",
    "         'youve': 'you have',     \n",
    "         '2f4u': 'too fast for you',\n",
    "         '4yeo': 'for your eyes only',\n",
    "         'aamof': 'as a matter of fact',\n",
    "         'ack': 'acknowledgment',\n",
    "         'afaik': 'as far as i know',\n",
    "         'afair': 'as far as i recall',\n",
    "         'afk': 'away from keyboard',\n",
    "         'aka': 'also known as',\n",
    "         'ama': 'ask me anything',\n",
    "         'b2k btk': 'back to keyboard',\n",
    "         'bff': 'best friends forever',\n",
    "         'brah': 'bro',\n",
    "         'brb': 'be right back',\n",
    "         'bs': 'bullshit',\n",
    "         'btt': 'back to topic',\n",
    "         'btw': 'by the way',\n",
    "         'c&p': 'copy and paste',\n",
    "         'cu': 'see you',\n",
    "         'cys': 'check your settings',\n",
    "         'dgaf': 'donâ€™t give a fuck',\n",
    "         'dgmw': 'do not get me wrong',\n",
    "         'diy': 'do it yourself',\n",
    "         'dtf': 'down to fuck',\n",
    "         'eobd': 'end of business day',\n",
    "         'eod': 'end of discussion',\n",
    "         'eom': 'end of message',\n",
    "         'eot': 'end of transmission',\n",
    "         'fack': 'full acknowledge',\n",
    "         'faq': 'frequently asked questions',\n",
    "         'fb': 'facebook',\n",
    "         'fbf': 'flashback friday',\n",
    "         'fbo': 'facebook official',\n",
    "         'ffs': 'for fuck sake',\n",
    "         'fka': 'formerly known as',\n",
    "         'fomo': 'fear of missing out',\n",
    "         'ftw': 'for the win',\n",
    "         'fu': 'fuck you',\n",
    "         'fvck': 'fuck',\n",
    "         'fwiw': \"for what it's worth\",\n",
    "         'fyeo': 'for your eyes only',\n",
    "         'fyi': 'for your information',\n",
    "         'gg': 'good game',\n",
    "         'gtfo': 'get the fuck out',\n",
    "         'gtg': 'got to go',\n",
    "         'hbd': 'happy birthday',\n",
    "         'hf': 'have fun',\n",
    "         'hmb': 'hit me back',\n",
    "         'hmu': 'hit me up',\n",
    "         'hth': 'hope this helps',\n",
    "         'hwy': 'highway',\n",
    "         'icymi': 'in case you missed it',\n",
    "         'idc': 'i donâ€™t care',\n",
    "         'idk': 'i do not know',\n",
    "         'iirc': 'if i recall',\n",
    "         'ikr': 'i know right',\n",
    "         'ily': 'i love you',\n",
    "         'imho': 'in my humble opinion',\n",
    "         'imnsho': 'in my not so honest opinion',\n",
    "         'imo': 'in my opinion',\n",
    "         'iow': 'in other words',\n",
    "         'irl': 'in real life',\n",
    "         'itt': 'in this thread',\n",
    "         'jfyi': 'just for your information',\n",
    "         'jk': 'just kidding',\n",
    "         'lmk': 'let me know',\n",
    "         'lms': 'like my status',\n",
    "         'lol': 'laughing out loud',\n",
    "         'mcm': 'mancrush monday',\n",
    "         'mmw': 'mark my words',\n",
    "         'n/a': 'not applicable',\n",
    "         'n00b': 'newbie',\n",
    "         'nm': 'not much',\n",
    "         'nntr': 'no need to reply',\n",
    "         'noob': 'newbie',\n",
    "         'noyb': 'none of your business',\n",
    "         'nrn': 'no reply necessary',\n",
    "         'nsfw': 'not safe for work',\n",
    "         'nvm': 'nevermind',\n",
    "         'omg': 'oh my god',\n",
    "         'omw': 'on my way',\n",
    "         'ootd': 'outfit of the day',\n",
    "         'op': 'original post',\n",
    "         'ot': 'off topic',\n",
    "         'otoh': 'on the other hand',\n",
    "         'pebkac': 'problem exists between keyboard and chair',\n",
    "         'potd': 'photo of the day',\n",
    "         'pov': 'point of view',\n",
    "         'ppl': 'people',\n",
    "         'ptfo': 'passed the fuck out',\n",
    "         'r': 'are',\n",
    "         'rofl': 'rolling on the floor laughing',\n",
    "         'roflmao': 'rolling on floor laughing my ass off',\n",
    "         'rotfl': 'rolling on the floor laughing',\n",
    "         'rsvp': 'repondez sil vous plait',\n",
    "         'rt': 'retweet',\n",
    "         'rtfm': 'read the fine manual',\n",
    "         'scnr': 'sorry could not resist',\n",
    "         'sflr': 'sorry for late reply',\n",
    "         'sfw': 'safe for work',\n",
    "         'smfh': 'shaking my fucking head',\n",
    "         'spoc': 'single point of contact',\n",
    "         'stfu': 'shut the fuck up',\n",
    "         'sup': 'what is up',\n",
    "         'tba': 'to be announced',\n",
    "         'tbc': 'to be continued',\n",
    "         'tbh': 'to be honest',\n",
    "         'tbt': 'throwback thursday',\n",
    "         'tgif': 'thanks god it is friday',\n",
    "         'thx': 'thanks',\n",
    "         'tia': 'thanks in advance',\n",
    "         'tmi': 'too much information',\n",
    "         'tnx': 'thanks',\n",
    "         'tq': 'thank you',\n",
    "         'ttyl': 'talk to you later',\n",
    "         'ttyn': 'talk to you never',\n",
    "         'ttys': 'talk to you soon',\n",
    "         'txt': 'text',\n",
    "         'tyt': 'take your time',\n",
    "         'tyvm': 'thank you very much',\n",
    "         'u': 'you',\n",
    "         'ur': 'your',\n",
    "         'w00t': 'woot',\n",
    "         'wcw': 'womancrush wednesday',\n",
    "         'wfm': 'works for me',\n",
    "         'wrt': 'with regard to',\n",
    "         'wtf': 'what the fuck',\n",
    "         'wth': 'what the hell',\n",
    "         'yam': 'yet another meeting',\n",
    "         'ymmd': 'you made my day',\n",
    "         'ymmv': 'your mileage may vary',\n",
    "         'yolo': 'you only live once',\n",
    "}\n",
    "    c_re = re.compile('|'.join('(\\b%s\\b)' % re.escape(s) for s in word_expansion_dict.keys()), re.IGNORECASE)\n",
    "    def replace(match):\n",
    "        expansion =  f\" {word_expansion_dict[match.group(0)]}\"\n",
    "        return expansion\n",
    "    text = c_re.sub(replace, text.lower())\n",
    "    return text\n",
    "\n",
    "    # function to expand contractions, remove urls and characters before tokenization processing\n",
    "    def denoise_text(text):\n",
    "        new_text = re.sub(r\"\\S*https?:\\S*\",  r\"\", text.lower())\n",
    "        new_text_contractions = expand_abbreviations_contractions(new_text)\n",
    "        x = re.sub('#cafire', 'california fire', new_text_contractions)\n",
    "        x = re.sub('#calfires', 'california fires', x)\n",
    "        x = re.sub('#calwildfires', 'california wildfires', x)\n",
    "        x = re.sub('#cadrought', 'california drought', x)\n",
    "        new_text_punct = re.sub(r\"[^\\w\\s@]\",  r\"\", x)\n",
    "        new_text_chars = re.sub('[^\\u0000-\\u007f]', '',  new_text_punct)\n",
    "        strip_text = new_text_chars.strip()\n",
    "        #remove_hashtags_text = re.sub('#\\w+', '',  strip_text)\n",
    "        return strip_text \n",
    "    \n",
    "# tokenization & lemmatization function returns tokens    \n",
    "    def lemmatize_text(text):\n",
    "        tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        return [lemmatizer.lemmatize(w, pos='v') for w in tokenizer.tokenize(text)]\n",
    "\n",
    "# tokenization & stemmer function returns tokens\n",
    "    def stem_text(text):\n",
    "        tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "        stemmer = PorterStemmer()\n",
    "        return [stemmer.stem(w) for w in tokenizer.tokenize(text)]\n",
    "\n",
    "    def replace_numbers(tokens):\n",
    "# replace integers with string formatted words for numbers\n",
    "        dig2word = inflect.engine()\n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            if word.isdigit():\n",
    "                new_word = dig2word.number_to_words(word)\n",
    "                new_tokens.append(new_word)\n",
    "            else:\n",
    "                new_tokens.append(word)\n",
    "        return new_tokens\n",
    "    \n",
    "    def remove_non_ascii(tokens):\n",
    "# remove non ascii characters from text\n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            new_token = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_tokens.append(new_token)\n",
    "        return new_tokens\n",
    "    \n",
    "# remove stopwords   \n",
    "    def remove_stopwords(tokens):\n",
    "        stop_list = stopwords.words('english')  \n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            if word not in stop_list:\n",
    "                new_tokens.append(word)\n",
    "        return new_tokens\n",
    "  \n",
    " \n",
    "    def norm_text(tokens):\n",
    "        words = replace_numbers(tokens)\n",
    "        #tokens = remove_stopwords(words)\n",
    "        words = remove_non_ascii(words)\n",
    "        return words\n",
    "    \n",
    "\n",
    "    def process_text(text):\n",
    "        clean_text = denoise_text(text)\n",
    "        lem_text = lemmatize_text(clean_text)\n",
    "        text = ' '.join([x for x in norm_text(lem_text)])\n",
    "        text = re.sub(r\"-\",  r\" \", text)\n",
    "        return text\n",
    "    \n",
    "    new_df = [process_text(x) for x in df]\n",
    "\n",
    "    return new_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "boxed-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hashtags(tweets):\n",
    "    hashtag_list = []\n",
    "    tweets = list(tweets)\n",
    "    def denoise_text(text):\n",
    "        new_text = re.sub(r\"\\S*https?:\\S*\",  r\"\", text.lower())\n",
    "        new_text_punct = re.sub(r\"[^\\w\\s]\",  r\"\", new_text)\n",
    "        new_text_chars = re.sub('[^\\u0000-\\u007f]', '',  new_text_punct)\n",
    "        new_text_ = re.sub('_', '',  new_text_punct)\n",
    "        return new_text_\n",
    "    def replace_numbers(tokens):\n",
    "# replace integers with string formatted words for numbers\n",
    "        dig2word = inflect.engine()\n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            if word.isdigit():\n",
    "                new_word = dig2word.number_to_words(word)\n",
    "                new_tokens.append(new_word)\n",
    "            else:\n",
    "                new_tokens.append(word)\n",
    "        return new_tokens\n",
    "    # splitting the text into words\n",
    "    for tweet in tweets:\n",
    "        for x in tweet.split():\n",
    "            if x.startswith('#') == True:\n",
    "                clean_text = denoise_text(x)\n",
    "                cleaner_text = replace_numbers(clean_text)\n",
    "                hashtag_list.append(''.join(cleaner_text))\n",
    "    return hashtag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "skilled-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_location(df):\n",
    "    import re\n",
    "def word_expansion(text):\n",
    "    word_expansion_dict = {\n",
    "         'AK': 'Alaska',\n",
    "         'AL': 'Alabama',\n",
    "         'AR': 'Arkansas',\n",
    "         'AZ': 'Arizona',\n",
    "         'CA': 'California',\n",
    "         'CO': 'Colorado',\n",
    "         'CT': 'Connecticut',\n",
    "         'DC': 'District of Columbia',\n",
    "         'DE': 'Delaware',\n",
    "         'FL': 'Florida',\n",
    "         'GA': 'Georgia',\n",
    "         'GU': 'Guam',\n",
    "         'HI': 'Hawaii',\n",
    "         'IA': 'Iowa',\n",
    "         'ID': 'Idaho',\n",
    "         'IL': 'Illinois',\n",
    "         'IN': 'Indiana',\n",
    "         'KS': 'Kansas',\n",
    "         'KY': 'Kentucky',\n",
    "         'LA': 'Louisiana',\n",
    "         'MA': 'Massachusetts',\n",
    "         'MD': 'Maryland',\n",
    "         'ME': 'Maine',\n",
    "         'MI': 'Michigan',\n",
    "         'MN': 'Minnesota',\n",
    "         'MO': 'Missouri',\n",
    "         'MP': 'Northern Mariana Islands',\n",
    "         'MS': 'Mississippi',\n",
    "         'MT': 'Montana',\n",
    "         'NC': 'North Carolina',\n",
    "         'ND': 'North Dakota',\n",
    "         'NE': 'Nebraska',\n",
    "         'NH': 'New Hampshire',\n",
    "         'NJ': 'New Jersey',\n",
    "         'NM': 'New Mexico',\n",
    "         'NV': 'Nevada',\n",
    "         'NY': 'New York',\n",
    "         'OH': 'Ohio',\n",
    "         'OK': 'Oklahoma',\n",
    "         'OR': 'Oregon',\n",
    "         'PA': 'Pennsylvania',\n",
    "         'PR': 'Puerto Rico',\n",
    "         'PW': 'Palau',\n",
    "         'RI': 'Rhode Island',\n",
    "         'SC': 'South Carolina',\n",
    "         'SD': 'South Dakota',\n",
    "         'TN': 'Tennessee',\n",
    "         'TX': 'Texas',\n",
    "         'UT': 'Utah',\n",
    "         'VA': 'Virginia',\n",
    "         'VI': 'Virgin Islands',\n",
    "         'VT': 'Vermont',\n",
    "         'WA': 'Washington',\n",
    "         'WI': 'Wisconsin',\n",
    "         'WV': 'West Virginia',\n",
    "         'WY': 'Wyoming',\n",
    "         'AB': 'Alberta',\n",
    "          'BC': 'British Columbia',\n",
    "          'MB': 'Manitoba',\n",
    "          'NB': 'New Brunswick',\n",
    "          'NL': 'Newfoundland and Labrador',\n",
    "          'NS': 'Nova Scotia',\n",
    "          'NT': 'Northwest Territories',\n",
    "          'NU': 'Nunavut',\n",
    "          'ON': 'Ontario',\n",
    "          'PE': 'Prince Edward Island',\n",
    "          'QC': 'Quebec',\n",
    "          'SK': 'Saskatchewan',\n",
    "          'YT': 'Yukon',\n",
    "         \"\\'cause\": 'because',\n",
    "         '1st': 'first',\n",
    "         '2nd': 'second',\n",
    "         '3rd': 'third',\n",
    "         '4th': 'fourth',\n",
    "         \"ain't\": 'are not',\n",
    "         'aint': 'are not',\n",
    "         \"aren't\": 'are not',\n",
    "         'arent': 'are not',\n",
    "         'b/c': 'because',\n",
    "         'bc': 'because',\n",
    "         \"can't\": 'can not',\n",
    "         \"can't've\": 'can not have',\n",
    "         'cant': 'can not',\n",
    "         'cause': 'because',\n",
    "         \"could've\": 'could have',\n",
    "         \"couldn't\": 'could not',\n",
    "         \"couldn't've\": 'could not have',\n",
    "         'couldnt': 'could not',\n",
    "         'couldve': 'could have',\n",
    "         \"didn't\": 'did not',\n",
    "         'didnt': 'did not',\n",
    "         \"doesn't\": 'does not',\n",
    "         'doesnt': 'does not',\n",
    "         \"don't\": 'do not',\n",
    "         'dont': 'do not',\n",
    "         \"hadn't\": 'had not',\n",
    "         \"hadn't've\": 'had not have',\n",
    "         'hadnt': 'had not',\n",
    "         \"hasn't\": 'has not',\n",
    "         'hasnt': 'has not',\n",
    "         \"haven't\": 'have not',\n",
    "         'havent': 'have not',\n",
    "         \"he'd\": 'he would',\n",
    "         \"he'd've\": 'he would have',\n",
    "         \"he'll\": 'he will',\n",
    "         \"he'll've\": 'he will have',\n",
    "         \"he's\": 'he is',\n",
    "         'hes': 'he is',\n",
    "         \"how'd\": 'how did',\n",
    "         \"how'd'y\": 'how do you',\n",
    "         \"how'll\": 'how will',\n",
    "         \"how's\": 'how is',\n",
    "         'howd': 'how did',\n",
    "         'howdy': 'how do you',\n",
    "         'howll': 'how will',\n",
    "         'hows': 'how is',\n",
    "         \"i'd\": 'i would',\n",
    "         \"i'd've\": 'i would have',\n",
    "         \"i'll\": 'i will',\n",
    "         \"i'll've\": 'i will have',\n",
    "         \"i'm\": 'i am',\n",
    "         \"i've\": 'i have',\n",
    "         'id': 'i would',\n",
    "         'ida': 'i would have',\n",
    "         'im': 'i am',\n",
    "         \"isn't\": 'is not',\n",
    "         'isnt': 'is not',\n",
    "         \"it'd\": 'it had',\n",
    "         \"it'd've\": 'it would have',\n",
    "         \"it'll\": 'it will',\n",
    "         \"it'll've\": 'it will have',\n",
    "         \"it's\": 'it is',\n",
    "         'itd': 'it had',\n",
    "         'itll': 'it will',\n",
    "         'its': 'it is',\n",
    "         'ive': 'i have',\n",
    "         \"let's\": 'let us',\n",
    "         'lets': 'let us',\n",
    "         \"ma'am\": 'madam',\n",
    "         'maam': 'madam',\n",
    "         \"mayn't\": 'may not',\n",
    "         \"might've\": 'might have',\n",
    "         'mighta': 'might have',\n",
    "         \"mightn't\": 'might not',\n",
    "         \"mightn't've\": 'might not have',\n",
    "         'mightnt': 'might not',\n",
    "         'mightve': 'might have',\n",
    "         \"must've\": 'must have',\n",
    "         'musta': 'must have',\n",
    "         \"mustn't\": 'must not',\n",
    "         \"mustn't've\": 'must not have',\n",
    "         'mustnt': 'must not',\n",
    "         'mustve': 'must have',\n",
    "         \"needn't\": 'need not',\n",
    "         \"needn't've\": 'need not have',\n",
    "         'neednt': 'need not',\n",
    "         \"o'clock\": 'of the clock',\n",
    "         'oclock': 'of the clock',\n",
    "         \"oughtn't\": 'ought not',\n",
    "         \"oughtn't've\": 'ought not have',\n",
    "         \"sha'n't\": 'shall not',\n",
    "         \"shan't\": 'shall not',\n",
    "         \"shan't've\": 'shall not have',\n",
    "         \"she'd\": 'she would',\n",
    "         \"she'd've\": 'she would have',\n",
    "         \"she'll\": 'she will',\n",
    "         \"she'll've\": 'she will have',\n",
    "         \"she's\": 'she is',\n",
    "         'shes': 'she is',\n",
    "         \"should've\": 'should have',\n",
    "         'shoulda': 'should have',\n",
    "         \"shouldn't\": 'should not',\n",
    "         \"shouldn't've\": 'should not have',\n",
    "         'shouldnt': 'should not',\n",
    "         'shouldve': 'should have',\n",
    "         \"so'd\": 'so did',\n",
    "         \"so's\": 'so is',\n",
    "         \"so've\": 'so have',\n",
    "         \"that'd\": 'that would',\n",
    "         \"that'd've\": 'that would have',\n",
    "         \"that's\": 'that is',\n",
    "         'thatd': 'that would',\n",
    "         'thats': 'that is',\n",
    "         \"there'd\": 'there had',\n",
    "         \"there'd've\": 'there would have',\n",
    "         \"there's\": 'there is',\n",
    "         'thered': 'there had',\n",
    "         'theres': 'there is',\n",
    "         \"they'd\": 'they would',\n",
    "         \"they'd've\": 'they would have',\n",
    "         \"they'll\": 'they will',\n",
    "         \"they'll've\": 'they will have',\n",
    "         \"they're\": 'they are',\n",
    "         \"they've\": 'they have',\n",
    "         'theyd': 'they would',\n",
    "         'theyda': 'they would have',\n",
    "         'theyll': 'they will',\n",
    "         'theyre': 'they are',\n",
    "         'theyve': 'they have',\n",
    "         \"to've\": 'to have',\n",
    "         \"wasn't\": 'was not',\n",
    "         'wasnt': 'was not',\n",
    "         \"we'd\": 'we had',\n",
    "         \"we'd've\": 'we would have',\n",
    "         \"we'll\": 'we will',\n",
    "         \"we'll've\": 'we will have',\n",
    "         \"we're\": 'we are',\n",
    "         \"we've\": 'we have',\n",
    "         \"weren't\": 'were not',\n",
    "         'werent': 'were not',\n",
    "         'weve': 'we have',\n",
    "         \"what'll\": 'what will',\n",
    "         \"what'll've\": 'what will have',\n",
    "         \"what're\": 'what are',\n",
    "         \"what's\": 'what is',\n",
    "         \"what've\": 'what have',\n",
    "         'whatll': 'what will',\n",
    "         'whatllve': 'what will have',\n",
    "         'whatre': 'what are',\n",
    "         'whats': 'what is',\n",
    "         'whatve': 'what have',\n",
    "         \"when's\": 'when is',\n",
    "         \"when've\": 'when have',\n",
    "         'whens': 'when is',\n",
    "         'whenve': 'when have',\n",
    "         \"where'd\": 'where did',\n",
    "         \"where's\": 'where is',\n",
    "         \"where've\": 'where have',\n",
    "         'whered': 'where did',\n",
    "         'whereve': 'where have',\n",
    "         'whers': 'where is',\n",
    "         \"who'll\": 'who will',\n",
    "         \"who'll've\": 'who will have',\n",
    "         \"who's\": 'who is',\n",
    "         \"who've\": 'who have',\n",
    "         'wholl': 'who will',\n",
    "         'whollve': 'who will have',\n",
    "         'whos': 'who is',\n",
    "         'whove': 'who have',\n",
    "         \"why's\": 'why is',\n",
    "         \"why've\": 'why have',\n",
    "         'whys': 'why is',\n",
    "         'whyve': 'why have',\n",
    "         \"will've\": 'will have',\n",
    "         'willve': 'will have',\n",
    "         \"won't\": 'will not',\n",
    "         \"won't've\": 'will not have',\n",
    "         'wont': 'will not',\n",
    "         'wontve': 'will not have',\n",
    "         \"would've\": 'would have',\n",
    "         \"wouldn't\": 'would not',\n",
    "         \"wouldn't've\": 'would not have',\n",
    "         'wouldnt': 'would not',\n",
    "         'wouldntve': 'would not have',\n",
    "         'wouldve': 'would have',\n",
    "         \"y'all\": 'you all',\n",
    "         \"y'all'd\": 'you all would',\n",
    "         \"y'all'd've\": 'you all would have',\n",
    "         \"y'all're\": 'you all are',\n",
    "         \"y'all've\": 'you all have',\n",
    "         \"y'alls\": 'you alls',\n",
    "         'yall': 'you all',\n",
    "         'yalld': 'you all would',\n",
    "         'yalldve': 'you all would have',\n",
    "         'yallre': 'you all are',\n",
    "         'yalls': 'you alls',\n",
    "         'yallve': 'you all have',\n",
    "         \"you'd\": 'you had',\n",
    "         \"you'd've\": 'you would have',\n",
    "         \"you'da\": 'you would have',\n",
    "         \"you'll\": 'you you will',\n",
    "         \"you'll've\": 'you you will have',\n",
    "         \"you're\": 'you are',\n",
    "         \"you've\": 'you have',\n",
    "         'youd': 'you had',\n",
    "         'youda': 'you would have',\n",
    "         'youdve': 'you would have',\n",
    "         'youll': 'you you will',\n",
    "         'youllve': 'you you will have',\n",
    "         'youre': 'you are',\n",
    "         'youve': 'you have',     \n",
    "         '2f4u': 'too fast for you',\n",
    "         '4yeo': 'for your eyes only',\n",
    "         'aamof': 'as a matter of fact',\n",
    "         'ack': 'acknowledgment',\n",
    "         'afaik': 'as far as i know',\n",
    "         'afair': 'as far as i recall',\n",
    "         'afk': 'away from keyboard',\n",
    "         'aka': 'also known as',\n",
    "         'ama': 'ask me anything',\n",
    "         'b2k btk': 'back to keyboard',\n",
    "         'bff': 'best friends forever',\n",
    "         'brah': 'bro',\n",
    "         'brb': 'be right back',\n",
    "         'bs': 'bullshit',\n",
    "         'btt': 'back to topic',\n",
    "         'btw': 'by the way',\n",
    "         'c&p': 'copy and paste',\n",
    "         'cu': 'see you',\n",
    "         'cys': 'check your settings',\n",
    "         'dgaf': 'donâ€™t give a fuck',\n",
    "         'dgmw': 'do not get me wrong',\n",
    "         'diy': 'do it yourself',\n",
    "         'dtf': 'down to fuck',\n",
    "         'eobd': 'end of business day',\n",
    "         'eod': 'end of discussion',\n",
    "         'eom': 'end of message',\n",
    "         'eot': 'end of transmission',\n",
    "         'fack': 'full acknowledge',\n",
    "         'faq': 'frequently asked questions',\n",
    "         'fb': 'facebook',\n",
    "         'fbf': 'flashback friday',\n",
    "         'fbo': 'facebook official',\n",
    "         'ffs': 'for fuck sake',\n",
    "         'fka': 'formerly known as',\n",
    "         'fomo': 'fear of missing out',\n",
    "         'ftw': 'for the win',\n",
    "         'fu': 'fuck you',\n",
    "         'fvck': 'fuck',\n",
    "         'fwiw': \"for what it's worth\",\n",
    "         'fyeo': 'for your eyes only',\n",
    "         'fyi': 'for your information',\n",
    "         'gg': 'good game',\n",
    "         'gtfo': 'get the fuck out',\n",
    "         'gtg': 'got to go',\n",
    "         'hbd': 'happy birthday',\n",
    "         'hf': 'have fun',\n",
    "         'hmb': 'hit me back',\n",
    "         'hmu': 'hit me up',\n",
    "         'hth': 'hope this helps',\n",
    "         'hwy': 'highway',\n",
    "         'icymi': 'in case you missed it',\n",
    "         'idc': 'i donâ€™t care',\n",
    "         'idk': 'i do not know',\n",
    "         'iirc': 'if i recall',\n",
    "         'ikr': 'i know right',\n",
    "         'ily': 'i love you',\n",
    "         'imho': 'in my humble opinion',\n",
    "         'imnsho': 'in my not so honest opinion',\n",
    "         'imo': 'in my opinion',\n",
    "         'iow': 'in other words',\n",
    "         'irl': 'in real life',\n",
    "         'itt': 'in this thread',\n",
    "         'jfyi': 'just for your information',\n",
    "         'jk': 'just kidding',\n",
    "         'lmk': 'let me know',\n",
    "         'lms': 'like my status',\n",
    "         'lol': 'laughing out loud',\n",
    "         'mcm': 'mancrush monday',\n",
    "         'mmw': 'mark my words',\n",
    "         'n/a': 'not applicable',\n",
    "         'n00b': 'newbie',\n",
    "         'nm': 'not much',\n",
    "         'nntr': 'no need to reply',\n",
    "         'noob': 'newbie',\n",
    "         'noyb': 'none of your business',\n",
    "         'nrn': 'no reply necessary',\n",
    "         'nsfw': 'not safe for work',\n",
    "         'nvm': 'nevermind',\n",
    "         'omg': 'oh my god',\n",
    "         'omw': 'on my way',\n",
    "         'ootd': 'outfit of the day',\n",
    "         'op': 'original post',\n",
    "         'ot': 'off topic',\n",
    "         'otoh': 'on the other hand',\n",
    "         'pebkac': 'problem exists between keyboard and chair',\n",
    "         'potd': 'photo of the day',\n",
    "         'pov': 'point of view',\n",
    "         'ppl': 'people',\n",
    "         'ptfo': 'passed the fuck out',\n",
    "         'r': 'are',\n",
    "         'rofl': 'rolling on the floor laughing',\n",
    "         'roflmao': 'rolling on floor laughing my ass off',\n",
    "         'rotfl': 'rolling on the floor laughing',\n",
    "         'rsvp': 'repondez sil vous plait',\n",
    "         'rt': 'retweet',\n",
    "         'rtfm': 'read the fine manual',\n",
    "         'scnr': 'sorry could not resist',\n",
    "         'sflr': 'sorry for late reply',\n",
    "         'sfw': 'safe for work',\n",
    "         'smfh': 'shaking my fucking head',\n",
    "         'spoc': 'single point of contact',\n",
    "         'stfu': 'shut the fuck up',\n",
    "         'sup': 'what is up',\n",
    "         'tba': 'to be announced',\n",
    "         'tbc': 'to be continued',\n",
    "         'tbh': 'to be honest',\n",
    "         'tbt': 'throwback thursday',\n",
    "         'tgif': 'thanks god it is friday',\n",
    "         'thx': 'thanks',\n",
    "         'tia': 'thanks in advance',\n",
    "         'tmi': 'too much information',\n",
    "         'tnx': 'thanks',\n",
    "         'tq': 'thank you',\n",
    "         'ttyl': 'talk to you later',\n",
    "         'ttyn': 'talk to you never',\n",
    "         'ttys': 'talk to you soon',\n",
    "         'txt': 'text',\n",
    "         'tyt': 'take your time',\n",
    "         'tyvm': 'thank you very much',\n",
    "         'u': 'you',\n",
    "         'ur': 'your',\n",
    "         'w00t': 'woot',\n",
    "         'wcw': 'womancrush wednesday',\n",
    "         'wfm': 'works for me',\n",
    "         'wrt': 'with regard to',\n",
    "         'wtf': 'what the fuck',\n",
    "         'wth': 'what the hell',\n",
    "         'yam': 'yet another meeting',\n",
    "         'ymmd': 'you made my day',\n",
    "         'ymmv': 'your mileage may vary',\n",
    "         'yolo': 'you only live once',\n",
    "}\n",
    "    c_re = re.compile('|'.join('(\\b%s\\b)' % (re.escape(s) for s in word_expansions.keys()), re.IGNORECASE))\n",
    "    def replace(match):\n",
    "        expansion =  f\" {contractions_dict[match.group(0)]}\"\n",
    "        return expansion\n",
    "    text = c_re.sub(replace, text.lower())\n",
    "    return text\n",
    "    # tokenization & lemmatization function returns tokens    \n",
    "    def lemmatize_text(text):\n",
    "        tokenizer = TweetTokenizer(strip_handles=True)\n",
    "        lemmatizer = WordNetLemmatizer() \n",
    "        return [lemmatizer.lemmatize(w, pos='n') for w in tokenizer.tokenize(text)]\n",
    "\n",
    "    \n",
    "    def remove_non_ascii(tokens):\n",
    "# remove non ascii characters from text\n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            new_token = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_tokens.append(new_token)\n",
    "        return new_tokens\n",
    "    \n",
    "    def norm_text(tokens):\n",
    "        words = remove_non_ascii(tokens)\n",
    "        return words\n",
    "    \n",
    "    def denoise_location_text(text):\n",
    "        text = str(text)\n",
    "        new_text = re.sub(r\"\\S*https?:\\S*\",  r\"\", text.lower())\n",
    "        new_string = re.sub(r\"[^\\w\\s]\",  r\" \", new_text)\n",
    "        new_string = re.sub(r\"\\d\",  r\"\", new_string)\n",
    "        unicode_string = re.sub('[^\\u0000-\\u007f]', '',  new_string)\n",
    "        new_text_contractions = expand_abbreviations_contractions(unicode_string)\n",
    "        clean_text = re.sub(r\"est september   \",  r\"\", new_text_contractions)\n",
    "        lem_text = lemmatize_text(clean_text)\n",
    "        text = ' '.join([x for x in norm_text(lem_text)])\n",
    "        text = re.sub(r\"-\",  r\" \", text)\n",
    "        return text\n",
    "\n",
    "    \n",
    "    new_df = [denoise_location_text(x) for x in df]\n",
    "    return new_df \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "intimate-ratio",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_expansion_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-1672ff077661>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#hashtags = extract_hashtags(df.text)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tweets'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34mr\"([#[A-Z][a-z])\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34mr\" \\1\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'keyword'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtweet_preprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34mr\"%20\"\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34mr\" \"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-02924b22c417>\u001b[0m in \u001b[0;36mtweet_preprocess\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m    420\u001b[0m          \u001b[1;34m'yolo'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'you only live once'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m }\n\u001b[1;32m--> 422\u001b[1;33m     \u001b[0mc_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'(\\b%s\\b)'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mescape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_expansion_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIGNORECASE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    423\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m         \u001b[0mexpansion\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;34mf\" {word_expansion_dict[match.group(0)]}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_expansion_dict' is not defined"
     ]
    }
   ],
   "source": [
    "#hashtags = extract_hashtags(df.text)\n",
    "df['tweets'] = tweet_preprocess((df.text.astype(str).replace({r\"([#[A-Z][a-z])\": r\" \\1\"}, regex=True)))\n",
    "df['keyword'] = tweet_preprocess(df.keyword.astype(str).replace({r\"%20\" : r\" \"}, regex=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "selective-price",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds be the reason of this earthquake may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents ask to shelter in place be be no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>thirteen thousand people receive wildfires eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>just get send this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#RockyFire Update =&gt; California Hwy. 20 closed...</td>\n",
       "      <td>1</td>\n",
       "      <td>rocky fire update california highway twenty cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#flood #disaster Heavy rain causes flash flood...</td>\n",
       "      <td>1</td>\n",
       "      <td>flood disaster heavy rain becauses flash flood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>13</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm on top of the hill and I can see a fire in...</td>\n",
       "      <td>1</td>\n",
       "      <td>im on top of the hill and i can see a fire in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's an emergency evacuation happening now ...</td>\n",
       "      <td>1</td>\n",
       "      <td>there be an emergency evacuation happen now in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm afraid that the tornado is coming to our a...</td>\n",
       "      <td>1</td>\n",
       "      <td>im afraid that the tornado be come to our area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Three people died from the heat wave so far</td>\n",
       "      <td>1</td>\n",
       "      <td>three people die from the heat wave so far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Haha South Tampa is getting flooded hah- WAIT ...</td>\n",
       "      <td>1</td>\n",
       "      <td>haha south tampa be get flood hah wait a secon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>18</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#raining #flooding #Florida #TampaBay #Tampa 1...</td>\n",
       "      <td>1</td>\n",
       "      <td>rain flood florida tampa bay tampa eighteen or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>19</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#Flood in Bago Myanmar #We arrived Bago</td>\n",
       "      <td>1</td>\n",
       "      <td>flood in bago myanmar we arrive bago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>20</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Damage to school bus on 80 in multi car crash ...</td>\n",
       "      <td>1</td>\n",
       "      <td>damage to school bus on eighty in multi car cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>23</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What's up man?</td>\n",
       "      <td>0</td>\n",
       "      <td>what be up man</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I love fruits</td>\n",
       "      <td>0</td>\n",
       "      <td>i love fruit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Summer is lovely</td>\n",
       "      <td>0</td>\n",
       "      <td>summer be lovely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>26</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My car is so fast</td>\n",
       "      <td>0</td>\n",
       "      <td>my car be so fast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>28</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a goooooooaaaaaal!!!!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>what a goooaaal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>31</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is ridiculous....</td>\n",
       "      <td>0</td>\n",
       "      <td>this be ridiculous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London is cool ;)</td>\n",
       "      <td>0</td>\n",
       "      <td>london be cool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love skiing</td>\n",
       "      <td>0</td>\n",
       "      <td>love ski</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>34</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>What a wonderful day!</td>\n",
       "      <td>0</td>\n",
       "      <td>what a wonderful day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LOOOOOOL</td>\n",
       "      <td>0</td>\n",
       "      <td>loool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No way...I can't eat that shit</td>\n",
       "      <td>0</td>\n",
       "      <td>no wayi can not eat that shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Was in NYC last week!</td>\n",
       "      <td>0</td>\n",
       "      <td>be in nyc last week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Love my girlfriend</td>\n",
       "      <td>0</td>\n",
       "      <td>love my girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>40</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cooool :)</td>\n",
       "      <td>0</td>\n",
       "      <td>coool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>41</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Do you like pasta?</td>\n",
       "      <td>0</td>\n",
       "      <td>do you like pasta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>44</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The end!</td>\n",
       "      <td>0</td>\n",
       "      <td>the end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "      <td>wholesale market ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "      <td>we always try to bring the heavy metal rt ao1e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "      <td>africanbaze break news nigeria flag set ablaze...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "      <td>cry out for more set me ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "      <td>on plus side look at the sky last night it be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>54</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Pretoria</td>\n",
       "      <td>@PhDSquares #mufc they've built so much hype a...</td>\n",
       "      <td>0</td>\n",
       "      <td>@ phd square mufc they have build so much hype...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>55</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>World Wide!!</td>\n",
       "      <td>INEC Office in Abia Set Ablaze - http://t.co/3...</td>\n",
       "      <td>1</td>\n",
       "      <td>inec office in abia set ablaze i amaomknna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>56</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Barbados #Bridgetown JAMAICA Â‰Ã›Ã’ Two cars set ...</td>\n",
       "      <td>1</td>\n",
       "      <td>barbados bridgetown jamaica two cars set ablaz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>57</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Paranaque City</td>\n",
       "      <td>Ablaze for you Lord :D</td>\n",
       "      <td>0</td>\n",
       "      <td>ablaze for you lord d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>59</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>check these out smejj tj8 zjin twenty one uix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>61</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>on the outside you're ablaze and alive\\nbut yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>on the outside you be ablaze and alive but you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>62</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>milky way</td>\n",
       "      <td>Had an awesome time visiting the CFC head offi...</td>\n",
       "      <td>0</td>\n",
       "      <td>have an awesome time visit the cfc head office...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>63</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SOOOO PUMPED FOR ABLAZE ???? @southridgelife</td>\n",
       "      <td>0</td>\n",
       "      <td>sooo pump for ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>64</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I wanted to set Chicago ablaze with my preachi...</td>\n",
       "      <td>0</td>\n",
       "      <td>i want to set chicago ablaze with my preach bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>65</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I gained 3 followers in the last week. You? Kn...</td>\n",
       "      <td>0</td>\n",
       "      <td>i gain three followers in the last week you kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>66</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>GREENSBORO,NORTH CAROLINA</td>\n",
       "      <td>How the West was burned: Thousands of wildfire...</td>\n",
       "      <td>1</td>\n",
       "      <td>how the west be burn thousands of wildfires ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>67</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Building the perfect tracklist to life leave t...</td>\n",
       "      <td>0</td>\n",
       "      <td>build the perfect tracklist to life leave the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>68</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Live On Webcam</td>\n",
       "      <td>Check these out: http://t.co/rOI2NSmEJJ http:/...</td>\n",
       "      <td>0</td>\n",
       "      <td>check these out smejj tj8 zjin twenty one uix ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>71</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>England.</td>\n",
       "      <td>First night with retainers in. It's quite weir...</td>\n",
       "      <td>0</td>\n",
       "      <td>first night with retainers in it be quite weir...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id keyword                       location  \\\n",
       "0    1     nan                            NaN   \n",
       "1    4     nan                            NaN   \n",
       "2    5     nan                            NaN   \n",
       "3    6     nan                            NaN   \n",
       "4    7     nan                            NaN   \n",
       "5    8     nan                            NaN   \n",
       "6   10     nan                            NaN   \n",
       "7   13     nan                            NaN   \n",
       "8   14     nan                            NaN   \n",
       "9   15     nan                            NaN   \n",
       "10  16     nan                            NaN   \n",
       "11  17     nan                            NaN   \n",
       "12  18     nan                            NaN   \n",
       "13  19     nan                            NaN   \n",
       "14  20     nan                            NaN   \n",
       "15  23     nan                            NaN   \n",
       "16  24     nan                            NaN   \n",
       "17  25     nan                            NaN   \n",
       "18  26     nan                            NaN   \n",
       "19  28     nan                            NaN   \n",
       "20  31     nan                            NaN   \n",
       "21  32     nan                            NaN   \n",
       "22  33     nan                            NaN   \n",
       "23  34     nan                            NaN   \n",
       "24  36     nan                            NaN   \n",
       "25  37     nan                            NaN   \n",
       "26  38     nan                            NaN   \n",
       "27  39     nan                            NaN   \n",
       "28  40     nan                            NaN   \n",
       "29  41     nan                            NaN   \n",
       "30  44     nan                            NaN   \n",
       "31  48  ablaze                     Birmingham   \n",
       "32  49  ablaze  Est. September 2012 - Bristol   \n",
       "33  50  ablaze                         AFRICA   \n",
       "34  52  ablaze               Philadelphia, PA   \n",
       "35  53  ablaze                     London, UK   \n",
       "36  54  ablaze                       Pretoria   \n",
       "37  55  ablaze                   World Wide!!   \n",
       "38  56  ablaze                            NaN   \n",
       "39  57  ablaze                 Paranaque City   \n",
       "40  59  ablaze                 Live On Webcam   \n",
       "41  61  ablaze                            NaN   \n",
       "42  62  ablaze                      milky way   \n",
       "43  63  ablaze                            NaN   \n",
       "44  64  ablaze                            NaN   \n",
       "45  65  ablaze                            NaN   \n",
       "46  66  ablaze      GREENSBORO,NORTH CAROLINA   \n",
       "47  67  ablaze                            NaN   \n",
       "48  68  ablaze                 Live On Webcam   \n",
       "49  71  ablaze                       England.   \n",
       "\n",
       "                                                 text  target  \\\n",
       "0   Our Deeds are the Reason of this #earthquake M...       1   \n",
       "1              Forest fire near La Ronge Sask. Canada       1   \n",
       "2   All residents asked to 'shelter in place' are ...       1   \n",
       "3   13,000 people receive #wildfires evacuation or...       1   \n",
       "4   Just got sent this photo from Ruby #Alaska as ...       1   \n",
       "5   #RockyFire Update => California Hwy. 20 closed...       1   \n",
       "6   #flood #disaster Heavy rain causes flash flood...       1   \n",
       "7   I'm on top of the hill and I can see a fire in...       1   \n",
       "8   There's an emergency evacuation happening now ...       1   \n",
       "9   I'm afraid that the tornado is coming to our a...       1   \n",
       "10        Three people died from the heat wave so far       1   \n",
       "11  Haha South Tampa is getting flooded hah- WAIT ...       1   \n",
       "12  #raining #flooding #Florida #TampaBay #Tampa 1...       1   \n",
       "13            #Flood in Bago Myanmar #We arrived Bago       1   \n",
       "14  Damage to school bus on 80 in multi car crash ...       1   \n",
       "15                                     What's up man?       0   \n",
       "16                                      I love fruits       0   \n",
       "17                                   Summer is lovely       0   \n",
       "18                                  My car is so fast       0   \n",
       "19                       What a goooooooaaaaaal!!!!!!       0   \n",
       "20                             this is ridiculous....       0   \n",
       "21                                  London is cool ;)       0   \n",
       "22                                        Love skiing       0   \n",
       "23                              What a wonderful day!       0   \n",
       "24                                           LOOOOOOL       0   \n",
       "25                     No way...I can't eat that shit       0   \n",
       "26                              Was in NYC last week!       0   \n",
       "27                                 Love my girlfriend       0   \n",
       "28                                          Cooool :)       0   \n",
       "29                                 Do you like pasta?       0   \n",
       "30                                           The end!       0   \n",
       "31  @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   \n",
       "32  We always try to bring the heavy. #metal #RT h...       0   \n",
       "33  #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   \n",
       "34                 Crying out for more! Set me ablaze       0   \n",
       "35  On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0   \n",
       "36  @PhDSquares #mufc they've built so much hype a...       0   \n",
       "37  INEC Office in Abia Set Ablaze - http://t.co/3...       1   \n",
       "38  Barbados #Bridgetown JAMAICA Â‰Ã›Ã’ Two cars set ...       1   \n",
       "39                             Ablaze for you Lord :D       0   \n",
       "40  Check these out: http://t.co/rOI2NSmEJJ http:/...       0   \n",
       "41  on the outside you're ablaze and alive\\nbut yo...       0   \n",
       "42  Had an awesome time visiting the CFC head offi...       0   \n",
       "43       SOOOO PUMPED FOR ABLAZE ???? @southridgelife       0   \n",
       "44  I wanted to set Chicago ablaze with my preachi...       0   \n",
       "45  I gained 3 followers in the last week. You? Kn...       0   \n",
       "46  How the West was burned: Thousands of wildfire...       1   \n",
       "47  Building the perfect tracklist to life leave t...       0   \n",
       "48  Check these out: http://t.co/rOI2NSmEJJ http:/...       0   \n",
       "49  First night with retainers in. It's quite weir...       0   \n",
       "\n",
       "                                               tweets  \n",
       "0   our deeds be the reason of this earthquake may...  \n",
       "1               forest fire near la ronge sask canada  \n",
       "2   all residents ask to shelter in place be be no...  \n",
       "3   thirteen thousand people receive wildfires eva...  \n",
       "4   just get send this photo from ruby alaska as s...  \n",
       "5   rocky fire update california highway twenty cl...  \n",
       "6   flood disaster heavy rain becauses flash flood...  \n",
       "7   im on top of the hill and i can see a fire in ...  \n",
       "8   there be an emergency evacuation happen now in...  \n",
       "9      im afraid that the tornado be come to our area  \n",
       "10         three people die from the heat wave so far  \n",
       "11  haha south tampa be get flood hah wait a secon...  \n",
       "12  rain flood florida tampa bay tampa eighteen or...  \n",
       "13               flood in bago myanmar we arrive bago  \n",
       "14  damage to school bus on eighty in multi car cr...  \n",
       "15                                     what be up man  \n",
       "16                                       i love fruit  \n",
       "17                                   summer be lovely  \n",
       "18                                  my car be so fast  \n",
       "19                                    what a goooaaal  \n",
       "20                                 this be ridiculous  \n",
       "21                                     london be cool  \n",
       "22                                           love ski  \n",
       "23                               what a wonderful day  \n",
       "24                                              loool  \n",
       "25                      no wayi can not eat that shit  \n",
       "26                                be in nyc last week  \n",
       "27                                 love my girlfriend  \n",
       "28                                              coool  \n",
       "29                                  do you like pasta  \n",
       "30                                            the end  \n",
       "31                            wholesale market ablaze  \n",
       "32  we always try to bring the heavy metal rt ao1e...  \n",
       "33  africanbaze break news nigeria flag set ablaze...  \n",
       "34                     cry out for more set me ablaze  \n",
       "35  on plus side look at the sky last night it be ...  \n",
       "36  @ phd square mufc they have build so much hype...  \n",
       "37         inec office in abia set ablaze i amaomknna  \n",
       "38  barbados bridgetown jamaica two cars set ablaz...  \n",
       "39                              ablaze for you lord d  \n",
       "40  check these out smejj tj8 zjin twenty one uix ...  \n",
       "41  on the outside you be ablaze and alive but you...  \n",
       "42  have an awesome time visit the cfc head office...  \n",
       "43                               sooo pump for ablaze  \n",
       "44  i want to set chicago ablaze with my preach bu...  \n",
       "45  i gain three followers in the last week you kn...  \n",
       "46  how the west be burn thousands of wildfires ab...  \n",
       "47  build the perfect tracklist to life leave the ...  \n",
       "48  check these out smejj tj8 zjin twenty one uix ...  \n",
       "49  first night with retainers in it be quite weir...  "
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "speaking-trademark",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_location'] = denoise_location(df.location)\n",
    "df['clean_location'] = clean_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "unlikely-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = df.filter(['tweets','clean_location','keyword'], axis=1)\n",
    "train_y = df.filter(['target'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "passive-jersey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds be the reason of this earthquake may...\n",
       "1           forest fire near la ronge sask canada nan nan\n",
       "2       all residents ask to shelter in place be be no...\n",
       "3       thirteen thousand people receive wildfires eva...\n",
       "4       just get send this photo from ruby alaska as s...\n",
       "                              ...                        \n",
       "7604    world news fall powerlines on glink tram you p...\n",
       "7605    on the flip side i be at walmart and there be ...\n",
       "7606    suicide bomber kill fifteen in saudi security ...\n",
       "7608    two giant crane hold a bridge collapse into ne...\n",
       "7612    the latest more home raze by northern californ...\n",
       "Length: 7503, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = df.tweets + \" \" + df.clean_location + \" \" + df.keyword\n",
    "all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "median-civilian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>tweets</th>\n",
       "      <th>clean_location</th>\n",
       "      <th>all_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Our  Deeds are the  Reason of this #earthqua...</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds be the reason of this earthquake may...</td>\n",
       "      <td>nan</td>\n",
       "      <td>our deeds be the reason of this earthquake may...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Forest fire near  La  Ronge  Sask.  Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>nan</td>\n",
       "      <td>forest fire near la ronge sask canada nan nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>All residents asked to 'shelter in place' ar...</td>\n",
       "      <td>1</td>\n",
       "      <td>all residents ask to shelter in place be be no...</td>\n",
       "      <td>nan</td>\n",
       "      <td>all residents ask to shelter in place be be no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>13,000 people receive #wildfires evacuation o...</td>\n",
       "      <td>1</td>\n",
       "      <td>thirteen thousand people receive wildfires eva...</td>\n",
       "      <td>nan</td>\n",
       "      <td>thirteen thousand people receive wildfires eva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Just got sent this photo from  Ruby # Alaska...</td>\n",
       "      <td>1</td>\n",
       "      <td>just get send this photo from ruby alaska as s...</td>\n",
       "      <td>nan</td>\n",
       "      <td>just get send this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7604</th>\n",
       "      <td>10863</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td># World News  Fallen powerlines on  G:link tr...</td>\n",
       "      <td>1</td>\n",
       "      <td>world news fall powerlines on glink tram you p...</td>\n",
       "      <td>nan</td>\n",
       "      <td>world news fall powerlines on glink tram you p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>10864</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>on the flip side  I'm at  Walmart and there i...</td>\n",
       "      <td>1</td>\n",
       "      <td>on the flip side i be at walmart and there be ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>on the flip side i be at walmart and there be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>10866</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Suicide bomber kills 15 in  Saudi security s...</td>\n",
       "      <td>1</td>\n",
       "      <td>suicide bomber kill fifteen in saudi security ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>suicide bomber kill fifteen in saudi security ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>Two giant cranes holding a bridge collapse i...</td>\n",
       "      <td>1</td>\n",
       "      <td>two giant crane hold a bridge collapse into ne...</td>\n",
       "      <td>nan</td>\n",
       "      <td>two giant crane hold a bridge collapse into ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>The  Latest:  More  Homes  Razed by  Norther...</td>\n",
       "      <td>1</td>\n",
       "      <td>the latest more home raze by northern californ...</td>\n",
       "      <td>nan</td>\n",
       "      <td>the latest more home raze by northern californ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7503 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id keyword location  \\\n",
       "0         1     nan      nan   \n",
       "1         4     nan      nan   \n",
       "2         5     nan      nan   \n",
       "3         6     nan      nan   \n",
       "4         7     nan      nan   \n",
       "...     ...     ...      ...   \n",
       "7604  10863     nan      nan   \n",
       "7605  10864     nan      nan   \n",
       "7606  10866     nan      nan   \n",
       "7608  10869     nan      nan   \n",
       "7612  10873     nan      nan   \n",
       "\n",
       "                                                   text  target  \\\n",
       "0       Our  Deeds are the  Reason of this #earthqua...       1   \n",
       "1            Forest fire near  La  Ronge  Sask.  Canada       1   \n",
       "2       All residents asked to 'shelter in place' ar...       1   \n",
       "3      13,000 people receive #wildfires evacuation o...       1   \n",
       "4       Just got sent this photo from  Ruby # Alaska...       1   \n",
       "...                                                 ...     ...   \n",
       "7604   # World News  Fallen powerlines on  G:link tr...       1   \n",
       "7605   on the flip side  I'm at  Walmart and there i...       1   \n",
       "7606    Suicide bomber kills 15 in  Saudi security s...       1   \n",
       "7608    Two giant cranes holding a bridge collapse i...       1   \n",
       "7612    The  Latest:  More  Homes  Razed by  Norther...       1   \n",
       "\n",
       "                                                 tweets clean_location  \\\n",
       "0     our deeds be the reason of this earthquake may...            nan   \n",
       "1                 forest fire near la ronge sask canada            nan   \n",
       "2     all residents ask to shelter in place be be no...            nan   \n",
       "3     thirteen thousand people receive wildfires eva...            nan   \n",
       "4     just get send this photo from ruby alaska as s...            nan   \n",
       "...                                                 ...            ...   \n",
       "7604  world news fall powerlines on glink tram you p...            nan   \n",
       "7605  on the flip side i be at walmart and there be ...            nan   \n",
       "7606  suicide bomber kill fifteen in saudi security ...            nan   \n",
       "7608  two giant crane hold a bridge collapse into ne...            nan   \n",
       "7612  the latest more home raze by northern californ...            nan   \n",
       "\n",
       "                                               all_text  \n",
       "0     our deeds be the reason of this earthquake may...  \n",
       "1         forest fire near la ronge sask canada nan nan  \n",
       "2     all residents ask to shelter in place be be no...  \n",
       "3     thirteen thousand people receive wildfires eva...  \n",
       "4     just get send this photo from ruby alaska as s...  \n",
       "...                                                 ...  \n",
       "7604  world news fall powerlines on glink tram you p...  \n",
       "7605  on the flip side i be at walmart and there be ...  \n",
       "7606  suicide bomber kill fifteen in saudi security ...  \n",
       "7608  two giant crane hold a bridge collapse into ne...  \n",
       "7612  the latest more home raze by northern californ...  \n",
       "\n",
       "[7503 rows x 8 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['all_text'] = all_text\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text_tags = df['tweets'].apply(lambda row: [nltk.pos_tag(row) for item in row])\n",
    "pos_keyword_tags = df['keyword'].apply(lambda row: [nltk.pos_tag(row) for item in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finished-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location[df.location != 'nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.keyword.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11,11))\n",
    "colors = ['lightblue', 'red']\n",
    "expl = (0, 0.1)\n",
    "df.target.value_counts().plot(kind='pie', legend=True, startangle=45, shadow=True, \n",
    "                             colors=colors, autopct='%1.1f%%')\n",
    "plt.title('target count', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opposed-floor",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-petersburg",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled = spell.unknown(df.tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eligible-repository",
   "metadata": {},
   "outputs": [],
   "source": [
    "misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-rabbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df.tweets\n",
    "nlp = Word2Vec(corpus, size=200,   \n",
    "            window=6, min_count=1, sg=1, iter=40)\n",
    "len(nlp.wv.vocab) # number of words in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-earthquake",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkg_resources\n",
    "from symspellpy.symspellpy import SymSpell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags[99:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hash=[]\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=1)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "for term in hashtags:\n",
    "    if len(term)>1:\n",
    "        result = sym_spell.word_segmentation(term)\n",
    "        r = result.corrected_string\n",
    "    else:\n",
    "        r = ''\n",
    "    new_hash.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-breach",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[10:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-sperm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the dicti\n",
    "word_dict = {} \n",
    "\n",
    "for row in df.tweets: \n",
    "    words = tokenizer.tokenize(row) \n",
    "    for word in words: \n",
    "        if word not in word_dict.keys(): \n",
    "            word_dict[word] = 1\n",
    "        else: \n",
    "            word_dict[word] += 1\n",
    "print(len(word_dict))\n",
    "max(word_dict, key=word_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-ranking",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-hometown",
   "metadata": {},
   "outputs": [],
   "source": [
    "hlist=[]\n",
    "for x in hashtags:\n",
    "    x = x.str.replace('[^a-zA-Z]', '')\n",
    "    hlist.append(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
