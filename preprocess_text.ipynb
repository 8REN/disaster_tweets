{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda env export > environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc847e3",
   "metadata": {},
   "source": [
    "replace contractions/location abbreviations via dictionary\n",
    "denoise text/apply word segmentation\n",
    "create word count dictionary\n",
    "next, spell check:\n",
    "add to dictionary from reoccuring words in text? or use freq dict symspell\n",
    "apply spell check to words in word count dictionary occuring only one time\n",
    "apply pos tag to tweets creating new column\n",
    "lemmatize tweets based on pos tag column\n",
    "bigrams with Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unlikely-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import pandas as pd \n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5473e8c8",
   "metadata": {},
   "source": [
    "## import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0c2cab3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "#target = pd.read_csv('train.csv', usecols=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc632945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "#print(len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f433b065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df.drop(['id'], axis=1, inplace=True)\n",
    "df.drop(['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14e104",
   "metadata": {},
   "source": [
    "#dups = df[df.duplicated(['text'])]\n",
    "df.drop_duplicates(subset=['text', 'location', 'keyword'], keep='first', inplace=True)\n",
    "#test_df.drop_duplicates(subset=['text'], keep='first', inplace=True)\n",
    "\n",
    "print(len(df))\n",
    "#print(len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cba7f01",
   "metadata": {},
   "source": [
    "## visualize target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c03cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style(theme='solarizedl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7a200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize distribution of target in dataset\n",
    "plt.figure(figsize=(11,11))\n",
    "colors = ['lightblue', 'red']\n",
    "expl = (0, 0.1)\n",
    "df.target.value_counts().plot(kind='pie', legend=True, startangle=45, shadow=True, \n",
    "                             colors=colors, autopct='%1.1f%%')\n",
    "plt.title('Binary Distribution of Disaster Tweet Dataset', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755eb75a",
   "metadata": {},
   "source": [
    "# clean and process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8862ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from symspellpy.symspellpy import SymSpell, Verbosity\n",
    "from spellchecker import SpellChecker\n",
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0461e",
   "metadata": {},
   "source": [
    "## hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4408dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# functions related to expanded hashtags in text\n",
    "def pascal_case_split(text):\n",
    "    # expand hashtags formatted in pascal case, ex: #ThisIsPascal\n",
    "    return re.sub(r'([A-Z])([?=a-z0-9+])', r' \\1\\2', text)\n",
    "\n",
    "def camel_case_split(text):\n",
    "    # expand hashtags formatted in pascal case, ex: #thisIsCamel\n",
    "    return re.sub(r'([a-z0-9+])([?<=A-Z])', r'\\1 \\2', text)\n",
    "\n",
    "def spell_check_word(word):\n",
    "    # lookup suggestions for individual words\n",
    "    spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=5)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "                      \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    corrections = spell.lookup(word, Verbosity.TOP, include_unknown=True)\n",
    "    closest = corrections[0]\n",
    "    # display suggestion term, edit distance, and term frequency\n",
    "    return closest.term\n",
    "    \n",
    "def spell_check_words(word):\n",
    "    # lookup suggestions for multi word string input\n",
    "    spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=5)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "    corrections = spell.lookup_compound(word, max_edit_distance=2)\n",
    "    closest = corrections[0]\n",
    "# display suggestion term, edit distance, and term frequency\n",
    "    return closest.term\n",
    "\n",
    "def desegment_strings(text):\n",
    "    spell = SymSpell(max_dictionary_edit_distance=0, prefix_length=8)\n",
    "    dictionary_path = pkg_resources.resource_filename(\"symspellpy\",\"frequency_dictionary_en_82_765.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    if text.isupper() | text.islower() and text.startswith('#'):\n",
    "        result = spell.word_segmentation(text)\n",
    "        text = result.corrected_string\n",
    "    return text \n",
    "\n",
    "def remove_hash(text):\n",
    "    # remove hash symbol in front of hashtag text and remove non unicode chars\n",
    "    return re.sub('#', '', text)\n",
    "\n",
    "def expand_strings(text):\n",
    "    # desegment hashtags & other strings with similar formatting\n",
    "    text = re.sub('CAfire', 'california fire', text)\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    h_text = remove_hash(p_text)\n",
    "    return h_text\n",
    "\n",
    "def expand_strings_sc(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash, spell check using compound lookup\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    lu_text = spell_check_words(p_text)\n",
    "    h_text = remove_hash(lu_text)\n",
    "    return h_text\n",
    "\n",
    "def expand_strings_ds(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash. spell check using wordsegmentation\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    lu_text = desegment_strings(p_text)\n",
    "    h_text = remove_hash(lu_text)\n",
    "    return h_text\n",
    "\n",
    "def expand_strings_indv_word(text):\n",
    "    # combine hashtag split functions for specific cases and remove hash. spell check using wordsegmentation\n",
    "    c_text = camel_case_split(text)\n",
    "    p_text = pascal_case_split(c_text)\n",
    "    lu_text = spell_check_word(p_text)\n",
    "    h_text = remove_hash(lu_text)\n",
    "    return h_text\n",
    "\n",
    "# def expand_strings_ds(text):\n",
    "#     # combine hashtag split functions for specific cases and remove hash\n",
    "#     c_text = camel_case_split(text)\n",
    "#     p_text = pascal_case_split(c_text)\n",
    "#     lu_text = desegment_hashtag(p_text)\n",
    "#     h_text = remove_hash(lu_text)\n",
    "#     return h_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b79731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column for extracted hashtags\n",
    "df['hashtags'] = df.text.str.findall(r'(?:(?<=\\s)|(?<=^))#.*?(?=\\s|$)')\n",
    "#test_df['hashtags'] = test_df.text.str.findall(r'(?:(?<=\\s)|(?<=^))#.*?(?=\\s|$)')\n",
    "df.hashtags = df.hashtags.astype(str)\n",
    "#test_df.hashtags = test_df.hashtags.astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e207d9a",
   "metadata": {},
   "source": [
    "%time\n",
    "df['exp_hash'] = df['hashtags'].map(expand_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989489e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1dd13",
   "metadata": {},
   "source": [
    "%time\n",
    "# with desegment_hashtag\n",
    "df['exp_hash'] = df['hashtags'].map(expand_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ed85af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fce86907",
   "metadata": {},
   "source": [
    "# process text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d573344",
   "metadata": {},
   "source": [
    "## dictionary based word replacement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83272deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "mineral-glory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and read previously created dictionary as csv for abbreviations and expansions of words\n",
    "def csv2dict(csv_name):\n",
    "    with open(csv_name, mode='r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        return {rows[1]:rows[2] for rows in reader}\n",
    "location_expansion_dict = csv2dict('utils\\location_expansion.csv')\n",
    "contractions_dict = csv2dict('utils\\contractions.csv')\n",
    "internet_initialisms_dict = csv2dict('utils\\internet_initialisms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df5d477",
   "metadata": {},
   "source": [
    "## denoise text functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c90dec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5aa60f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_expansion_from_dict(text, expansion_dict):\n",
    "    c_re = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, expansion_dict.keys()))+r'\\b')\n",
    "    def replace(match):\n",
    "        expansion =  f\"{expansion_dict[match.group(0)]}\"\n",
    "        return expansion\n",
    "    text = c_re.sub(replace, text)\n",
    "    return text\n",
    "\n",
    "def reduce_repeat_vowels(text, keep_reps=1):\n",
    "    # function to reduce repeated vowel occurences in string, \n",
    "    # keep_reps argument as int refers to number of occurences to keep in string\n",
    "    # keep_reps argument default : 1, can be set to any integer\n",
    "    vowels = ['a','e','i','o','u']\n",
    "    for ch in iter(vowels):\n",
    "        text = re.sub(r'(?i)'+rf'([{ch}])'+r'{3,}', r'\\1'*int(keep_reps), text)\n",
    "    return text\n",
    "\n",
    "def reduce_repeat_consonants(text, keep_reps=2):\n",
    "    # function to reduce repeated consonant occurences in string, \n",
    "    # keep_reps argument as int refers to number of occurences to keep in string\n",
    "    # keep_reps argument default : 1, can be set to any integer\n",
    "    consonants = ['b','c','d','f','g','h','j','k','l','m','n','p','q','r','s','t','v','w','x','y','z']\n",
    "    for ch in iter(consonants):\n",
    "        text = re.sub(r'(?i)'+ rf'([{ch}])'+r'{3,}', r'\\1'*int(keep_reps), text)\n",
    "    return text\n",
    "    \n",
    "def remove_urls(text):\n",
    "    # function to remove urls from string\n",
    "    return re.sub(r\"http\\S+\",  r\"\", text)\n",
    "\n",
    "def remove_handles(text):\n",
    "    # function to remove twitter handles from string\n",
    "    return re.sub(\"@\\S*\",  r\" \", text)\n",
    "\n",
    "def remove_non_uni(text):\n",
    "    # function to remove non unicode characters from string\n",
    "    return re.sub('[^\\u0000-\\u007f]', ' ', text)\n",
    "\n",
    "def remove_nan_str(text):\n",
    "    # function to remove 'nan' from string\n",
    "    return re.sub('nan', '', text)\n",
    "\n",
    "def remove_ws(text):\n",
    "    return re.sub(' +', ' ', text)\n",
    "\n",
    "def remove_non_alpha(text):\n",
    "    # function to remove non alpha chars from string\n",
    "    return re.sub(\"[^a-zA-Z]\",  r\" \", text)\n",
    "\n",
    "def denoise_text(text):\n",
    "    #reduce repeating characters\n",
    "    text = reduce_repeat_vowels(text)\n",
    "    text = reduce_repeat_consonants(text)\n",
    "    #remove xml tag strings\n",
    "    text = html.unescape(text)\n",
    "    #remove numeric and punctuation chars\n",
    "    text = remove_non_alpha(text)\n",
    "    #remove extra spaces\n",
    "    text = remove_ws(text)\n",
    "    return text   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf0fa6",
   "metadata": {},
   "source": [
    "## tweet text denoise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "concrete-strip",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### text preprocessing specifically formatted for tweets but will work on any text\n",
    "def process_tweet(df): \n",
    "    \"\"\"combine regex and nltk processing for tweet text and keyword text processing\"\"\"\n",
    "    def expand_strings_ds(text):\n",
    "        # combine hashtag split functions for specific cases and remove hash\n",
    "        # remove urls and usernames first due to alternating casing inside before case format expansion\n",
    "        u_text = remove_urls(text)\n",
    "        hd_text = remove_handles(u_text)\n",
    "        c_text = camel_case_split(hd_text)\n",
    "        p_text = pascal_case_split(c_text)\n",
    "        lu_text = desegment_strings(p_text)\n",
    "        h_text = remove_hash(lu_text)\n",
    "        return h_text\n",
    "    \n",
    "    def expand_text(text):\n",
    "        e_text = expand_strings_ds(text)\n",
    "        le_text = word_expansion_from_dict(e_text, expansion_dict=location_expansion_dict)\n",
    "        ce_text = word_expansion_from_dict(le_text.lower(), expansion_dict=contractions_dict)\n",
    "        ii_text = word_expansion_from_dict(ce_text.lower(), expansion_dict=internet_initialisms_dict)\n",
    "        return ii_text\n",
    "\n",
    "    def process_text(text):\n",
    "        exp_text = expand_text(text)\n",
    "        clean_text = denoise_text(exp_text)\n",
    "        #lem_text = lemmatize_text(clean_text)\n",
    "        return clean_text\n",
    "\n",
    "    processed_df = [process_text(x) for x in df]\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c16f797",
   "metadata": {},
   "source": [
    "## keyword text denoise process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ee31a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_keyword(df):\n",
    "    text = df.str.replace('%20', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aed85b7",
   "metadata": {},
   "source": [
    "## location text denoise process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "skilled-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_location(df):\n",
    "    \"\"\"combine regex and nltk processing for location text processing\"\"\"\n",
    "    \n",
    "    def expand_loc_text(text):\n",
    "        #exp_text = expand_strings_ds(text) \n",
    "        loc_dict_lower = {k.lower(): v for k, v in location_expansion_dict.items()}\n",
    "        exp_text = word_expansion_from_dict(text.lower(), loc_dict_lower)\n",
    "        return exp_text \n",
    "        \n",
    "    def process_text(text):\n",
    "        exp_text = expand_loc_text(text)\n",
    "        clean_text = denoise_text(exp_text)\n",
    "        #lem_text = lemmatize_text(clean_text)\n",
    "        return clean_text\n",
    "    \n",
    "    loc_df = [process_text(x) for x in df]\n",
    "    return loc_df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08771c0",
   "metadata": {},
   "source": [
    "# create new dataframe of denoised text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "084979e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_data_to_df(df):\n",
    "    df['tweets'] = process_tweet(df.text)\n",
    "    df['clean_keyword'] = process_keyword(df.keyword.astype(str))\n",
    "    df['clean_location'] = process_location(df.location.astype(str))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0608f50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = scrub_data_to_df(df)\n",
    "test_df = scrub_data_to_df(test)\n",
    "all_tweets = pd.DataFrame()\n",
    "all_tweets['tweets'] = pd.concat([train_df.tweets, test_df.tweets], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "784bb6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets.to_csv('complete_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2853c6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tweets',\n",
       " '0',\n",
       " 'our',\n",
       " 'deeds',\n",
       " 'are',\n",
       " 'the',\n",
       " 'reason',\n",
       " 'of',\n",
       " 'this',\n",
       " 'earthquake',\n",
       " 'm',\n",
       " '...',\n",
       " '1',\n",
       " 'forest',\n",
       " 'fire',\n",
       " 'near',\n",
       " 'la',\n",
       " 'ronge',\n",
       " 'sask',\n",
       " 'canada',\n",
       " '2',\n",
       " 'all',\n",
       " 'residents',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'shelter',\n",
       " 'in',\n",
       " 'place',\n",
       " 'are',\n",
       " 'b',\n",
       " '...',\n",
       " '3',\n",
       " 'people',\n",
       " 'receive',\n",
       " 'wildfires',\n",
       " 'evacuation',\n",
       " 'orders',\n",
       " 'in',\n",
       " '...',\n",
       " '4',\n",
       " 'just',\n",
       " 'got',\n",
       " 'sent',\n",
       " 'this',\n",
       " 'photo',\n",
       " 'from',\n",
       " 'ruby',\n",
       " 'alaska',\n",
       " 'as',\n",
       " '...',\n",
       " '...',\n",
       " '...',\n",
       " '10871',\n",
       " 'earthquake',\n",
       " 'safety',\n",
       " 'los',\n",
       " 'angeles',\n",
       " 'safety',\n",
       " 'fasteners',\n",
       " '...',\n",
       " '10872',\n",
       " 'storm',\n",
       " 'in',\n",
       " 'rhode',\n",
       " 'island',\n",
       " 'worse',\n",
       " 'than',\n",
       " 'last',\n",
       " 'hurrica',\n",
       " '...',\n",
       " '10873',\n",
       " 'green',\n",
       " 'line',\n",
       " 'derailment',\n",
       " 'in',\n",
       " 'chicago',\n",
       " '10874',\n",
       " 'meg',\n",
       " 'issues',\n",
       " 'hazardous',\n",
       " 'weather',\n",
       " 'outlook',\n",
       " 'hwo',\n",
       " '10875',\n",
       " 'cityof',\n",
       " 'calgary',\n",
       " 'has',\n",
       " 'activated',\n",
       " 'it',\n",
       " 'is',\n",
       " 'municipal',\n",
       " '...',\n",
       " '[',\n",
       " '10876',\n",
       " 'rows',\n",
       " 'x',\n",
       " '1',\n",
       " 'columns',\n",
       " ']']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb314aa",
   "metadata": {},
   "source": [
    "## create frequency dictionary for tweet text for further spell corrections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f8696778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "af80f2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(df):\n",
    "    tweets = all_tweets.split()\n",
    "    corpus = ' '.join(word for word in tweets)\n",
    "    return corpus\n",
    "\n",
    "def word_count(text):\n",
    "    counts = dict()\n",
    "    for row in text:\n",
    "        words = row.split()\n",
    "        for w in words:\n",
    "            if w in counts:\n",
    "                counts[w] += 1\n",
    "            else:\n",
    "                counts[w] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "da3b0caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 153365 words in the entire training dataset\n",
      "There are 16645 unique words in the training dataset\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#tokens = all_tweets.tweets.astype(str).split()\n",
    "# count frequency of words in corpus\n",
    "word_count_dict = word_count(all_tweets.tweets)\n",
    "\n",
    "# count number of words in corpus\n",
    "num_words = sum(word_count_dict[w] for w in word_count_dict)\n",
    "print(f'There are {num_words} words in the entire training dataset')\n",
    "\n",
    "# count number of unique words in corpus\n",
    "word_count_sorted = [(value, key) for key, value in word_count_dict.items()]\n",
    "word_count_sorted.sort()\n",
    "print(f'There are {len(word_count_sorted)} unique words in the training dataset')\n",
    "\n",
    "single_occ_words =  [k for k,v in word_count_dict.items() if int(v) == 1]\n",
    "filtered_dict = {k:v for (k,v) in word_count_dict.items() if int(v)>=4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "624aecde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'our': 153,\n",
       " 'deeds': 2,\n",
       " 'are': 893,\n",
       " 'the': 4689,\n",
       " 'reason': 29,\n",
       " 'of': 2634,\n",
       " 'this': 709,\n",
       " 'earthquake': 68,\n",
       " 'may': 113,\n",
       " 'allah': 11,\n",
       " 'forgive': 4,\n",
       " 'us': 230,\n",
       " 'all': 394,\n",
       " 'forest': 105,\n",
       " 'fire': 383,\n",
       " 'near': 85,\n",
       " 'la': 17,\n",
       " 'ronge': 1,\n",
       " 'sask': 1,\n",
       " 'canada': 20,\n",
       " 'residents': 13,\n",
       " 'asked': 15,\n",
       " 'to': 2846,\n",
       " 'shelter': 9,\n",
       " 'in': 2833,\n",
       " 'place': 34,\n",
       " 'being': 121,\n",
       " 'notified': 1,\n",
       " 'by': 785,\n",
       " 'officers': 11,\n",
       " 'no': 422,\n",
       " 'other': 68,\n",
       " 'evacuation': 67,\n",
       " 'or': 277,\n",
       " 'orders': 12,\n",
       " 'expected': 18,\n",
       " 'people': 311,\n",
       " 'receive': 3,\n",
       " 'wildfires': 18,\n",
       " 'california': 186,\n",
       " 'just': 467,\n",
       " 'got': 160,\n",
       " 'sent': 14,\n",
       " 'photo': 65,\n",
       " 'from': 618,\n",
       " 'ruby': 3,\n",
       " 'alaska': 15,\n",
       " 'as': 496,\n",
       " 'smoke': 66,\n",
       " 'pours': 1,\n",
       " 'into': 239,\n",
       " 'a': 3172,\n",
       " 'school': 86,\n",
       " 'rocky': 20,\n",
       " 'update': 53,\n",
       " 'highway': 21,\n",
       " 'closed': 27,\n",
       " 'both': 31,\n",
       " 'directions': 1,\n",
       " 'due': 53,\n",
       " 'lake': 22,\n",
       " 'county': 56,\n",
       " 'c': 94,\n",
       " 'afire': 1,\n",
       " 'flood': 87,\n",
       " 'disaster': 219,\n",
       " 'heavy': 27,\n",
       " 'rain': 64,\n",
       " 'causes': 21,\n",
       " 'flash': 27,\n",
       " 'flooding': 64,\n",
       " 'streets': 11,\n",
       " 'manitou': 1,\n",
       " 'colorado': 26,\n",
       " 'springs': 6,\n",
       " 'areas': 13,\n",
       " 'i': 2683,\n",
       " 'am': 536,\n",
       " 'on': 1236,\n",
       " 'top': 82,\n",
       " 'hill': 10,\n",
       " 'and': 2028,\n",
       " 'can': 384,\n",
       " 'see': 150,\n",
       " 'woods': 3,\n",
       " 'there': 277,\n",
       " 'is': 2085,\n",
       " 'an': 358,\n",
       " 'emergency': 230,\n",
       " 'happening': 18,\n",
       " 'now': 330,\n",
       " 'building': 44,\n",
       " 'across': 27,\n",
       " 'street': 38,\n",
       " 'afraid': 8,\n",
       " 'that': 862,\n",
       " 'tornado': 46,\n",
       " 'coming': 72,\n",
       " 'area': 63,\n",
       " 'three': 50,\n",
       " 'died': 47,\n",
       " 'heat': 70,\n",
       " 'wave': 55,\n",
       " 'so': 437,\n",
       " 'far': 44,\n",
       " 'haha': 26,\n",
       " 'south': 58,\n",
       " 'tampa': 7,\n",
       " 'getting': 75,\n",
       " 'flooded': 6,\n",
       " 'hah': 3,\n",
       " 'wait': 37,\n",
       " 'second': 55,\n",
       " 'live': 99,\n",
       " 'what': 337,\n",
       " 'gonna': 67,\n",
       " 'do': 523,\n",
       " 'fuck': 76,\n",
       " 'raining': 4,\n",
       " 'florida': 16,\n",
       " 'bay': 53,\n",
       " 'days': 43,\n",
       " 'have': 642,\n",
       " 'lost': 32,\n",
       " 'count': 3,\n",
       " 'bago': 2,\n",
       " 'myanmar': 25,\n",
       " 'we': 459,\n",
       " 'arrived': 13,\n",
       " 'damage': 73,\n",
       " 'bus': 49,\n",
       " 'multi': 5,\n",
       " 'car': 135,\n",
       " 'crash': 168,\n",
       " 'breaking': 64,\n",
       " 'up': 486,\n",
       " 'man': 155,\n",
       " 'love': 162,\n",
       " 'fruits': 2,\n",
       " 'summer': 87,\n",
       " 'lovely': 12,\n",
       " 'my': 1020,\n",
       " 'fast': 29,\n",
       " 'goal': 8,\n",
       " 'ridiculous': 4,\n",
       " 'london': 23,\n",
       " 'cool': 41,\n",
       " 'skiing': 1,\n",
       " 'wonderful': 7,\n",
       " 'day': 167,\n",
       " 'lol': 2,\n",
       " 'way': 118,\n",
       " 'not': 1163,\n",
       " 'eat': 18,\n",
       " 'shit': 81,\n",
       " 'was': 578,\n",
       " 'new': 386,\n",
       " 'york': 54,\n",
       " 'city': 102,\n",
       " 'last': 126,\n",
       " 'week': 61,\n",
       " 'girlfriend': 6,\n",
       " 'col': 4,\n",
       " 'you': 1550,\n",
       " 'like': 496,\n",
       " 'pasta': 3,\n",
       " 'end': 58,\n",
       " 'wholesale': 7,\n",
       " 'markets': 9,\n",
       " 'ablaze': 41,\n",
       " 'always': 67,\n",
       " 'try': 37,\n",
       " 'bring': 29,\n",
       " 'metal': 18,\n",
       " 'retweet': 162,\n",
       " 'africanbaze': 1,\n",
       " 'news': 366,\n",
       " 'nigeria': 13,\n",
       " 'flag': 29,\n",
       " 'set': 66,\n",
       " 'aba': 21,\n",
       " 'crying': 12,\n",
       " 'out': 514,\n",
       " 'for': 1263,\n",
       " 'more': 331,\n",
       " 'me': 430,\n",
       " 'plus': 9,\n",
       " 'side': 33,\n",
       " 'look': 95,\n",
       " 'at': 749,\n",
       " 'sky': 22,\n",
       " 'night': 83,\n",
       " 'it': 1258,\n",
       " 'manchester': 13,\n",
       " 'united': 79,\n",
       " 'football': 20,\n",
       " 'club': 13,\n",
       " 'they': 331,\n",
       " 'built': 10,\n",
       " 'much': 86,\n",
       " 'hype': 4,\n",
       " 'around': 61,\n",
       " 'acquisitions': 2,\n",
       " 'but': 435,\n",
       " 'doubt': 6,\n",
       " 'will': 511,\n",
       " 'epl': 1,\n",
       " 'season': 24,\n",
       " 'inec': 2,\n",
       " 'office': 17,\n",
       " 'abia': 2,\n",
       " 'barbados': 1,\n",
       " 'bridgetown': 1,\n",
       " 'jamaica': 5,\n",
       " 'two': 153,\n",
       " 'cars': 26,\n",
       " 'santa': 7,\n",
       " 'cruz': 11,\n",
       " 'head': 61,\n",
       " 'st': 48,\n",
       " 'elizabeth': 2,\n",
       " 'police': 200,\n",
       " 'superintende': 1,\n",
       " 'lord': 27,\n",
       " 'd': 95,\n",
       " 'check': 64,\n",
       " 'these': 70,\n",
       " 'safe': 22,\n",
       " 'work': 109,\n",
       " 'outside': 32,\n",
       " 'alive': 19,\n",
       " 'dead': 137,\n",
       " 'inside': 35,\n",
       " 'had': 177,\n",
       " 'awesome': 25,\n",
       " 'time': 184,\n",
       " 'visiting': 2,\n",
       " 'cfc': 2,\n",
       " 'ancop': 1,\n",
       " 'site': 40,\n",
       " 'thanks': 52,\n",
       " 'tita': 2,\n",
       " 'vida': 2,\n",
       " 'taking': 25,\n",
       " 'care': 45,\n",
       " 'pumped': 1,\n",
       " 'wanted': 23,\n",
       " 'chicago': 28,\n",
       " 'with': 799,\n",
       " 'preaching': 2,\n",
       " 'hotel': 11,\n",
       " 'gained': 6,\n",
       " 'followers': 8,\n",
       " 'know': 170,\n",
       " 'your': 483,\n",
       " 'stats': 3,\n",
       " 'grow': 9,\n",
       " 'how': 294,\n",
       " 'west': 44,\n",
       " 'burned': 58,\n",
       " 'thousands': 27,\n",
       " 'alone': 24,\n",
       " 'perfect': 13,\n",
       " 'tracklist': 1,\n",
       " 'life': 139,\n",
       " 'leave': 31,\n",
       " 'first': 191,\n",
       " 'retainers': 1,\n",
       " 'quite': 12,\n",
       " 'weird': 13,\n",
       " 'better': 51,\n",
       " 'get': 340,\n",
       " 'used': 44,\n",
       " 'wear': 6,\n",
       " 'them': 165,\n",
       " 'every': 73,\n",
       " 'single': 18,\n",
       " 'next': 69,\n",
       " 'year': 143,\n",
       " 'least': 51,\n",
       " 'deputies': 4,\n",
       " 'shot': 34,\n",
       " 'before': 82,\n",
       " 'brighton': 3,\n",
       " 'home': 107,\n",
       " 'wife': 22,\n",
       " 'six': 12,\n",
       " 'years': 111,\n",
       " 'jail': 5,\n",
       " 'setting': 16,\n",
       " 'niece': 3,\n",
       " 'superintendent': 1,\n",
       " 'lanford': 1,\n",
       " 'salmon': 3,\n",
       " 'has': 357,\n",
       " 'arsonist': 27,\n",
       " 'deliberately': 1,\n",
       " 'black': 106,\n",
       " 'church': 11,\n",
       " 'north': 56,\n",
       " 'carolina': 21,\n",
       " 'noches': 1,\n",
       " 'el': 17,\n",
       " 'bestia': 1,\n",
       " 'happy': 35,\n",
       " 'teammates': 1,\n",
       " 'training': 19,\n",
       " 'hard': 27,\n",
       " 'goodnight': 1,\n",
       " 'gunners': 1,\n",
       " 'kurds': 1,\n",
       " 'trampling': 1,\n",
       " 'turkmen': 2,\n",
       " 'later': 18,\n",
       " 'while': 76,\n",
       " 'others': 27,\n",
       " 'vandalized': 2,\n",
       " 'offices': 3,\n",
       " 'front': 25,\n",
       " 'diyala': 1,\n",
       " 'truck': 76,\n",
       " 'r': 23,\n",
       " 'voortrekker': 1,\n",
       " 'ave': 21,\n",
       " 'oregon': 23,\n",
       " 'tambo': 1,\n",
       " 'intl': 3,\n",
       " 'cargo': 4,\n",
       " 'section': 5,\n",
       " 'hearts': 6,\n",
       " 'gift': 7,\n",
       " 'skyline': 2,\n",
       " 'kiss': 4,\n",
       " 'upon': 22,\n",
       " 'lips': 1,\n",
       " 'tonight': 62,\n",
       " 'los': 9,\n",
       " 'angeles': 7,\n",
       " 'expecting': 4,\n",
       " 'ig': 6,\n",
       " 'facebook': 19,\n",
       " 'be': 607,\n",
       " 'filled': 4,\n",
       " 'sunset': 8,\n",
       " 'shots': 20,\n",
       " 'if': 353,\n",
       " 'peeps': 3,\n",
       " 'climate': 29,\n",
       " 'energy': 10,\n",
       " 'revel': 2,\n",
       " 'yours': 32,\n",
       " 'wmv': 2,\n",
       " 'videos': 11,\n",
       " 'means': 20,\n",
       " 'mac': 10,\n",
       " 'farewell': 1,\n",
       " 'en': 11,\n",
       " 'route': 15,\n",
       " 'dvd': 7,\n",
       " 'gtx': 1,\n",
       " 'wm': 1,\n",
       " 'progressive': 1,\n",
       " 'greetings': 1,\n",
       " 'about': 312,\n",
       " 'month': 21,\n",
       " 'students': 16,\n",
       " 'would': 274,\n",
       " 'their': 129,\n",
       " 'pens': 2,\n",
       " 'torch': 5,\n",
       " 'publications': 1,\n",
       " 'rene': 2,\n",
       " 'jacinta': 1,\n",
       " 'secret': 21,\n",
       " 'k': 56,\n",
       " 'fallen': 5,\n",
       " 'skies': 4,\n",
       " 'edit': 3,\n",
       " 'mar': 3,\n",
       " 'steve': 13,\n",
       " 'fires': 154,\n",
       " 'here': 125,\n",
       " 'something': 49,\n",
       " 'else': 30,\n",
       " 'tinderbox': 1,\n",
       " 'clown': 3,\n",
       " 'hood': 2,\n",
       " 'playing': 45,\n",
       " 'ian': 4,\n",
       " 'buff': 1,\n",
       " 'magnitude': 5,\n",
       " 'edm': 15,\n",
       " 'huge': 32,\n",
       " 'does': 108,\n",
       " 'talk': 39,\n",
       " 'go': 153,\n",
       " 'until': 62,\n",
       " 'make': 126,\n",
       " 'kids': 44,\n",
       " 'cuz': 8,\n",
       " 'bicycle': 3,\n",
       " 'accident': 125,\n",
       " 'split': 4,\n",
       " 'testicles': 1,\n",
       " 'impossible': 5,\n",
       " 'michael': 21,\n",
       " 'father': 10,\n",
       " 'w': 180,\n",
       " 'nashville': 9,\n",
       " 'traffic': 36,\n",
       " 'moving': 14,\n",
       " 'm': 241,\n",
       " 'slower': 4,\n",
       " 'than': 189,\n",
       " 'usual': 5,\n",
       " 'center': 30,\n",
       " 'lane': 10,\n",
       " 'blocked': 16,\n",
       " 'clara': 1,\n",
       " 'brunswick': 4,\n",
       " 'great': 84,\n",
       " 'america': 47,\n",
       " 'pkwy': 2,\n",
       " 'personalinjury': 1,\n",
       " 'read': 96,\n",
       " 'advice': 6,\n",
       " 'solicitor': 2,\n",
       " 'help': 97,\n",
       " 'otley': 1,\n",
       " 'hour': 19,\n",
       " 'stlouis': 1,\n",
       " 'caraccidentlawyer': 1,\n",
       " 'speeding': 1,\n",
       " 'among': 17,\n",
       " 'teen': 16,\n",
       " 'accidents': 9,\n",
       " 'tee': 2,\n",
       " 'reported': 21,\n",
       " 'motor': 5,\n",
       " 'vehicle': 27,\n",
       " 'curry': 1,\n",
       " 'herman': 1,\n",
       " 'rd': 36,\n",
       " 'stephenson': 1,\n",
       " 'involving': 18,\n",
       " 'overturned': 2,\n",
       " 'please': 102,\n",
       " 'use': 54,\n",
       " 'big': 61,\n",
       " 'rig': 3,\n",
       " 'radio': 35,\n",
       " 'awareness': 2,\n",
       " 'mile': 13,\n",
       " 'marker': 4,\n",
       " 'mooresville': 2,\n",
       " 'iredell': 2,\n",
       " 'ramp': 1,\n",
       " 'pm': 142,\n",
       " 'sleeping': 22,\n",
       " 'pills': 1,\n",
       " 'double': 25,\n",
       " 'risk': 21,\n",
       " 'knew': 13,\n",
       " 'gon': 4,\n",
       " 'happen': 23,\n",
       " 'n': 84,\n",
       " 'cabrillo': 1,\n",
       " 'magellan': 1,\n",
       " 'av': 8,\n",
       " 'mir': 1,\n",
       " 'congestion': 1,\n",
       " 'pastor': 2,\n",
       " 'scene': 23,\n",
       " 'who': 270,\n",
       " 'owner': 15,\n",
       " 'range': 4,\n",
       " 'rover': 2,\n",
       " 'mom': 32,\n",
       " 'did': 153,\n",
       " 'wished': 1,\n",
       " 'why': 176,\n",
       " 'some': 179,\n",
       " 'spilt': 1,\n",
       " 'mayonnaise': 1,\n",
       " 'over': 258,\n",
       " 'horrible': 49,\n",
       " 'past': 56,\n",
       " 'sunday': 20,\n",
       " 'finally': 34,\n",
       " 'able': 12,\n",
       " 'thank': 43,\n",
       " 'pissed': 5,\n",
       " 'donnie': 2,\n",
       " 'when': 365,\n",
       " 'tell': 43,\n",
       " 'him': 106,\n",
       " 'another': 100,\n",
       " 'overturns': 1,\n",
       " 'fort': 9,\n",
       " 'worth': 17,\n",
       " 'interstate': 2,\n",
       " 'click': 9,\n",
       " 'been': 249,\n",
       " 'ashville': 1,\n",
       " 'sb': 7,\n",
       " 'sr': 9,\n",
       " 'motorcyclist': 12,\n",
       " 'dies': 21,\n",
       " 'crossed': 6,\n",
       " 'median': 1,\n",
       " 'motorcycle': 6,\n",
       " 'rider': 5,\n",
       " 'traveling': 2,\n",
       " 'information': 27,\n",
       " 'cad': 3,\n",
       " 'property': 32,\n",
       " 'nhs': 4,\n",
       " 'piner': 2,\n",
       " 'horndale': 2,\n",
       " 'dr': 18,\n",
       " 'aa': 5,\n",
       " 'yf': 1,\n",
       " 'turning': 5,\n",
       " 'onto': 11,\n",
       " 'chandanee': 1,\n",
       " 'magu': 1,\n",
       " 'mma': 7,\n",
       " 'taxi': 3,\n",
       " 'rammed': 2,\n",
       " 'halfway': 2,\n",
       " 'turned': 18,\n",
       " 'everyone': 73,\n",
       " 'conf': 1,\n",
       " 'left': 43,\n",
       " 'eddy': 1,\n",
       " 'stop': 85,\n",
       " 'back': 166,\n",
       " 'hampshire': 10,\n",
       " 'delay': 6,\n",
       " 'mins': 10,\n",
       " 'wpd': 1,\n",
       " 's': 930,\n",
       " 'th': 87,\n",
       " 'injury': 59,\n",
       " 'willis': 2,\n",
       " 'foreman': 1,\n",
       " 'aashiqui': 1,\n",
       " 'actress': 2,\n",
       " 'anu': 1,\n",
       " 'aggarwal': 1,\n",
       " 'her': 198,\n",
       " 'fatal': 92,\n",
       " 'suffield': 1,\n",
       " 'alberta': 9,\n",
       " 'backup': 2,\n",
       " 'blocking': 6,\n",
       " 'right': 108,\n",
       " 'lanes': 8,\n",
       " 'exit': 12,\n",
       " 'langtree': 1,\n",
       " 'consider': 10,\n",
       " 'alternate': 2,\n",
       " 'changed': 5,\n",
       " 'determine': 3,\n",
       " 'options': 6,\n",
       " 'financially': 1,\n",
       " 'support': 37,\n",
       " 'plans': 37,\n",
       " 'going': 150,\n",
       " 'treatment': 8,\n",
       " 'deadly': 12,\n",
       " 'happened': 32,\n",
       " 'hagerstown': 1,\n",
       " 'today': 150,\n",
       " 'details': 13,\n",
       " 'whag': 1,\n",
       " 'were': 188,\n",
       " 'marinading': 1,\n",
       " 'only': 128,\n",
       " 'even': 112,\n",
       " 'fucking': 64,\n",
       " 'mfs': 4,\n",
       " 'drive': 24,\n",
       " 'bahrain': 1,\n",
       " 'previously': 5,\n",
       " 'road': 50,\n",
       " 'killed': 129,\n",
       " 'explosion': 58,\n",
       " 'still': 180,\n",
       " 'heard': 56,\n",
       " 'leaders': 5,\n",
       " 'kenya': 5,\n",
       " 'forward': 7,\n",
       " 'comment': 19,\n",
       " 'issue': 14,\n",
       " 'disciplinary': 1,\n",
       " 'measures': 1,\n",
       " 'arrest': 8,\n",
       " 'nganga': 2,\n",
       " 'scuf': 2,\n",
       " 'ps': 13,\n",
       " 'game': 71,\n",
       " 'cya': 1,\n",
       " 'himself': 14,\n",
       " 'further': 5,\n",
       " 'once': 27,\n",
       " 'effort': 7,\n",
       " 'gets': 45,\n",
       " 'painful': 2,\n",
       " 'win': 28,\n",
       " 'roger': 2,\n",
       " 'bannister': 1,\n",
       " 'ir': 11,\n",
       " 'icemoon': 10,\n",
       " 'aftershock': 32,\n",
       " 'dubstep': 11,\n",
       " 'trap': 17,\n",
       " 'music': 54,\n",
       " 'dn': 11,\n",
       " 'b': 118,\n",
       " 'dance': 20,\n",
       " 'ices': 10,\n",
       " 'victory': 8,\n",
       " 'bargain': 9,\n",
       " 'basement': 6,\n",
       " 'prices': 6,\n",
       " 'dwight': 2,\n",
       " 'david': 14,\n",
       " 'eisenhower': 2,\n",
       " 'nobody': 8,\n",
       " 'remembers': 2,\n",
       " 'came': 47,\n",
       " 'charles': 5,\n",
       " 'schulz': 1,\n",
       " 'speaking': 7,\n",
       " 'someone': 63,\n",
       " 'using': 19,\n",
       " 'xb': 1,\n",
       " 'most': 68,\n",
       " 'also': 76,\n",
       " 'harder': 1,\n",
       " 'conflict': 7,\n",
       " 'glorious': 3,\n",
       " 'triumph': 1,\n",
       " 'thomas': 10,\n",
       " 'paine': 1,\n",
       " 'growing': 6,\n",
       " 'spoiled': 3,\n",
       " 'clay': 3,\n",
       " 'pigeon': 1,\n",
       " 'shooting': 44,\n",
       " 'because': 204,\n",
       " 'guess': 21,\n",
       " 'one': 290,\n",
       " 'actually': 41,\n",
       " 'wants': 18,\n",
       " 'any': 94,\n",
       " 'free': 72,\n",
       " 'tc': 3,\n",
       " 'terrifying': 5,\n",
       " 'best': 116,\n",
       " 'roller': 3,\n",
       " 'coaster': 2,\n",
       " 'ever': 79,\n",
       " 'disclaimer': 1,\n",
       " 'very': 52,\n",
       " 'few': 36,\n",
       " 'seeing': 24,\n",
       " 'issues': 41,\n",
       " 'wisdom': 5,\n",
       " 'wed': 9,\n",
       " 'bonus': 1,\n",
       " 'minute': 49,\n",
       " 'daily': 26,\n",
       " 'habits': 2,\n",
       " 'could': 128,\n",
       " 'really': 121,\n",
       " 'improve': 2,\n",
       " 'many': 113,\n",
       " 'already': 40,\n",
       " 'lifehacks': 1,\n",
       " 'protect': 13,\n",
       " 'yourself': 32,\n",
       " 'profit': 7,\n",
       " 'global': 27,\n",
       " 'financial': 16,\n",
       " 'meltdown': 51,\n",
       " 'wiedemer': 1,\n",
       " 'http': 1,\n",
       " 'moment': 25,\n",
       " 'scary': 10,\n",
       " 'guy': 34,\n",
       " 'behind': 25,\n",
       " 'screaming': 58,\n",
       " 'bloody': 64,\n",
       " 'murder': 64,\n",
       " 'silverwood': 1,\n",
       " 'full': 143,\n",
       " 'streaming': 3,\n",
       " 'tube': 29,\n",
       " 'book': 32,\n",
       " 'sometimes': 18,\n",
       " 'face': 67,\n",
       " 'difficulties': 1,\n",
       " 'doing': 43,\n",
       " 'wrong': 28,\n",
       " 'joel': 6,\n",
       " 'osteen': 1,\n",
       " 'thing': 56,\n",
       " 'stands': 6,\n",
       " 'between': 23,\n",
       " 'dream': 12,\n",
       " 'belief': 2,\n",
       " 'possible': 42,\n",
       " 'brown': 41,\n",
       " 'praise': 1,\n",
       " 'god': 130,\n",
       " 'ministry': 6,\n",
       " 'tells': 7,\n",
       " 'wdyouth': 1,\n",
       " 'biblestudy': 1,\n",
       " 'remembering': 8,\n",
       " 'die': 39,\n",
       " 'avoid': 13,\n",
       " 'thinking': 24,\n",
       " 'lose': 13,\n",
       " 'jobs': 25,\n",
       " 'tried': 24,\n",
       " 'orange': 5,\n",
       " 'never': 99,\n",
       " 'same': 43,\n",
       " 'bb': 5,\n",
       " 'kick': 9,\n",
       " 'off': 175,\n",
       " 'want': 113,\n",
       " 'making': 37,\n",
       " 'say': 121,\n",
       " 'cannot': 16,\n",
       " 'done': 41,\n",
       " 'should': 115,\n",
       " 'interrupt': 1,\n",
       " 'those': 88,\n",
       " 'george': 8,\n",
       " 'bernard': 1,\n",
       " 'shaw': 2,\n",
       " 'oyster': 1,\n",
       " 'shell': 7,\n",
       " 'andrew': 2,\n",
       " 'carnegie': 1,\n",
       " 'anyone': 34,\n",
       " 'need': 112,\n",
       " 'p': 102,\n",
       " 'play': 48,\n",
       " 'hybrid': 3,\n",
       " 'slayer': 7,\n",
       " 'eu': 4,\n",
       " 'hit': 69,\n",
       " 'experts': 24,\n",
       " 'france': 22,\n",
       " 'begin': 17,\n",
       " 'examining': 13,\n",
       " 'airplane': 52,\n",
       " 'debris': 69,\n",
       " 'found': 70,\n",
       " 'reunion': 41,\n",
       " 'island': 57,\n",
       " 'french': 15,\n",
       " 'air': 63,\n",
       " 'o': 94,\n",
       " 'strict': 3,\n",
       " 'liability': 2,\n",
       " 'context': 5,\n",
       " 'pilot': 16,\n",
       " 'error': 6,\n",
       " 'common': 15,\n",
       " 'component': 1,\n",
       " 'aviation': 3,\n",
       " 'cr': 5,\n",
       " 'lifetime': 3,\n",
       " 'odds': 4,\n",
       " 'dying': 12,\n",
       " 'wedn': 4,\n",
       " 'aww': 2,\n",
       " 'cuties': 1,\n",
       " 'good': 127,\n",
       " 'job': 37,\n",
       " 'family': 70,\n",
       " 'members': 23,\n",
       " 'osama': 1,\n",
       " 'bin': 12,\n",
       " 'laden': 11,\n",
       " 'ironic': 2,\n",
       " 'mhmm': 1,\n",
       " 'gov': 13,\n",
       " 'suspect': 50,\n",
       " 'goes': 44,\n",
       " 'engine': 6,\n",
       " 'via': 326,\n",
       " 'wings': 8,\n",
       " 'cessna': 1,\n",
       " 'ocampo': 2,\n",
       " 'coahuila': 4,\n",
       " 'mexico': 11,\n",
       " 'july': 14,\n",
       " 'four': 28,\n",
       " 'men': 43,\n",
       " 'including': 11,\n",
       " 'state': 76,\n",
       " 'government': 44,\n",
       " 'official': 25,\n",
       " 'watch': 140,\n",
       " 'video': 235,\n",
       " 'wednesday': 26,\n",
       " 'began': 10,\n",
       " 't': 151,\n",
       " 'kca': 5,\n",
       " 'vote': 19,\n",
       " 'kidding': 11,\n",
       " 'idaho': 27,\n",
       " 'mbataweel': 1,\n",
       " 'rip': 12,\n",
       " 'binladen': 1,\n",
       " 'almost': 35,\n",
       " 'coworker': 3,\n",
       " 'nudes': 1,\n",
       " 'mode': 19,\n",
       " 'might': 34,\n",
       " 'wreck': 74,\n",
       " 'politics': 15,\n",
       " 'mlb': 6,\n",
       " 'unbelievably': 1,\n",
       " 'insane': 8,\n",
       " 'airport': 46,\n",
       " 'aircraft': 29,\n",
       " 'aeroplane': 1,\n",
       " 'runway': 9,\n",
       " 'freaky': 2,\n",
       " 'usama': 1,\n",
       " 'ladins': 1,\n",
       " 'naturally': 1,\n",
       " 'plane': 43,\n",
       " 'festival': 14,\n",
       " 'death': 106,\n",
       " 'fest': 6,\n",
       " 'dtn': 4,\n",
       " 'brazil': 7,\n",
       " 'exp': 9,\n",
       " 'believe': 41,\n",
       " 'eyes': 36,\n",
       " 'nicole': 1,\n",
       " 'fletcher': 1,\n",
       " 'victim': 19,\n",
       " 'crashed': 54,\n",
       " 'times': 72,\n",
       " 'ago': 39,\n",
       " 'little': 70,\n",
       " 'bit': 22,\n",
       " 'trauma': 55,\n",
       " 'although': 6,\n",
       " 'she': 121,\n",
       " 'oh': 91,\n",
       " 'bro': 16,\n",
       " 'jet': 10,\n",
       " 'turbo': 1,\n",
       " 'boing': 1,\n",
       " 'g': 45,\n",
       " 'phone': 53,\n",
       " 'looks': 65,\n",
       " 'ship': 23,\n",
       " 'terrible': 14,\n",
       " 'statistically': 1,\n",
       " 'cop': 13,\n",
       " 'crashes': 5,\n",
       " 'house': 76,\n",
       " 'colombia': 1,\n",
       " 'drone': 9,\n",
       " 'pilots': 7,\n",
       " 'worried': 6,\n",
       " 'drones': 7,\n",
       " 'esp': 6,\n",
       " 'close': 23,\n",
       " 'vicinity': 2,\n",
       " 'airports': 1,\n",
       " 'early': 23,\n",
       " 'wake': 39,\n",
       " 'call': 62,\n",
       " 'sister': 14,\n",
       " 'begging': 2,\n",
       " 'come': 79,\n",
       " 'ride': 8,\n",
       " 'ambulance': 53,\n",
       " 'hospital': 17,\n",
       " 'rod': 1,\n",
       " 'kiai': 1,\n",
       " 'twelve': 16,\n",
       " 'feared': 34,\n",
       " 'pakistani': 20,\n",
       " 'helicopter': 33,\n",
       " 'ambulances': 3,\n",
       " 'serious': 18,\n",
       " 'lorry': 1,\n",
       " 'ems': 10,\n",
       " 'ne': 2,\n",
       " 'reuters': 30,\n",
       " 'yugvani': 3,\n",
       " 'leading': 12,\n",
       " 'services': 81,\n",
       " 'boss': 8,\n",
       " 'welcomes': 2,\n",
       " 'charity': 11,\n",
       " 'travelling': 2,\n",
       " 'aberystwyth': 1,\n",
       " 'shrewsbury': 1,\n",
       " 'incident': 17,\n",
       " 'halt': 3,\n",
       " 'shrews': 1,\n",
       " 'sprinter': 5,\n",
       " 'automatic': 8,\n",
       " 'frontline': 5,\n",
       " 'choice': 10,\n",
       " 'lez': 5,\n",
       " 'compliant': 8,\n",
       " 'e': 97,\n",
       " 'nanotech': 2,\n",
       " 'device': 7,\n",
       " 'target': 14,\n",
       " 'destroy': 58,\n",
       " 'blood': 61,\n",
       " 'clots': 2,\n",
       " 'hella': 3,\n",
       " 'crazy': 34,\n",
       " 'fights': 7,\n",
       " 'couple': 22,\n",
       " 'mosh': 1,\n",
       " 'pits': 3,\n",
       " 'run': 64,\n",
       " 'lucky': 16,\n",
       " 'justsaying': 1,\n",
       " 'randomthought': 1,\n",
       " 'til': 9,\n",
       " 'dna': 1,\n",
       " 'waiting': 19,\n",
       " 'ok': 26,\n",
       " 'hahahah': 2,\n",
       " 'pakistan': 32,\n",
       " 'kills': 59,\n",
       " 'nine': 13,\n",
       " 'nissan': 2,\n",
       " 'medical': 14,\n",
       " 'assistance': 5,\n",
       " 'em': 20,\n",
       " 'ts': 10,\n",
       " 'petition': 17,\n",
       " 'per': 7,\n",
       " 'minimum': 3,\n",
       " 'wage': 2,\n",
       " 'paramedics': 1,\n",
       " 'parking': 6,\n",
       " 'lot': 41,\n",
       " 'he': 359,\n",
       " 'said': 81,\n",
       " 'johns': 2,\n",
       " 'which': 63,\n",
       " 'dog': 29,\n",
       " 'thinks': 3,\n",
       " 'hatzolah': 1,\n",
       " 'responding': 4,\n",
       " 'dual': 4,\n",
       " 'sirens': 50,\n",
       " 'world': 161,\n",
       " 'worldnews': 7,\n",
       " 'number': 15,\n",
       " 'lesotho': 1,\n",
       " 'body': 178,\n",
       " 'surprised': 6,\n",
       " 'standardised': 1,\n",
       " 'clinical': 1,\n",
       " 'practice': 10,\n",
       " 'trust': 23,\n",
       " 'j': 21,\n",
       " 'walk': 19,\n",
       " 'passing': 9,\n",
       " 'hate': 31,\n",
       " 'episode': 20,\n",
       " 'where': 89,\n",
       " 'trunks': 1,\n",
       " 'annihilated': 45,\n",
       " 'freiza': 1,\n",
       " 'cleanest': 1,\n",
       " 'showed': 3,\n",
       " 'nigga': 16,\n",
       " 'mercy': 5,\n",
       " 'shall': 13,\n",
       " 'petebests': 1,\n",
       " 'dessicated': 1,\n",
       " 'laid': 3,\n",
       " 'bare': 4,\n",
       " 'then': 111,\n",
       " 'kneel': 2,\n",
       " 'maine': 25,\n",
       " ...}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_occ_words = [k for k,v in word_dict.items() if int(v) == 1]\n",
    "freq_words = [k for k,v in word_dict.items() if int(v) >= 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "1d10834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_word(word):\n",
    "    # lookup suggestions for individual words using frequently used words within dataset\n",
    "    spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=5)\n",
    "    #spell.load_dictionary(filtered_dict, term_index=0, count_index=1)\n",
    "    spell.create_dictionary(filtered_dict)\n",
    "    corrections = spell.lookup(word, Verbosity.CLOSEST, include_unknown=True)\n",
    "    closest = corrections[0]\n",
    "    # display suggestion term, edit distance, and term frequency\n",
    "    return closest.term\n",
    "        \n",
    "def spell_check_words(word):\n",
    "    # lookup suggestions for multi word string input using third party dictionaries\n",
    "    spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n",
    "    dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "    bigram_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n",
    "    spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "    spell.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)\n",
    "    corrections = spell.lookup_compound(word, max_edit_distance=3)\n",
    "    closest = corrections[0]\n",
    "# display suggestion term, edit distance, and term frequency\n",
    "    return closest.term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c4eb3abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_rare(df):\n",
    "    corrections = []\n",
    "    for x in single_occ_words:\n",
    "        repl = spell_check_words(x)\n",
    "        corrections.append(repl)\n",
    "    replacement_dict = dict(zip(single_occ_words, corrections))\n",
    "    filtered_repl = dict(filter(lambda elem: elem[0] != elem[1], replacement_dict.items()))\n",
    "    def replace_spell(text):\n",
    "        r_re = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, filtered_repl.keys()))+r'\\b')\n",
    "        def replace(match):\n",
    "            replacement =  f\"{filtered_repl[match.group(0)]}\"\n",
    "            return replacement\n",
    "        text = r_re.sub(replace, text)\n",
    "        return text\n",
    "    all_tweets['sc'] = all_tweets['tweets'].apply(replace_spell)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a92df06d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>our deeds are the reason of this earthquake m...</td>\n",
       "      <td>our deeds are the reason of this earthquake m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la range ask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all residents asked to shelter in place are b...</td>\n",
       "      <td>all residents asked to shelter in place are b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "      <td>people receive wildfires evacuation orders in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent this photo from ruby alaska as ...</td>\n",
       "      <td>just got sent this photo from ruby alaska as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10871</th>\n",
       "      <td>earthquake safety los angeles safety fasteners...</td>\n",
       "      <td>earthquake safety los angeles safety fasteners...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10872</th>\n",
       "      <td>storm in rhode island worse than last hurrica...</td>\n",
       "      <td>storm in rhode island worse than last hurrica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10873</th>\n",
       "      <td>green line derailment in chicago</td>\n",
       "      <td>green line derailment in chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10874</th>\n",
       "      <td>meg issues hazardous weather outlook hwo</td>\n",
       "      <td>meg issues hazardous weather outlook hwo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>cityof calgary has activated it is municipal ...</td>\n",
       "      <td>cityof calgary has activated it is municipal ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10876 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets  \\\n",
       "0       our deeds are the reason of this earthquake m...   \n",
       "1                  forest fire near la ronge sask canada   \n",
       "2       all residents asked to shelter in place are b...   \n",
       "3       people receive wildfires evacuation orders in...   \n",
       "4       just got sent this photo from ruby alaska as ...   \n",
       "...                                                  ...   \n",
       "10871  earthquake safety los angeles safety fasteners...   \n",
       "10872   storm in rhode island worse than last hurrica...   \n",
       "10873                  green line derailment in chicago    \n",
       "10874          meg issues hazardous weather outlook hwo    \n",
       "10875   cityof calgary has activated it is municipal ...   \n",
       "\n",
       "                                                     new  \n",
       "0       our deeds are the reason of this earthquake m...  \n",
       "1                   forest fire near la range ask canada  \n",
       "2       all residents asked to shelter in place are b...  \n",
       "3       people receive wildfires evacuation orders in...  \n",
       "4       just got sent this photo from ruby alaska as ...  \n",
       "...                                                  ...  \n",
       "10871  earthquake safety los angeles safety fasteners...  \n",
       "10872   storm in rhode island worse than last hurrica...  \n",
       "10873                  green line derailment in chicago   \n",
       "10874          meg issues hazardous weather outlook hwo   \n",
       "10875   cityof calgary has activated it is municipal ...  \n",
       "\n",
       "[10876 rows x 2 columns]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_check_rare(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "4f813f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1        False\n",
       "2        False\n",
       "3         True\n",
       "4        False\n",
       "         ...  \n",
       "10871    False\n",
       "10872     True\n",
       "10873     True\n",
       "10874     True\n",
       "10875     True\n",
       "Length: 10876, dtype: bool"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_tweets.tweets == all_tweets.new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "a5c1fb7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>new</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>forest fire near la range ask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all residents asked to shelter in place are b...</td>\n",
       "      <td>all residents asked to shelter in place are b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>just got sent this photo from ruby alaska as ...</td>\n",
       "      <td>just got sent this photo from ruby alaska as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>rocky fire update california highway closed i...</td>\n",
       "      <td>rocky fire update california highway closed i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flood disaster heavy rain causes flash floodin...</td>\n",
       "      <td>flood disaster heavy rain causes flash floodin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>love skiing</td>\n",
       "      <td>love skin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>manchester united football club they have bui...</td>\n",
       "      <td>manchester united football club they have bui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>had an awesome time visiting the cfc head off...</td>\n",
       "      <td>had an awesome time visiting the cfc head off...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>so pumped for ablaze</td>\n",
       "      <td>so pump for ablaze</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>noches el bestia happy to see my teammates an...</td>\n",
       "      <td>coaches el best happy to see my teammates and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>kurds trampling on turkmen flag later set it ...</td>\n",
       "      <td>kurdish trampling on turkmen flag later set i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>truck ablaze r voortrekker ave outside oregon ...</td>\n",
       "      <td>truck ablaze r voortrekker ave outside oregon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>set our hearts ablaze and every city was a gi...</td>\n",
       "      <td>set our hearts ablaze and every city was a gi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>revel in yours wmv videos by means of mac far...</td>\n",
       "      <td>revel in yours wmv videos by means of mac far...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>now playing rene ablaze ian buff magnitude edm</td>\n",
       "      <td>now playing rene ablaze ian stuff magnitude edm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>accident center lane blocked in santa clara o...</td>\n",
       "      <td>accident center lane blocked in santa clear o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>had a personalinjury accident this summer rea...</td>\n",
       "      <td>had a personalinjury accident this summer rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>stlouis caraccidentlawyer speeding among top c...</td>\n",
       "      <td>louis caraccidentlawyer spending among top cau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>reported motor vehicle accident in curry on h...</td>\n",
       "      <td>reported motor vehicle accident in carry on g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>i mile marker south mooresville iredell vehicl...</td>\n",
       "      <td>i mile marker south mooresville iredell vehicl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>retweet sleeping pills double your risk of a c...</td>\n",
       "      <td>retweet sleeping kills double your risk of a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>traffic accident n cabrillo highway magellan ...</td>\n",
       "      <td>traffic accident n cabrillo highway magellan ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>i mile marker to south mooresville iredell veh...</td>\n",
       "      <td>i mile marker to south mooresville iredell veh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>mom we did not get home as fast as we wished m...</td>\n",
       "      <td>mom we did not get home as fast as we washed m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>accident in ashville on us sb before sr traffic</td>\n",
       "      <td>accident in nashville on us sb before sr traf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>carolina accident motorcyclist dies in i cras...</td>\n",
       "      <td>carolina accident motorcyclist dies in i cras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>retweet n aa yf first accident in years turnin...</td>\n",
       "      <td>retweet n aa of first accident in years turnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>accident left lane blocked in manchester on r...</td>\n",
       "      <td>accident left lane blocked in manchester on r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>for your information cad for your information ...</td>\n",
       "      <td>for your information cad for your information ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>aashiqui actress anu aggarwal on her near fat...</td>\n",
       "      <td>aashiqui actress and aggarwal on her near fat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>has an accident changed your life we will hel...</td>\n",
       "      <td>has an accident changed your life we will hel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>breaking there was a deadly motorcycle car acc...</td>\n",
       "      <td>breaking there was a deadly motorcycle car acc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>bahrain police had previously died in a road ...</td>\n",
       "      <td>bargain police had previously died in a road ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>scuf ps live and the game cya</td>\n",
       "      <td>scuf ps live and the game ya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>i am speaking from someone that is using a sc...</td>\n",
       "      <td>i am speaking from someone that is using a sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>the harder the conflict the more glorious the...</td>\n",
       "      <td>the harper the conflict the more glorious the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>wisdom wed bonus minute daily habits that cou...</td>\n",
       "      <td>wisdom wed bones minute daily habits that cou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>aftershock protect yourself and profit in the...</td>\n",
       "      <td>aftershock protect yourself and profit in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>sometimes you face difficulties not because y...</td>\n",
       "      <td>sometimes you face difficulties not because y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>praise god that we have ministry that tells i...</td>\n",
       "      <td>rise god that we have ministry that tells it ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>the first man gets the oyster the second man ...</td>\n",
       "      <td>the first man gets the system the second man ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>aww they are on an airplane accident and they...</td>\n",
       "      <td>aww they are on an airplane accident and they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>family members of osama bin laden have died in...</td>\n",
       "      <td>family members of obama bin laden have died in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>i almost sent my coworker nudes on accident th...</td>\n",
       "      <td>i almost sent my coworker dudes on accident th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>this is unbelievably insane man airport airpl...</td>\n",
       "      <td>this is unbelievably insane man airport airpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>usama bin ladins family dead in airplane cras...</td>\n",
       "      <td>obama bin ladies family dead in airplane cras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>nicole fletcher one of a victim of crashed ai...</td>\n",
       "      <td>cole fletcher one of a victim of crashed airp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>oh my god i do not believe this rip bro air pl...</td>\n",
       "      <td>oh my god i do not believe this rip bro air pl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>airplane crashes on house in colombia people d...</td>\n",
       "      <td>airplane crashes on house in columbia people d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>could a drone because an airplane accident pi...</td>\n",
       "      <td>could a drone because an airplane accident pi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweets  \\\n",
       "1                forest fire near la ronge sask canada   \n",
       "2     all residents asked to shelter in place are b...   \n",
       "4     just got sent this photo from ruby alaska as ...   \n",
       "5     rocky fire update california highway closed i...   \n",
       "6    flood disaster heavy rain causes flash floodin...   \n",
       "22                                         love skiing   \n",
       "36    manchester united football club they have bui...   \n",
       "42    had an awesome time visiting the cfc head off...   \n",
       "43                               so pumped for ablaze    \n",
       "54    noches el bestia happy to see my teammates an...   \n",
       "55    kurds trampling on turkmen flag later set it ...   \n",
       "56   truck ablaze r voortrekker ave outside oregon ...   \n",
       "57    set our hearts ablaze and every city was a gi...   \n",
       "60    revel in yours wmv videos by means of mac far...   \n",
       "64      now playing rene ablaze ian buff magnitude edm   \n",
       "69    accident center lane blocked in santa clara o...   \n",
       "70    had a personalinjury accident this summer rea...   \n",
       "71   stlouis caraccidentlawyer speeding among top c...   \n",
       "72    reported motor vehicle accident in curry on h...   \n",
       "74   i mile marker south mooresville iredell vehicl...   \n",
       "75   retweet sleeping pills double your risk of a c...   \n",
       "77    traffic accident n cabrillo highway magellan ...   \n",
       "78   i mile marker to south mooresville iredell veh...   \n",
       "80   mom we did not get home as fast as we wished m...   \n",
       "84    accident in ashville on us sb before sr traffic    \n",
       "85    carolina accident motorcyclist dies in i cras...   \n",
       "87   retweet n aa yf first accident in years turnin...   \n",
       "88    accident left lane blocked in manchester on r...   \n",
       "91   for your information cad for your information ...   \n",
       "93    aashiqui actress anu aggarwal on her near fat...   \n",
       "96    has an accident changed your life we will hel...   \n",
       "97   breaking there was a deadly motorcycle car acc...   \n",
       "100   bahrain police had previously died in a road ...   \n",
       "102                      scuf ps live and the game cya   \n",
       "108   i am speaking from someone that is using a sc...   \n",
       "109   the harder the conflict the more glorious the...   \n",
       "120   wisdom wed bonus minute daily habits that cou...   \n",
       "121   aftershock protect yourself and profit in the...   \n",
       "125   sometimes you face difficulties not because y...   \n",
       "127   praise god that we have ministry that tells i...   \n",
       "134   the first man gets the oyster the second man ...   \n",
       "140   aww they are on an airplane accident and they...   \n",
       "141  family members of osama bin laden have died in...   \n",
       "149  i almost sent my coworker nudes on accident th...   \n",
       "152   this is unbelievably insane man airport airpl...   \n",
       "155   usama bin ladins family dead in airplane cras...   \n",
       "161   nicole fletcher one of a victim of crashed ai...   \n",
       "163  oh my god i do not believe this rip bro air pl...   \n",
       "168  airplane crashes on house in colombia people d...   \n",
       "170   could a drone because an airplane accident pi...   \n",
       "\n",
       "                                                   new  \n",
       "1                 forest fire near la range ask canada  \n",
       "2     all residents asked to shelter in place are b...  \n",
       "4     just got sent this photo from ruby alaska as ...  \n",
       "5     rocky fire update california highway closed i...  \n",
       "6    flood disaster heavy rain causes flash floodin...  \n",
       "22                                           love skin  \n",
       "36    manchester united football club they have bui...  \n",
       "42    had an awesome time visiting the cfc head off...  \n",
       "43                                 so pump for ablaze   \n",
       "54    coaches el best happy to see my teammates and...  \n",
       "55    kurdish trampling on turkmen flag later set i...  \n",
       "56   truck ablaze r voortrekker ave outside oregon ...  \n",
       "57    set our hearts ablaze and every city was a gi...  \n",
       "60    revel in yours wmv videos by means of mac far...  \n",
       "64     now playing rene ablaze ian stuff magnitude edm  \n",
       "69    accident center lane blocked in santa clear o...  \n",
       "70    had a personalinjury accident this summer rea...  \n",
       "71   louis caraccidentlawyer spending among top cau...  \n",
       "72    reported motor vehicle accident in carry on g...  \n",
       "74   i mile marker south mooresville iredell vehicl...  \n",
       "75   retweet sleeping kills double your risk of a c...  \n",
       "77    traffic accident n cabrillo highway magellan ...  \n",
       "78   i mile marker to south mooresville iredell veh...  \n",
       "80   mom we did not get home as fast as we washed m...  \n",
       "84    accident in nashville on us sb before sr traf...  \n",
       "85    carolina accident motorcyclist dies in i cras...  \n",
       "87   retweet n aa of first accident in years turnin...  \n",
       "88    accident left lane blocked in manchester on r...  \n",
       "91   for your information cad for your information ...  \n",
       "93    aashiqui actress and aggarwal on her near fat...  \n",
       "96    has an accident changed your life we will hel...  \n",
       "97   breaking there was a deadly motorcycle car acc...  \n",
       "100   bargain police had previously died in a road ...  \n",
       "102                       scuf ps live and the game ya  \n",
       "108   i am speaking from someone that is using a sc...  \n",
       "109   the harper the conflict the more glorious the...  \n",
       "120   wisdom wed bones minute daily habits that cou...  \n",
       "121   aftershock protect yourself and profit in the...  \n",
       "125   sometimes you face difficulties not because y...  \n",
       "127   rise god that we have ministry that tells it ...  \n",
       "134   the first man gets the system the second man ...  \n",
       "140   aww they are on an airplane accident and they...  \n",
       "141  family members of obama bin laden have died in...  \n",
       "149  i almost sent my coworker dudes on accident th...  \n",
       "152   this is unbelievably insane man airport airpl...  \n",
       "155   obama bin ladies family dead in airplane cras...  \n",
       "161   cole fletcher one of a victim of crashed airp...  \n",
       "163  oh my god i do not believe this rip bro air pl...  \n",
       "168  airplane crashes on house in columbia people d...  \n",
       "170   could a drone because an airplane accident pi...  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=all_tweets.loc[all_tweets.tweets != all_tweets.new]\n",
    "a[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = []\n",
    "for x in single_occ_words:\n",
    "    repl = spell_check_words(x)\n",
    "    corrections.append(repl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9226f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_dict = dict(zip(single_occ_words, corrections))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9475982",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_repl = dict(filter(lambda elem: elem[0] != elem[1], replacement_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fbb70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_spell(text):\n",
    "    r_re = re.compile(r'\\b'+r'\\b|\\b'.join(map(re.escape, filtered_repl.keys()))+r'\\b')\n",
    "    def replace(match):\n",
    "        replacement =  f\"{filtered_repl[match.group(0)]}\"\n",
    "        return replacement\n",
    "    text = r_re.sub(replace, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b05299",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweets_spellchecked'] = df['tweets'].apply(replace_spell)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f827683",
   "metadata": {},
   "source": [
    "## tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945c043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6121baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if word not in stopword_list:\n",
    "                tokens.append(word.lower())\n",
    "    return tokens\n",
    "\n",
    "def stem_text(text):\n",
    "    stemmer = SnowballStemmer()\n",
    "    stemmed_text = stemmer(tokenize_text(text))  \n",
    "     return ' '.join(x for x in stemmed_text if len(x) > 1)\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "# function to convert nltk tag to wordnet tag\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_text(text):\n",
    "    # define lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(tokenize_text(text))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    # initialize empty list \n",
    "    lemmatized_text = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_text.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_text.append(lemmatizer.lemmatize(word, tag))\n",
    "    return ' '.join(x for x in lemmatized_text if len(x) > 1)\n",
    "\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # function to remove stopwords from text\n",
    "    stop_list = stopwords.words('english')  \n",
    "    new_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_list:\n",
    "            new_tokens.append(word)\n",
    "    return new_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2870d95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### text preprocessing specifically formatted for tweets but will work on any text\n",
    "def tokenize_tweet(text): \n",
    "    \"\"\"combine regex and nltk processing for tweet text and keyword text processing\"\"\"\n",
    "    lem_text = lemmatize_text(text)\n",
    "    return lem_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff52303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df['new'].apply(tokenize_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476305c",
   "metadata": {},
   "source": [
    "## create column containing combined column text & tokens columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a220324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_columns(df):\n",
    "    df['clean_keyword'] = df.clean_keyword.astype(str)\n",
    "    df['clean_keyword'] = df.clean_keyword.str.lower()\n",
    "    df['clean_keyword'] = df.clean_keyword.str.replace(r'\\bnan\\b', r'')\n",
    "    df['clean_location'] = df.clean_location.astype(str)\n",
    "    df['clean_location'] = df.clean_location.str.lower()\n",
    "    df['clean_location'] = df.clean_location.str.replace(r'\\bnan\\b', r'')\n",
    "    df['clean_tweet'] = df.clean_tweet.astype(str)\n",
    "    df['all_text'] = df['clean_tweet'].str.cat(df['clean_location'],sep=\" \")\n",
    "    df['all_text'] = df['all_text'].str.cat(df['clean_keyword'],sep=\" \")\n",
    "    df['all_text'] = df['all_text'].str.strip()\n",
    "    df['combined_tokens'] = df.all_text.str.lower()\n",
    "    df['tweet_tokens'] = df.clean_tweet.str.lower()\n",
    "    df['combined_tokens'] = df.combined_tokens.apply(lambda row: row.split())\n",
    "    df['tweet_tokens'] = df.tweet_tokens.apply(lambda row: row.split())\n",
    "    new_df = df.drop(['keyword', 'location', 'text'], axis=1)\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd053b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = combine_columns(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803cca63",
   "metadata": {},
   "source": [
    "## check single occurence words again/remove words used only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d2348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_use_words(tokens):\n",
    "    # function to remove stopwords from text\n",
    "    stop_list = filtered_remove \n",
    "    new_tokens = []\n",
    "    for word in tokens:\n",
    "        if word not in stop_list:\n",
    "            new_tokens.append(word)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a3f8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = train_df.tweet_tokens\n",
    "word_dict = Counter(tweets.sum())\n",
    "# count number of words in corpus\n",
    "num_words = sum(word_dict[w] for w in word_dict)\n",
    "print(f'There are {num_words} words in the entire training dataset')\n",
    "\n",
    "# count number of unique words in corpus\n",
    "word_count_sorted = [(value, key) for key, value in word_dict.items()]\n",
    "word_count_sorted.sort()\n",
    "print(f'There are {len(word_dict)} unique words in the training dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28b7cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_remove = dict(filter(lambda elem: elem[1] == 1, word_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['filtered'] = train_df.tweet_tokens.apply(remove_single_use_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798527ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('train_tokenized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3dab3d",
   "metadata": {},
   "source": [
    "## process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57076d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrub_data_to_df(test_df)\n",
    "spell_check_rare(test_df)\n",
    "test_df['clean_tweet'] = test_df['new'].apply(tokenize_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b331a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df = combine_columns(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb772d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = test_df.tweet_tokens\n",
    "test_word_dict = Counter(test_tweets.sum())\n",
    "filtered_remove = dict(filter(lambda elem: elem[1] == 1, test_word_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae803b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['filtered'] = test_df.tweet_tokens.apply(remove_single_use_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('test_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f8c9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
